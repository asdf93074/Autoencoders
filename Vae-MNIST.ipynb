{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x27644e42d10>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torchvision\n",
    "from torch import nn\n",
    "from torchvision.transforms import v2\n",
    "from torch.utils.data import DataLoader, random_split\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "np.random.seed(42)\n",
    "torch.manual_seed(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\F\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\torchvision\\transforms\\v2\\_deprecated.py:42: UserWarning: The transform `ToTensor()` is deprecated and will be removed in a future release. Instead, please use `v2.Compose([v2.ToImage(), v2.ToDtype(torch.float32, scale=True)])`.Output is equivalent up to float precision.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "device = 'cpu'\n",
    "# device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "transforms = v2.Compose([\n",
    "    v2.ToTensor(),\n",
    "])\n",
    "\n",
    "train_complete_set = torchvision.datasets.MNIST('./datasets', train=True, transform=transforms, download=True)\n",
    "train_set, val_set = random_split(train_complete_set, [0.95, 0.05], torch.Generator().manual_seed(42))\n",
    "\n",
    "test_set = torchvision.datasets.MNIST('./datasets', train=False, transform=transforms, download=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 280,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 256\n",
    "\n",
    "train_loader = DataLoader(train_set, batch_size=batch_size, shuffle=True, num_workers=2, pin_memory=True, prefetch_factor=2)\n",
    "val_loader = DataLoader(val_set, batch_size=batch_size, num_workers=2, pin_memory=True)\n",
    "test_loader = DataLoader(test_set, batch_size=batch_size, num_workers=2, pin_memory=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 348,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Solver:\n",
    "    def __init__(self, model, lr=3e-3, reg=0.0, optim=torch.optim.Adam, batch_size=batch_size, criterion=nn.CrossEntropyLoss, print_every=100, anneal=False):\n",
    "        self.lr = lr\n",
    "        self.reg = reg\n",
    "        self.model = model\n",
    "        self.batch_size = batch_size\n",
    "        self.optim = optim(model.parameters(), lr=lr, weight_decay=reg)\n",
    "        self.criterion = criterion(reduction=\"mean\")\n",
    "        self.dataloaders = {}\n",
    "        self.loss_history = []\n",
    "        self.print_every = print_every\n",
    "        self.kl_loss_weight = 1.0\n",
    "\n",
    "        self.anneal = anneal\n",
    "        self.anneal_counter = 0\n",
    "        self.last_anneal_idx = 0\n",
    "        self.max_anneal_count = 3\n",
    "\n",
    "    def set_data_loader(self, loader, split):\n",
    "        self.dataloaders[split] = loader\n",
    "\n",
    "    def estimate_loss(self, split):\n",
    "        dataloader = self.dataloaders[split]\n",
    "\n",
    "        if dataloader == None:\n",
    "            return -1.0\n",
    "\n",
    "        self.model.eval()\n",
    "        avg_loss = 0.0\n",
    "        avg_kl_loss = 0.0\n",
    "        with torch.no_grad():\n",
    "            for i, data in enumerate(dataloader):\n",
    "                if i % 5000 == 0 and i > 0:\n",
    "                    break\n",
    "                images, _ = data\n",
    "                N = images.shape[0]\n",
    "                images = images.to(device).reshape(N, -1)\n",
    "\n",
    "                out, mean, log_var  = self.model(images)\n",
    "                reconn_loss = self.criterion(out, images)\n",
    "                kl_loss = 0.5 * (mean.square() + log_var.exp() - 1 - log_var).sum()\n",
    "\n",
    "                loss = reconn_loss\n",
    "                avg_loss += loss.item()\n",
    "                avg_kl_loss += kl_loss.item()\n",
    "\n",
    "        self.model.train()\n",
    "        print(f\"{split} reconn loss: {avg_loss / len(dataloader)}, kl loss: {avg_kl_loss / len(dataloader)}\")\n",
    "        return (avg_loss / len(dataloader)) # + (avg_kl_loss / len(dataloader))\n",
    "    \n",
    "    def anneal_learning_rate(self):\n",
    "        num_losses = len(self.loss_history)\n",
    "        if (self.anneal_counter == self.max_anneal_count\n",
    "            or num_losses < 3\n",
    "            or num_losses - self.last_anneal_idx < 3):\n",
    "            return\n",
    "\n",
    "        train = [l[0] for l in self.loss_history]\n",
    "        arr = np.array(train[-3:])\n",
    "        if arr.std() >= 1e-4:\n",
    "            return\n",
    "\n",
    "        self.last_anneal_idx = num_losses\n",
    "        self.anneal_counter += 1\n",
    "        print(f\"Annealing learning_rate {self.lr} by 10. New learning rate {self.lr / 10}. Anneal count {self.anneal_counter} / {self.max_anneal_count}.\")\n",
    "        self.lr /= 10\n",
    "\n",
    "    def train(self, loader, epochs=1):\n",
    "        for e in range(epochs):\n",
    "            for i, data in enumerate(loader):\n",
    "\n",
    "                # if i % self.print_every == 0 and i > 0:\n",
    "                #     train_loss = self.estimate_loss('train')\n",
    "                #     val_loss = self.estimate_loss('val')\n",
    "                    \n",
    "                #     print(f\"epoch: {e} iter: {i} train_loss: {train_loss} val_loss: {val_loss}\")\n",
    "\n",
    "                #     self.loss_history.append((train_loss, val_loss))\n",
    "                #     if self.anneal:\n",
    "                #         self.anneal_learning_rate()\n",
    "\n",
    "                images, _ = data\n",
    "                N = images.shape[0]\n",
    "                images = images.to(device).reshape(N, -1)\n",
    "\n",
    "                self.optim.zero_grad()\n",
    "\n",
    "                out, mean, log_var  = self.model(images)\n",
    "                out.retain_grad = True\n",
    "                reconn_loss = self.criterion(out, images)\n",
    "\n",
    "                loss = reconn_loss\n",
    "                kl_loss = 0.5 * (mean.square() + log_var.exp() - 1 - log_var).sum()\n",
    "                loss += kl_loss\n",
    "                print(f\"epoch: {e} iter: {i} reconn_loss: {reconn_loss} kl_loss: {kl_loss}\")\n",
    "\n",
    "\n",
    "                loss.backward()\n",
    "                self.optim.step()\n",
    "\n",
    "                self.kl_loss_weight = self.kl_loss_weight + 0.001\n",
    "            \n",
    "            for p in self.model.decoder.named_parameters():\n",
    "              print(p[0], p[1].grad.max(), p[1].grad.min())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 349,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Net(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels):\n",
    "        super().__init__()\n",
    "\n",
    "        self.in_channels = in_channels\n",
    "        self.out_channels = out_channels\n",
    "\n",
    "        self.encoder = nn.Sequential(\n",
    "            nn.Linear(in_channels, 512, device=device),\n",
    "            nn.LeakyReLU(0.02),\n",
    "            nn.Dropout(0.3),\n",
    "            nn.Linear(512, 64, device=device),\n",
    "            nn.LeakyReLU(0.02),\n",
    "            nn.Dropout(0.3),\n",
    "            nn.Linear(64, out_channels, device=device),\n",
    "        )\n",
    "\n",
    "        self.decoder = nn.Sequential(\n",
    "            nn.Linear(out_channels, 64, device=device),\n",
    "            nn.LeakyReLU(0.02),\n",
    "            nn.Linear(64, 512, device=device),\n",
    "            nn.LeakyReLU(0.02),\n",
    "            nn.Linear(512, in_channels, device=device),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "        self.ln_mean = nn.Linear(out_channels, out_channels, device=device)\n",
    "        self.ln_var  = nn.Linear(out_channels, out_channels, device=device)\n",
    "\n",
    "    def forward(self, input):\n",
    "        mean, log_var = self.encode(input)\n",
    "        q = self.reparameterize(mean, log_var)\n",
    "        x = self.decode(q)\n",
    "        return [x, mean, log_var]\n",
    "    \n",
    "    def encode(self, x):\n",
    "        encoded = self.encoder(x)\n",
    "        mean = self.ln_mean(encoded)\n",
    "        log_var  = self.ln_var(encoded)\n",
    "        \n",
    "        return [mean, log_var]\n",
    "\n",
    "    def reparameterize(self, mean, log_var):\n",
    "        std = z = torch.exp(0.5 * log_var)\n",
    "        eps  = torch.randn_like(std, requires_grad=False)\n",
    "\n",
    "        return mean + (std * eps)\n",
    "\n",
    "\n",
    "    def decode(self, embd):\n",
    "        return self.decoder(embd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 350,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cpu\n"
     ]
    }
   ],
   "source": [
    "print(device)\n",
    "model = Net(784, 75).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 351,
   "metadata": {},
   "outputs": [],
   "source": [
    "solver = Solver(model, lr=0.0003, batch_size=batch_size, criterion=nn.MSELoss, print_every=2)\n",
    "solver.set_data_loader(train_loader, 'train')\n",
    "solver.set_data_loader(val_loader, 'val')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 373,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 0 iter: 0 reconn_loss: 12489.8779296875 kl_loss: 777.7919311523438\n",
      "epoch: 0 iter: 1 reconn_loss: 12078.287109375 kl_loss: 833.5556030273438\n",
      "epoch: 0 iter: 2 reconn_loss: 12106.95703125 kl_loss: 841.2825317382812\n",
      "epoch: 0 iter: 3 reconn_loss: 12186.634765625 kl_loss: 927.784423828125\n",
      "epoch: 0 iter: 4 reconn_loss: 12126.1328125 kl_loss: 915.503662109375\n",
      "epoch: 0 iter: 5 reconn_loss: 11909.6201171875 kl_loss: 907.7681884765625\n",
      "epoch: 0 iter: 6 reconn_loss: 12177.3017578125 kl_loss: 897.4285278320312\n",
      "epoch: 0 iter: 7 reconn_loss: 11746.009765625 kl_loss: 955.491455078125\n",
      "epoch: 0 iter: 8 reconn_loss: 11602.4716796875 kl_loss: 881.835205078125\n",
      "epoch: 0 iter: 9 reconn_loss: 12305.63671875 kl_loss: 854.3868408203125\n",
      "epoch: 0 iter: 10 reconn_loss: 11772.3984375 kl_loss: 796.670166015625\n",
      "epoch: 0 iter: 11 reconn_loss: 12141.5458984375 kl_loss: 822.266845703125\n",
      "epoch: 0 iter: 12 reconn_loss: 12003.0341796875 kl_loss: 863.9511108398438\n",
      "epoch: 0 iter: 13 reconn_loss: 11781.4189453125 kl_loss: 827.192626953125\n",
      "epoch: 0 iter: 14 reconn_loss: 11691.0888671875 kl_loss: 759.0221557617188\n",
      "epoch: 0 iter: 15 reconn_loss: 12064.560546875 kl_loss: 800.4981079101562\n",
      "epoch: 0 iter: 16 reconn_loss: 11949.8154296875 kl_loss: 828.9011840820312\n",
      "epoch: 0 iter: 17 reconn_loss: 12244.736328125 kl_loss: 787.2310791015625\n",
      "epoch: 0 iter: 18 reconn_loss: 12137.0888671875 kl_loss: 845.2532958984375\n",
      "epoch: 0 iter: 19 reconn_loss: 11900.4970703125 kl_loss: 739.451171875\n",
      "epoch: 0 iter: 20 reconn_loss: 12124.15234375 kl_loss: 756.6349487304688\n",
      "epoch: 0 iter: 21 reconn_loss: 11957.60546875 kl_loss: 869.0323486328125\n",
      "epoch: 0 iter: 22 reconn_loss: 11824.650390625 kl_loss: 904.72216796875\n",
      "epoch: 0 iter: 23 reconn_loss: 12018.9716796875 kl_loss: 801.0836181640625\n",
      "epoch: 0 iter: 24 reconn_loss: 11951.82421875 kl_loss: 769.8145141601562\n",
      "epoch: 0 iter: 25 reconn_loss: 11892.6640625 kl_loss: 888.3732299804688\n",
      "epoch: 0 iter: 26 reconn_loss: 12203.5146484375 kl_loss: 890.2782592773438\n",
      "epoch: 0 iter: 27 reconn_loss: 11811.2822265625 kl_loss: 850.632080078125\n",
      "epoch: 0 iter: 28 reconn_loss: 12298.173828125 kl_loss: 861.3904418945312\n",
      "epoch: 0 iter: 29 reconn_loss: 11761.1806640625 kl_loss: 899.5989990234375\n",
      "epoch: 0 iter: 30 reconn_loss: 12076.447265625 kl_loss: 903.8561401367188\n",
      "epoch: 0 iter: 31 reconn_loss: 11976.9404296875 kl_loss: 950.0271606445312\n",
      "epoch: 0 iter: 32 reconn_loss: 12094.416015625 kl_loss: 889.6422729492188\n",
      "epoch: 0 iter: 33 reconn_loss: 12175.3369140625 kl_loss: 879.3733520507812\n",
      "epoch: 0 iter: 34 reconn_loss: 12128.083984375 kl_loss: 863.9656982421875\n",
      "epoch: 0 iter: 35 reconn_loss: 11969.53125 kl_loss: 812.1383666992188\n",
      "epoch: 0 iter: 36 reconn_loss: 12169.6240234375 kl_loss: 871.3776245117188\n",
      "epoch: 0 iter: 37 reconn_loss: 12032.0029296875 kl_loss: 1004.5266723632812\n",
      "epoch: 0 iter: 38 reconn_loss: 12104.4091796875 kl_loss: 900.6241455078125\n",
      "epoch: 0 iter: 39 reconn_loss: 11681.5400390625 kl_loss: 830.792236328125\n",
      "epoch: 0 iter: 40 reconn_loss: 12619.955078125 kl_loss: 848.2747802734375\n",
      "epoch: 0 iter: 41 reconn_loss: 11809.896484375 kl_loss: 839.3831176757812\n",
      "epoch: 0 iter: 42 reconn_loss: 11879.55859375 kl_loss: 833.1741943359375\n",
      "epoch: 0 iter: 43 reconn_loss: 12095.0986328125 kl_loss: 882.0626831054688\n",
      "epoch: 0 iter: 44 reconn_loss: 11819.0146484375 kl_loss: 876.9575805664062\n",
      "epoch: 0 iter: 45 reconn_loss: 12081.39453125 kl_loss: 865.1751098632812\n",
      "epoch: 0 iter: 46 reconn_loss: 11817.021484375 kl_loss: 841.3192138671875\n",
      "epoch: 0 iter: 47 reconn_loss: 11990.4951171875 kl_loss: 802.2400512695312\n",
      "epoch: 0 iter: 48 reconn_loss: 12083.1142578125 kl_loss: 835.751708984375\n",
      "epoch: 0 iter: 49 reconn_loss: 11796.048828125 kl_loss: 931.5464477539062\n",
      "epoch: 0 iter: 50 reconn_loss: 12315.40234375 kl_loss: 812.2225952148438\n",
      "epoch: 0 iter: 51 reconn_loss: 11758.693359375 kl_loss: 792.8914184570312\n",
      "epoch: 0 iter: 52 reconn_loss: 12211.95703125 kl_loss: 803.029296875\n",
      "epoch: 0 iter: 53 reconn_loss: 12167.5400390625 kl_loss: 882.8922729492188\n",
      "epoch: 0 iter: 54 reconn_loss: 12434.080078125 kl_loss: 764.800537109375\n",
      "epoch: 0 iter: 55 reconn_loss: 11788.69140625 kl_loss: 897.7365112304688\n",
      "epoch: 0 iter: 56 reconn_loss: 12059.3046875 kl_loss: 875.9573974609375\n",
      "epoch: 0 iter: 57 reconn_loss: 12176.3505859375 kl_loss: 818.1873779296875\n",
      "epoch: 0 iter: 58 reconn_loss: 12031.0 kl_loss: 909.7279052734375\n",
      "epoch: 0 iter: 59 reconn_loss: 11501.00390625 kl_loss: 835.55322265625\n",
      "epoch: 0 iter: 60 reconn_loss: 12082.259765625 kl_loss: 827.1813354492188\n",
      "epoch: 0 iter: 61 reconn_loss: 11712.7158203125 kl_loss: 876.0853271484375\n",
      "epoch: 0 iter: 62 reconn_loss: 11997.263671875 kl_loss: 857.2494506835938\n",
      "epoch: 0 iter: 63 reconn_loss: 11686.4482421875 kl_loss: 952.687255859375\n",
      "epoch: 0 iter: 64 reconn_loss: 11845.935546875 kl_loss: 785.094482421875\n",
      "epoch: 0 iter: 65 reconn_loss: 12152.5908203125 kl_loss: 934.3544921875\n",
      "epoch: 0 iter: 66 reconn_loss: 12084.4189453125 kl_loss: 935.6612548828125\n",
      "epoch: 0 iter: 67 reconn_loss: 12226.435546875 kl_loss: 849.0447998046875\n",
      "epoch: 0 iter: 68 reconn_loss: 11868.09765625 kl_loss: 928.6907958984375\n",
      "epoch: 0 iter: 69 reconn_loss: 11672.056640625 kl_loss: 1035.5902099609375\n",
      "epoch: 0 iter: 70 reconn_loss: 12083.7841796875 kl_loss: 796.2556762695312\n",
      "epoch: 0 iter: 71 reconn_loss: 12181.849609375 kl_loss: 955.8558959960938\n",
      "epoch: 0 iter: 72 reconn_loss: 12055.7041015625 kl_loss: 857.1234130859375\n",
      "epoch: 0 iter: 73 reconn_loss: 12099.3583984375 kl_loss: 853.5704345703125\n",
      "epoch: 0 iter: 74 reconn_loss: 11923.0390625 kl_loss: 873.7539672851562\n",
      "epoch: 0 iter: 75 reconn_loss: 11865.263671875 kl_loss: 847.6934204101562\n",
      "epoch: 0 iter: 76 reconn_loss: 12103.7216796875 kl_loss: 890.7645263671875\n",
      "epoch: 0 iter: 77 reconn_loss: 12230.4541015625 kl_loss: 951.1815795898438\n",
      "epoch: 0 iter: 78 reconn_loss: 11722.544921875 kl_loss: 977.46240234375\n",
      "epoch: 0 iter: 79 reconn_loss: 11859.8955078125 kl_loss: 892.9251708984375\n",
      "epoch: 0 iter: 80 reconn_loss: 12168.8837890625 kl_loss: 864.088623046875\n",
      "epoch: 0 iter: 81 reconn_loss: 12013.9228515625 kl_loss: 906.8370361328125\n",
      "epoch: 0 iter: 82 reconn_loss: 12070.6953125 kl_loss: 900.6336669921875\n",
      "epoch: 0 iter: 83 reconn_loss: 12020.6572265625 kl_loss: 930.2930297851562\n",
      "epoch: 0 iter: 84 reconn_loss: 11750.4052734375 kl_loss: 991.4791870117188\n",
      "epoch: 0 iter: 85 reconn_loss: 11966.263671875 kl_loss: 940.7211303710938\n",
      "epoch: 0 iter: 86 reconn_loss: 12084.3681640625 kl_loss: 1083.977294921875\n",
      "epoch: 0 iter: 87 reconn_loss: 12195.96484375 kl_loss: 948.814697265625\n",
      "epoch: 0 iter: 88 reconn_loss: 12176.7734375 kl_loss: 890.61083984375\n",
      "epoch: 0 iter: 89 reconn_loss: 11944.19140625 kl_loss: 943.8675537109375\n",
      "epoch: 0 iter: 90 reconn_loss: 12005.794921875 kl_loss: 884.945068359375\n",
      "epoch: 0 iter: 91 reconn_loss: 12029.333984375 kl_loss: 831.8389892578125\n",
      "epoch: 0 iter: 92 reconn_loss: 11904.1982421875 kl_loss: 856.0278930664062\n",
      "epoch: 0 iter: 93 reconn_loss: 11743.42578125 kl_loss: 903.130859375\n",
      "epoch: 0 iter: 94 reconn_loss: 11801.68359375 kl_loss: 828.6622314453125\n",
      "epoch: 0 iter: 95 reconn_loss: 11726.3759765625 kl_loss: 955.2137451171875\n",
      "epoch: 0 iter: 96 reconn_loss: 12257.44140625 kl_loss: 859.95556640625\n",
      "epoch: 0 iter: 97 reconn_loss: 11689.3056640625 kl_loss: 934.3658447265625\n",
      "epoch: 0 iter: 98 reconn_loss: 11943.458984375 kl_loss: 892.426025390625\n",
      "epoch: 0 iter: 99 reconn_loss: 11659.7490234375 kl_loss: 925.3849487304688\n",
      "epoch: 0 iter: 100 reconn_loss: 11918.580078125 kl_loss: 956.9493408203125\n",
      "epoch: 0 iter: 101 reconn_loss: 11926.6474609375 kl_loss: 850.4138793945312\n",
      "epoch: 0 iter: 102 reconn_loss: 11739.3779296875 kl_loss: 872.7255859375\n",
      "epoch: 0 iter: 103 reconn_loss: 11707.2548828125 kl_loss: 876.8129272460938\n",
      "epoch: 0 iter: 104 reconn_loss: 11637.001953125 kl_loss: 858.8633422851562\n",
      "epoch: 0 iter: 105 reconn_loss: 11769.3544921875 kl_loss: 876.0885009765625\n",
      "epoch: 0 iter: 106 reconn_loss: 11756.74609375 kl_loss: 882.0103149414062\n",
      "epoch: 0 iter: 107 reconn_loss: 11789.8876953125 kl_loss: 933.9280395507812\n",
      "epoch: 0 iter: 108 reconn_loss: 12145.1650390625 kl_loss: 945.9407958984375\n",
      "epoch: 0 iter: 109 reconn_loss: 12188.2265625 kl_loss: 904.3764038085938\n",
      "epoch: 0 iter: 110 reconn_loss: 12052.583984375 kl_loss: 818.8651733398438\n",
      "epoch: 0 iter: 111 reconn_loss: 12113.5126953125 kl_loss: 908.654052734375\n",
      "epoch: 0 iter: 112 reconn_loss: 11849.986328125 kl_loss: 914.1751098632812\n",
      "epoch: 0 iter: 113 reconn_loss: 12092.806640625 kl_loss: 920.1163330078125\n",
      "epoch: 0 iter: 114 reconn_loss: 11947.8759765625 kl_loss: 935.3912353515625\n",
      "epoch: 0 iter: 115 reconn_loss: 11678.3359375 kl_loss: 1002.97607421875\n",
      "epoch: 0 iter: 116 reconn_loss: 11970.65234375 kl_loss: 902.7174682617188\n",
      "epoch: 0 iter: 117 reconn_loss: 11521.7802734375 kl_loss: 897.0025634765625\n",
      "epoch: 0 iter: 118 reconn_loss: 12089.791015625 kl_loss: 978.0390014648438\n",
      "epoch: 0 iter: 119 reconn_loss: 11929.255859375 kl_loss: 977.88818359375\n",
      "epoch: 0 iter: 120 reconn_loss: 11853.3779296875 kl_loss: 919.7286376953125\n",
      "epoch: 0 iter: 121 reconn_loss: 11928.5625 kl_loss: 930.896728515625\n",
      "epoch: 0 iter: 122 reconn_loss: 12099.2646484375 kl_loss: 769.6205444335938\n",
      "epoch: 0 iter: 123 reconn_loss: 11866.818359375 kl_loss: 930.9041137695312\n",
      "epoch: 0 iter: 124 reconn_loss: 11979.42578125 kl_loss: 889.453857421875\n",
      "epoch: 0 iter: 125 reconn_loss: 11628.06640625 kl_loss: 831.3418579101562\n",
      "epoch: 0 iter: 126 reconn_loss: 11822.0146484375 kl_loss: 899.9756469726562\n",
      "epoch: 0 iter: 127 reconn_loss: 11770.85546875 kl_loss: 798.2623291015625\n",
      "epoch: 0 iter: 128 reconn_loss: 11938.607421875 kl_loss: 837.9328002929688\n",
      "epoch: 0 iter: 129 reconn_loss: 11880.2783203125 kl_loss: 828.7855224609375\n",
      "epoch: 0 iter: 130 reconn_loss: 12092.4345703125 kl_loss: 948.5611572265625\n",
      "epoch: 0 iter: 131 reconn_loss: 11807.21484375 kl_loss: 930.4666137695312\n",
      "epoch: 0 iter: 132 reconn_loss: 12202.4912109375 kl_loss: 900.9712524414062\n",
      "epoch: 0 iter: 133 reconn_loss: 11944.232421875 kl_loss: 965.2237548828125\n",
      "epoch: 0 iter: 134 reconn_loss: 11908.7939453125 kl_loss: 949.5906982421875\n",
      "epoch: 0 iter: 135 reconn_loss: 11656.8271484375 kl_loss: 1124.2352294921875\n",
      "epoch: 0 iter: 136 reconn_loss: 11786.087890625 kl_loss: 962.5775146484375\n",
      "epoch: 0 iter: 137 reconn_loss: 12208.9140625 kl_loss: 1006.2442626953125\n",
      "epoch: 0 iter: 138 reconn_loss: 11549.52734375 kl_loss: 1044.54052734375\n",
      "epoch: 0 iter: 139 reconn_loss: 11901.7568359375 kl_loss: 994.0325927734375\n",
      "epoch: 0 iter: 140 reconn_loss: 11598.6142578125 kl_loss: 1001.8914794921875\n",
      "epoch: 0 iter: 141 reconn_loss: 12143.38671875 kl_loss: 955.5084228515625\n",
      "epoch: 0 iter: 142 reconn_loss: 11590.5888671875 kl_loss: 851.104736328125\n",
      "epoch: 0 iter: 143 reconn_loss: 11819.982421875 kl_loss: 917.4576416015625\n",
      "epoch: 0 iter: 144 reconn_loss: 11964.92578125 kl_loss: 955.968505859375\n",
      "epoch: 0 iter: 145 reconn_loss: 11790.4873046875 kl_loss: 942.8959350585938\n",
      "epoch: 0 iter: 146 reconn_loss: 11985.728515625 kl_loss: 917.666015625\n",
      "epoch: 0 iter: 147 reconn_loss: 11606.7333984375 kl_loss: 907.4280395507812\n",
      "epoch: 0 iter: 148 reconn_loss: 11951.1240234375 kl_loss: 825.2160034179688\n",
      "epoch: 0 iter: 149 reconn_loss: 11639.279296875 kl_loss: 887.3194580078125\n",
      "epoch: 0 iter: 150 reconn_loss: 11667.177734375 kl_loss: 929.8418579101562\n",
      "epoch: 0 iter: 151 reconn_loss: 11598.595703125 kl_loss: 855.864501953125\n",
      "epoch: 0 iter: 152 reconn_loss: 11374.5107421875 kl_loss: 970.8455810546875\n",
      "epoch: 0 iter: 153 reconn_loss: 11355.9140625 kl_loss: 912.564453125\n",
      "epoch: 0 iter: 154 reconn_loss: 12111.3642578125 kl_loss: 914.8809814453125\n",
      "epoch: 0 iter: 155 reconn_loss: 12033.88671875 kl_loss: 983.2385864257812\n",
      "epoch: 0 iter: 156 reconn_loss: 11993.8134765625 kl_loss: 956.0487670898438\n",
      "epoch: 0 iter: 157 reconn_loss: 11698.6396484375 kl_loss: 965.0436401367188\n",
      "epoch: 0 iter: 158 reconn_loss: 11664.1474609375 kl_loss: 978.3973999023438\n",
      "epoch: 0 iter: 159 reconn_loss: 11827.5419921875 kl_loss: 1027.0999755859375\n",
      "epoch: 0 iter: 160 reconn_loss: 11663.3291015625 kl_loss: 932.8470458984375\n",
      "epoch: 0 iter: 161 reconn_loss: 11816.7421875 kl_loss: 1055.6934814453125\n",
      "epoch: 0 iter: 162 reconn_loss: 12077.2373046875 kl_loss: 911.308837890625\n",
      "epoch: 0 iter: 163 reconn_loss: 12020.0478515625 kl_loss: 943.932373046875\n",
      "epoch: 0 iter: 164 reconn_loss: 11780.1591796875 kl_loss: 954.4782104492188\n",
      "epoch: 0 iter: 165 reconn_loss: 12142.455078125 kl_loss: 954.05517578125\n",
      "epoch: 0 iter: 166 reconn_loss: 11601.3837890625 kl_loss: 890.8799438476562\n",
      "epoch: 0 iter: 167 reconn_loss: 11506.98828125 kl_loss: 971.9616088867188\n",
      "epoch: 0 iter: 168 reconn_loss: 11645.109375 kl_loss: 976.760009765625\n",
      "epoch: 0 iter: 169 reconn_loss: 11946.9111328125 kl_loss: 907.6717529296875\n",
      "epoch: 0 iter: 170 reconn_loss: 11628.1904296875 kl_loss: 902.4915771484375\n",
      "epoch: 0 iter: 171 reconn_loss: 12115.47265625 kl_loss: 933.9829711914062\n",
      "epoch: 0 iter: 172 reconn_loss: 11858.1015625 kl_loss: 886.2550659179688\n",
      "epoch: 0 iter: 173 reconn_loss: 11804.220703125 kl_loss: 878.18896484375\n",
      "epoch: 0 iter: 174 reconn_loss: 12097.1474609375 kl_loss: 952.8682861328125\n",
      "epoch: 0 iter: 175 reconn_loss: 11637.7158203125 kl_loss: 985.0287475585938\n",
      "epoch: 0 iter: 176 reconn_loss: 12051.56640625 kl_loss: 940.7724609375\n",
      "epoch: 0 iter: 177 reconn_loss: 11980.4833984375 kl_loss: 923.2385864257812\n",
      "epoch: 0 iter: 178 reconn_loss: 12223.8896484375 kl_loss: 984.9385375976562\n",
      "epoch: 0 iter: 179 reconn_loss: 11916.7353515625 kl_loss: 985.518310546875\n",
      "epoch: 0 iter: 180 reconn_loss: 11733.3798828125 kl_loss: 1003.9052734375\n",
      "epoch: 0 iter: 181 reconn_loss: 11916.29296875 kl_loss: 999.717529296875\n",
      "epoch: 0 iter: 182 reconn_loss: 11735.7783203125 kl_loss: 932.8804931640625\n",
      "epoch: 0 iter: 183 reconn_loss: 12042.083984375 kl_loss: 930.9297485351562\n",
      "epoch: 0 iter: 184 reconn_loss: 11786.51953125 kl_loss: 1068.01416015625\n",
      "epoch: 0 iter: 185 reconn_loss: 11604.2744140625 kl_loss: 992.8607788085938\n",
      "epoch: 0 iter: 186 reconn_loss: 11756.1455078125 kl_loss: 976.3494262695312\n",
      "epoch: 0 iter: 187 reconn_loss: 12058.3271484375 kl_loss: 1015.393798828125\n",
      "epoch: 0 iter: 188 reconn_loss: 11687.0166015625 kl_loss: 953.6654663085938\n",
      "epoch: 0 iter: 189 reconn_loss: 11775.51171875 kl_loss: 911.0291748046875\n",
      "epoch: 0 iter: 190 reconn_loss: 11669.240234375 kl_loss: 924.8812255859375\n",
      "epoch: 0 iter: 191 reconn_loss: 11661.8427734375 kl_loss: 942.0521240234375\n",
      "epoch: 0 iter: 192 reconn_loss: 12042.9365234375 kl_loss: 936.835205078125\n",
      "epoch: 0 iter: 193 reconn_loss: 12362.23046875 kl_loss: 953.6268920898438\n",
      "epoch: 0 iter: 194 reconn_loss: 11612.046875 kl_loss: 928.0921020507812\n",
      "epoch: 0 iter: 195 reconn_loss: 11283.791015625 kl_loss: 883.0057373046875\n",
      "epoch: 0 iter: 196 reconn_loss: 12161.623046875 kl_loss: 935.39990234375\n",
      "epoch: 0 iter: 197 reconn_loss: 11648.6025390625 kl_loss: 875.5186767578125\n",
      "epoch: 0 iter: 198 reconn_loss: 12019.5791015625 kl_loss: 1066.710205078125\n",
      "epoch: 0 iter: 199 reconn_loss: 11972.09375 kl_loss: 962.4514770507812\n",
      "epoch: 0 iter: 200 reconn_loss: 12208.1162109375 kl_loss: 991.9603881835938\n",
      "epoch: 0 iter: 201 reconn_loss: 11779.28515625 kl_loss: 1063.9873046875\n",
      "epoch: 0 iter: 202 reconn_loss: 11709.236328125 kl_loss: 984.2802734375\n",
      "epoch: 0 iter: 203 reconn_loss: 11687.919921875 kl_loss: 970.7225952148438\n",
      "epoch: 0 iter: 204 reconn_loss: 11827.1552734375 kl_loss: 1081.162353515625\n",
      "epoch: 0 iter: 205 reconn_loss: 12005.7216796875 kl_loss: 1019.23828125\n",
      "epoch: 0 iter: 206 reconn_loss: 11864.0439453125 kl_loss: 1011.0554809570312\n",
      "epoch: 0 iter: 207 reconn_loss: 11872.248046875 kl_loss: 1045.560791015625\n",
      "epoch: 0 iter: 208 reconn_loss: 11949.82421875 kl_loss: 959.3767700195312\n",
      "epoch: 0 iter: 209 reconn_loss: 12040.9013671875 kl_loss: 1054.727783203125\n",
      "epoch: 0 iter: 210 reconn_loss: 12192.5478515625 kl_loss: 1007.8970947265625\n",
      "epoch: 0 iter: 211 reconn_loss: 11522.5185546875 kl_loss: 1034.4658203125\n",
      "epoch: 0 iter: 212 reconn_loss: 11682.525390625 kl_loss: 996.073974609375\n",
      "epoch: 0 iter: 213 reconn_loss: 11630.4794921875 kl_loss: 1048.1734619140625\n",
      "epoch: 0 iter: 214 reconn_loss: 12064.2490234375 kl_loss: 977.0984497070312\n",
      "epoch: 0 iter: 215 reconn_loss: 11785.68359375 kl_loss: 934.86767578125\n",
      "epoch: 0 iter: 216 reconn_loss: 11684.96484375 kl_loss: 1023.3370361328125\n",
      "epoch: 0 iter: 217 reconn_loss: 11477.6923828125 kl_loss: 948.4708251953125\n",
      "epoch: 0 iter: 218 reconn_loss: 11619.3701171875 kl_loss: 994.0280151367188\n",
      "epoch: 0 iter: 219 reconn_loss: 12098.0966796875 kl_loss: 959.493408203125\n",
      "epoch: 0 iter: 220 reconn_loss: 11886.5927734375 kl_loss: 983.351318359375\n",
      "epoch: 0 iter: 221 reconn_loss: 11397.69140625 kl_loss: 1011.9437866210938\n",
      "epoch: 0 iter: 222 reconn_loss: 7453.40380859375 kl_loss: 648.5698852539062\n",
      "0.weight tensor(45.3503) tensor(-44.6474)\n",
      "0.bias tensor(12.4342) tensor(-25.8064)\n",
      "2.weight tensor(16.1623) tensor(-12.9728)\n",
      "2.bias tensor(7.0832) tensor(-6.6920)\n",
      "4.weight tensor(7.7525) tensor(-8.4262)\n",
      "4.bias tensor(5.4256) tensor(-5.1700)\n",
      "epoch: 1 iter: 0 reconn_loss: 12286.3125 kl_loss: 943.44921875\n",
      "epoch: 1 iter: 1 reconn_loss: 11800.357421875 kl_loss: 903.4984741210938\n",
      "epoch: 1 iter: 2 reconn_loss: 11937.505859375 kl_loss: 978.2186279296875\n",
      "epoch: 1 iter: 3 reconn_loss: 11870.0654296875 kl_loss: 927.8258056640625\n",
      "epoch: 1 iter: 4 reconn_loss: 12010.009765625 kl_loss: 990.690185546875\n",
      "epoch: 1 iter: 5 reconn_loss: 11893.0712890625 kl_loss: 1022.565185546875\n",
      "epoch: 1 iter: 6 reconn_loss: 11704.5068359375 kl_loss: 1052.4969482421875\n",
      "epoch: 1 iter: 7 reconn_loss: 11563.8798828125 kl_loss: 991.8526000976562\n",
      "epoch: 1 iter: 8 reconn_loss: 12184.6435546875 kl_loss: 1068.222900390625\n",
      "epoch: 1 iter: 9 reconn_loss: 11686.4052734375 kl_loss: 986.3097534179688\n",
      "epoch: 1 iter: 10 reconn_loss: 11765.546875 kl_loss: 1056.093505859375\n",
      "epoch: 1 iter: 11 reconn_loss: 11965.7294921875 kl_loss: 1065.0347900390625\n",
      "epoch: 1 iter: 12 reconn_loss: 11876.5888671875 kl_loss: 1014.5123291015625\n",
      "epoch: 1 iter: 13 reconn_loss: 11443.7607421875 kl_loss: 1017.2100219726562\n",
      "epoch: 1 iter: 14 reconn_loss: 11594.400390625 kl_loss: 1086.263671875\n",
      "epoch: 1 iter: 15 reconn_loss: 11413.7470703125 kl_loss: 957.769287109375\n",
      "epoch: 1 iter: 16 reconn_loss: 11547.98828125 kl_loss: 1011.2697143554688\n",
      "epoch: 1 iter: 17 reconn_loss: 11629.9228515625 kl_loss: 954.7166137695312\n",
      "epoch: 1 iter: 18 reconn_loss: 11764.4287109375 kl_loss: 937.5426025390625\n",
      "epoch: 1 iter: 19 reconn_loss: 11404.12109375 kl_loss: 995.400390625\n",
      "epoch: 1 iter: 20 reconn_loss: 11935.7900390625 kl_loss: 969.546875\n",
      "epoch: 1 iter: 21 reconn_loss: 11456.080078125 kl_loss: 1005.29833984375\n",
      "epoch: 1 iter: 22 reconn_loss: 12196.66015625 kl_loss: 1066.970703125\n",
      "epoch: 1 iter: 23 reconn_loss: 11429.37109375 kl_loss: 1057.73291015625\n",
      "epoch: 1 iter: 24 reconn_loss: 11761.083984375 kl_loss: 1004.1696166992188\n",
      "epoch: 1 iter: 25 reconn_loss: 11910.1484375 kl_loss: 1104.232666015625\n",
      "epoch: 1 iter: 26 reconn_loss: 11914.271484375 kl_loss: 911.6041259765625\n",
      "epoch: 1 iter: 27 reconn_loss: 11432.71875 kl_loss: 1130.907470703125\n",
      "epoch: 1 iter: 28 reconn_loss: 11738.810546875 kl_loss: 1078.896728515625\n",
      "epoch: 1 iter: 29 reconn_loss: 11650.2275390625 kl_loss: 962.1444091796875\n",
      "epoch: 1 iter: 30 reconn_loss: 11707.681640625 kl_loss: 993.450439453125\n",
      "epoch: 1 iter: 31 reconn_loss: 11538.8974609375 kl_loss: 1078.4619140625\n",
      "epoch: 1 iter: 32 reconn_loss: 12108.3876953125 kl_loss: 1088.0086669921875\n",
      "epoch: 1 iter: 33 reconn_loss: 11999.919921875 kl_loss: 1034.4669189453125\n",
      "epoch: 1 iter: 34 reconn_loss: 12165.994140625 kl_loss: 992.322021484375\n",
      "epoch: 1 iter: 35 reconn_loss: 11708.802734375 kl_loss: 979.3452758789062\n",
      "epoch: 1 iter: 36 reconn_loss: 11984.341796875 kl_loss: 984.1907348632812\n",
      "epoch: 1 iter: 37 reconn_loss: 11554.369140625 kl_loss: 1099.5626220703125\n",
      "epoch: 1 iter: 38 reconn_loss: 11508.4208984375 kl_loss: 962.765380859375\n",
      "epoch: 1 iter: 39 reconn_loss: 11954.46484375 kl_loss: 954.5924682617188\n",
      "epoch: 1 iter: 40 reconn_loss: 11847.2236328125 kl_loss: 1071.8443603515625\n",
      "epoch: 1 iter: 41 reconn_loss: 11667.7978515625 kl_loss: 1088.9259033203125\n",
      "epoch: 1 iter: 42 reconn_loss: 11771.2900390625 kl_loss: 1105.721923828125\n",
      "epoch: 1 iter: 43 reconn_loss: 11902.7958984375 kl_loss: 1131.599853515625\n",
      "epoch: 1 iter: 44 reconn_loss: 11694.65625 kl_loss: 997.177978515625\n",
      "epoch: 1 iter: 45 reconn_loss: 11552.521484375 kl_loss: 1190.7843017578125\n",
      "epoch: 1 iter: 46 reconn_loss: 11318.90625 kl_loss: 1062.7388916015625\n",
      "epoch: 1 iter: 47 reconn_loss: 11771.4736328125 kl_loss: 992.876953125\n",
      "epoch: 1 iter: 48 reconn_loss: 11878.029296875 kl_loss: 981.7245483398438\n",
      "epoch: 1 iter: 49 reconn_loss: 11911.0458984375 kl_loss: 1126.6475830078125\n",
      "epoch: 1 iter: 50 reconn_loss: 11748.9638671875 kl_loss: 978.4959106445312\n",
      "epoch: 1 iter: 51 reconn_loss: 11628.7578125 kl_loss: 1038.5699462890625\n",
      "epoch: 1 iter: 52 reconn_loss: 11682.8125 kl_loss: 1080.837646484375\n",
      "epoch: 1 iter: 53 reconn_loss: 11756.3115234375 kl_loss: 1049.8680419921875\n",
      "epoch: 1 iter: 54 reconn_loss: 11837.265625 kl_loss: 897.8651123046875\n",
      "epoch: 1 iter: 55 reconn_loss: 11660.564453125 kl_loss: 1002.22900390625\n",
      "epoch: 1 iter: 56 reconn_loss: 11477.572265625 kl_loss: 1019.430419921875\n",
      "epoch: 1 iter: 57 reconn_loss: 11643.9462890625 kl_loss: 998.6824951171875\n",
      "epoch: 1 iter: 58 reconn_loss: 11536.37109375 kl_loss: 1069.69384765625\n",
      "epoch: 1 iter: 59 reconn_loss: 11596.30078125 kl_loss: 1062.02587890625\n",
      "epoch: 1 iter: 60 reconn_loss: 11887.16796875 kl_loss: 1052.0859375\n",
      "epoch: 1 iter: 61 reconn_loss: 11791.75390625 kl_loss: 950.5988159179688\n",
      "epoch: 1 iter: 62 reconn_loss: 11511.142578125 kl_loss: 995.68896484375\n",
      "epoch: 1 iter: 63 reconn_loss: 11797.6298828125 kl_loss: 987.3665161132812\n",
      "epoch: 1 iter: 64 reconn_loss: 11684.2822265625 kl_loss: 1022.0826416015625\n",
      "epoch: 1 iter: 65 reconn_loss: 11695.3818359375 kl_loss: 1111.42578125\n",
      "epoch: 1 iter: 66 reconn_loss: 12377.232421875 kl_loss: 1132.10791015625\n",
      "epoch: 1 iter: 67 reconn_loss: 11459.12890625 kl_loss: 986.150390625\n",
      "epoch: 1 iter: 68 reconn_loss: 11637.0654296875 kl_loss: 1095.1141357421875\n",
      "epoch: 1 iter: 69 reconn_loss: 11727.7666015625 kl_loss: 958.8424072265625\n",
      "epoch: 1 iter: 70 reconn_loss: 11733.0927734375 kl_loss: 970.823486328125\n",
      "epoch: 1 iter: 71 reconn_loss: 11964.7783203125 kl_loss: 966.301025390625\n",
      "epoch: 1 iter: 72 reconn_loss: 11890.8994140625 kl_loss: 1065.420166015625\n",
      "epoch: 1 iter: 73 reconn_loss: 11435.6513671875 kl_loss: 1005.0088500976562\n",
      "epoch: 1 iter: 74 reconn_loss: 11804.083984375 kl_loss: 973.22705078125\n",
      "epoch: 1 iter: 75 reconn_loss: 11342.43359375 kl_loss: 1009.8572998046875\n",
      "epoch: 1 iter: 76 reconn_loss: 11456.7001953125 kl_loss: 1031.2080078125\n",
      "epoch: 1 iter: 77 reconn_loss: 11480.28125 kl_loss: 1136.3865966796875\n",
      "epoch: 1 iter: 78 reconn_loss: 11712.08203125 kl_loss: 1095.91357421875\n",
      "epoch: 1 iter: 79 reconn_loss: 11841.95703125 kl_loss: 1085.8673095703125\n",
      "epoch: 1 iter: 80 reconn_loss: 11807.5869140625 kl_loss: 1070.9620361328125\n",
      "epoch: 1 iter: 81 reconn_loss: 11549.2255859375 kl_loss: 1015.3930053710938\n",
      "epoch: 1 iter: 82 reconn_loss: 11462.1416015625 kl_loss: 1044.675048828125\n",
      "epoch: 1 iter: 83 reconn_loss: 11554.474609375 kl_loss: 1025.236083984375\n",
      "epoch: 1 iter: 84 reconn_loss: 11341.7822265625 kl_loss: 992.6205444335938\n",
      "epoch: 1 iter: 85 reconn_loss: 11524.978515625 kl_loss: 912.4365234375\n",
      "epoch: 1 iter: 86 reconn_loss: 11254.7109375 kl_loss: 990.706787109375\n",
      "epoch: 1 iter: 87 reconn_loss: 12118.4306640625 kl_loss: 983.487548828125\n",
      "epoch: 1 iter: 88 reconn_loss: 11330.298828125 kl_loss: 981.306396484375\n",
      "epoch: 1 iter: 89 reconn_loss: 11737.9677734375 kl_loss: 1129.1627197265625\n",
      "epoch: 1 iter: 90 reconn_loss: 11782.0283203125 kl_loss: 1018.7095336914062\n",
      "epoch: 1 iter: 91 reconn_loss: 11392.181640625 kl_loss: 1165.2183837890625\n",
      "epoch: 1 iter: 92 reconn_loss: 11576.373046875 kl_loss: 1124.78271484375\n",
      "epoch: 1 iter: 93 reconn_loss: 11481.140625 kl_loss: 1042.111328125\n",
      "epoch: 1 iter: 94 reconn_loss: 11690.1748046875 kl_loss: 1180.9788818359375\n",
      "epoch: 1 iter: 95 reconn_loss: 12032.8232421875 kl_loss: 1114.630859375\n",
      "epoch: 1 iter: 96 reconn_loss: 11420.4169921875 kl_loss: 1053.22265625\n",
      "epoch: 1 iter: 97 reconn_loss: 11666.0869140625 kl_loss: 1116.694580078125\n",
      "epoch: 1 iter: 98 reconn_loss: 11793.734375 kl_loss: 1023.697998046875\n",
      "epoch: 1 iter: 99 reconn_loss: 11951.533203125 kl_loss: 1113.83251953125\n",
      "epoch: 1 iter: 100 reconn_loss: 11316.6259765625 kl_loss: 1054.831298828125\n",
      "epoch: 1 iter: 101 reconn_loss: 11856.2138671875 kl_loss: 995.1068725585938\n",
      "epoch: 1 iter: 102 reconn_loss: 11623.0390625 kl_loss: 1038.9510498046875\n",
      "epoch: 1 iter: 103 reconn_loss: 11546.41796875 kl_loss: 1048.1273193359375\n",
      "epoch: 1 iter: 104 reconn_loss: 11737.6640625 kl_loss: 1085.65185546875\n",
      "epoch: 1 iter: 105 reconn_loss: 11373.908203125 kl_loss: 1108.716552734375\n",
      "epoch: 1 iter: 106 reconn_loss: 12077.5830078125 kl_loss: 1158.1632080078125\n",
      "epoch: 1 iter: 107 reconn_loss: 11990.166015625 kl_loss: 1088.880859375\n",
      "epoch: 1 iter: 108 reconn_loss: 11657.0400390625 kl_loss: 1068.5247802734375\n",
      "epoch: 1 iter: 109 reconn_loss: 11515.58203125 kl_loss: 1096.97314453125\n",
      "epoch: 1 iter: 110 reconn_loss: 11524.6982421875 kl_loss: 1089.994873046875\n",
      "epoch: 1 iter: 111 reconn_loss: 11540.77734375 kl_loss: 1117.140380859375\n",
      "epoch: 1 iter: 112 reconn_loss: 11803.353515625 kl_loss: 1128.5411376953125\n",
      "epoch: 1 iter: 113 reconn_loss: 11609.7216796875 kl_loss: 1089.193603515625\n",
      "epoch: 1 iter: 114 reconn_loss: 11545.052734375 kl_loss: 1040.1007080078125\n",
      "epoch: 1 iter: 115 reconn_loss: 11540.6435546875 kl_loss: 997.6529541015625\n",
      "epoch: 1 iter: 116 reconn_loss: 11510.396484375 kl_loss: 1017.6754150390625\n",
      "epoch: 1 iter: 117 reconn_loss: 11657.3759765625 kl_loss: 1063.522705078125\n",
      "epoch: 1 iter: 118 reconn_loss: 11748.2216796875 kl_loss: 1045.422607421875\n",
      "epoch: 1 iter: 119 reconn_loss: 11443.1865234375 kl_loss: 1114.1748046875\n",
      "epoch: 1 iter: 120 reconn_loss: 11804.1416015625 kl_loss: 1093.1036376953125\n",
      "epoch: 1 iter: 121 reconn_loss: 11822.1337890625 kl_loss: 1064.8978271484375\n",
      "epoch: 1 iter: 122 reconn_loss: 11568.5830078125 kl_loss: 1065.071044921875\n",
      "epoch: 1 iter: 123 reconn_loss: 11371.3662109375 kl_loss: 1051.5372314453125\n",
      "epoch: 1 iter: 124 reconn_loss: 11649.1728515625 kl_loss: 1062.6710205078125\n",
      "epoch: 1 iter: 125 reconn_loss: 11965.5048828125 kl_loss: 985.455322265625\n",
      "epoch: 1 iter: 126 reconn_loss: 11822.2255859375 kl_loss: 1094.419677734375\n",
      "epoch: 1 iter: 127 reconn_loss: 12370.15234375 kl_loss: 1105.2559814453125\n",
      "epoch: 1 iter: 128 reconn_loss: 11722.0693359375 kl_loss: 1062.0089111328125\n",
      "epoch: 1 iter: 129 reconn_loss: 11726.0693359375 kl_loss: 1141.7362060546875\n",
      "epoch: 1 iter: 130 reconn_loss: 11662.2412109375 kl_loss: 1112.5938720703125\n",
      "epoch: 1 iter: 131 reconn_loss: 11559.14453125 kl_loss: 1113.5950927734375\n",
      "epoch: 1 iter: 132 reconn_loss: 11696.9296875 kl_loss: 1133.101318359375\n",
      "epoch: 1 iter: 133 reconn_loss: 11746.9296875 kl_loss: 1066.8880615234375\n",
      "epoch: 1 iter: 134 reconn_loss: 11501.7412109375 kl_loss: 1115.748779296875\n",
      "epoch: 1 iter: 135 reconn_loss: 11412.8671875 kl_loss: 1068.4881591796875\n",
      "epoch: 1 iter: 136 reconn_loss: 11843.919921875 kl_loss: 1034.797119140625\n",
      "epoch: 1 iter: 137 reconn_loss: 11470.38671875 kl_loss: 1074.5042724609375\n",
      "epoch: 1 iter: 138 reconn_loss: 11507.455078125 kl_loss: 1024.677001953125\n",
      "epoch: 1 iter: 139 reconn_loss: 11323.728515625 kl_loss: 1022.8464965820312\n",
      "epoch: 1 iter: 140 reconn_loss: 11520.37890625 kl_loss: 978.7031860351562\n",
      "epoch: 1 iter: 141 reconn_loss: 11474.625 kl_loss: 1080.593994140625\n",
      "epoch: 1 iter: 142 reconn_loss: 11156.90234375 kl_loss: 1156.23486328125\n",
      "epoch: 1 iter: 143 reconn_loss: 11544.0556640625 kl_loss: 1061.0130615234375\n",
      "epoch: 1 iter: 144 reconn_loss: 11503.828125 kl_loss: 1137.7027587890625\n",
      "epoch: 1 iter: 145 reconn_loss: 11284.6171875 kl_loss: 1018.9456787109375\n",
      "epoch: 1 iter: 146 reconn_loss: 11654.046875 kl_loss: 1098.4671630859375\n",
      "epoch: 1 iter: 147 reconn_loss: 11400.5732421875 kl_loss: 1162.0838623046875\n",
      "epoch: 1 iter: 148 reconn_loss: 11603.453125 kl_loss: 1016.0733032226562\n",
      "epoch: 1 iter: 149 reconn_loss: 11720.27734375 kl_loss: 1082.208984375\n",
      "epoch: 1 iter: 150 reconn_loss: 11624.18359375 kl_loss: 1103.560546875\n",
      "epoch: 1 iter: 151 reconn_loss: 11553.2294921875 kl_loss: 1084.770263671875\n",
      "epoch: 1 iter: 152 reconn_loss: 11283.099609375 kl_loss: 1103.1153564453125\n",
      "epoch: 1 iter: 153 reconn_loss: 11723.3681640625 kl_loss: 1093.722412109375\n",
      "epoch: 1 iter: 154 reconn_loss: 11450.86328125 kl_loss: 1002.2055053710938\n",
      "epoch: 1 iter: 155 reconn_loss: 11127.3515625 kl_loss: 1120.7626953125\n",
      "epoch: 1 iter: 156 reconn_loss: 11469.611328125 kl_loss: 999.2872924804688\n",
      "epoch: 1 iter: 157 reconn_loss: 11449.1533203125 kl_loss: 986.4837036132812\n",
      "epoch: 1 iter: 158 reconn_loss: 11801.5546875 kl_loss: 1075.283447265625\n",
      "epoch: 1 iter: 159 reconn_loss: 11322.0166015625 kl_loss: 1032.307373046875\n",
      "epoch: 1 iter: 160 reconn_loss: 11558.1787109375 kl_loss: 1084.4443359375\n",
      "epoch: 1 iter: 161 reconn_loss: 11220.5185546875 kl_loss: 1045.201904296875\n",
      "epoch: 1 iter: 162 reconn_loss: 11869.8896484375 kl_loss: 1146.8258056640625\n",
      "epoch: 1 iter: 163 reconn_loss: 11224.71875 kl_loss: 1101.4595947265625\n",
      "epoch: 1 iter: 164 reconn_loss: 11681.830078125 kl_loss: 1091.7437744140625\n",
      "epoch: 1 iter: 165 reconn_loss: 11927.4677734375 kl_loss: 1204.5257568359375\n",
      "epoch: 1 iter: 166 reconn_loss: 11587.619140625 kl_loss: 1077.50537109375\n",
      "epoch: 1 iter: 167 reconn_loss: 11429.5224609375 kl_loss: 1114.349365234375\n",
      "epoch: 1 iter: 168 reconn_loss: 11550.0849609375 kl_loss: 1101.7001953125\n",
      "epoch: 1 iter: 169 reconn_loss: 11816.2392578125 kl_loss: 1216.446044921875\n",
      "epoch: 1 iter: 170 reconn_loss: 11612.34375 kl_loss: 1149.3885498046875\n",
      "epoch: 1 iter: 171 reconn_loss: 11620.6640625 kl_loss: 1143.847412109375\n",
      "epoch: 1 iter: 172 reconn_loss: 11541.919921875 kl_loss: 1108.0738525390625\n",
      "epoch: 1 iter: 173 reconn_loss: 11549.9375 kl_loss: 1115.53369140625\n",
      "epoch: 1 iter: 174 reconn_loss: 11513.9775390625 kl_loss: 1106.483642578125\n",
      "epoch: 1 iter: 175 reconn_loss: 11610.556640625 kl_loss: 1148.9215087890625\n",
      "epoch: 1 iter: 176 reconn_loss: 11701.685546875 kl_loss: 1081.395263671875\n",
      "epoch: 1 iter: 177 reconn_loss: 11678.2451171875 kl_loss: 1088.3760986328125\n",
      "epoch: 1 iter: 178 reconn_loss: 11599.2294921875 kl_loss: 1091.4268798828125\n",
      "epoch: 1 iter: 179 reconn_loss: 11543.8818359375 kl_loss: 1094.7275390625\n",
      "epoch: 1 iter: 180 reconn_loss: 11556.30859375 kl_loss: 1110.0394287109375\n",
      "epoch: 1 iter: 181 reconn_loss: 11685.4697265625 kl_loss: 1073.8408203125\n",
      "epoch: 1 iter: 182 reconn_loss: 11566.7509765625 kl_loss: 1129.6905517578125\n",
      "epoch: 1 iter: 183 reconn_loss: 11379.068359375 kl_loss: 1072.658935546875\n",
      "epoch: 1 iter: 184 reconn_loss: 11668.6884765625 kl_loss: 1113.586669921875\n",
      "epoch: 1 iter: 185 reconn_loss: 11938.048828125 kl_loss: 1158.036865234375\n",
      "epoch: 1 iter: 186 reconn_loss: 11699.04296875 kl_loss: 1079.7471923828125\n",
      "epoch: 1 iter: 187 reconn_loss: 11382.40234375 kl_loss: 1125.721923828125\n",
      "epoch: 1 iter: 188 reconn_loss: 11956.0849609375 kl_loss: 1157.8204345703125\n",
      "epoch: 1 iter: 189 reconn_loss: 11713.806640625 kl_loss: 1159.031494140625\n",
      "epoch: 1 iter: 190 reconn_loss: 11388.71875 kl_loss: 1117.77001953125\n",
      "epoch: 1 iter: 191 reconn_loss: 12056.3330078125 kl_loss: 1115.9947509765625\n",
      "epoch: 1 iter: 192 reconn_loss: 11579.248046875 kl_loss: 1114.701904296875\n",
      "epoch: 1 iter: 193 reconn_loss: 11389.095703125 kl_loss: 1161.935302734375\n",
      "epoch: 1 iter: 194 reconn_loss: 11559.0830078125 kl_loss: 1211.744873046875\n",
      "epoch: 1 iter: 195 reconn_loss: 11750.310546875 kl_loss: 1085.7132568359375\n",
      "epoch: 1 iter: 196 reconn_loss: 11569.482421875 kl_loss: 1058.666259765625\n",
      "epoch: 1 iter: 197 reconn_loss: 11207.65234375 kl_loss: 1075.8505859375\n",
      "epoch: 1 iter: 198 reconn_loss: 11623.0517578125 kl_loss: 1088.3194580078125\n",
      "epoch: 1 iter: 199 reconn_loss: 11741.0712890625 kl_loss: 1199.282958984375\n",
      "epoch: 1 iter: 200 reconn_loss: 11258.607421875 kl_loss: 1213.111083984375\n",
      "epoch: 1 iter: 201 reconn_loss: 11340.1357421875 kl_loss: 1051.7598876953125\n",
      "epoch: 1 iter: 202 reconn_loss: 11735.3232421875 kl_loss: 1144.2642822265625\n",
      "epoch: 1 iter: 203 reconn_loss: 11766.6181640625 kl_loss: 1086.576171875\n",
      "epoch: 1 iter: 204 reconn_loss: 11409.2724609375 kl_loss: 1095.8792724609375\n",
      "epoch: 1 iter: 205 reconn_loss: 11542.8759765625 kl_loss: 1071.130615234375\n",
      "epoch: 1 iter: 206 reconn_loss: 11893.1708984375 kl_loss: 1073.3104248046875\n",
      "epoch: 1 iter: 207 reconn_loss: 11731.275390625 kl_loss: 1093.5350341796875\n",
      "epoch: 1 iter: 208 reconn_loss: 11174.4208984375 kl_loss: 1102.547607421875\n",
      "epoch: 1 iter: 209 reconn_loss: 11354.9599609375 kl_loss: 1127.2042236328125\n",
      "epoch: 1 iter: 210 reconn_loss: 11574.8544921875 kl_loss: 1174.3265380859375\n",
      "epoch: 1 iter: 211 reconn_loss: 11265.158203125 kl_loss: 1184.3106689453125\n",
      "epoch: 1 iter: 212 reconn_loss: 12086.115234375 kl_loss: 1061.517822265625\n",
      "epoch: 1 iter: 213 reconn_loss: 11362.3349609375 kl_loss: 1111.3955078125\n",
      "epoch: 1 iter: 214 reconn_loss: 11841.966796875 kl_loss: 1150.02392578125\n",
      "epoch: 1 iter: 215 reconn_loss: 12045.76953125 kl_loss: 1218.3543701171875\n",
      "epoch: 1 iter: 216 reconn_loss: 11188.619140625 kl_loss: 1206.379150390625\n",
      "epoch: 1 iter: 217 reconn_loss: 11650.7900390625 kl_loss: 1138.7938232421875\n",
      "epoch: 1 iter: 218 reconn_loss: 11430.3232421875 kl_loss: 1242.5985107421875\n",
      "epoch: 1 iter: 219 reconn_loss: 11912.3818359375 kl_loss: 1155.7060546875\n",
      "epoch: 1 iter: 220 reconn_loss: 11651.314453125 kl_loss: 1169.9359130859375\n",
      "epoch: 1 iter: 221 reconn_loss: 11584.369140625 kl_loss: 1149.4207763671875\n",
      "epoch: 1 iter: 222 reconn_loss: 7813.6220703125 kl_loss: 757.430908203125\n",
      "0.weight tensor(61.4324) tensor(-48.0105)\n",
      "0.bias tensor(23.9639) tensor(-23.0154)\n",
      "2.weight tensor(13.9142) tensor(-11.3678)\n",
      "2.bias tensor(9.2787) tensor(-7.8115)\n",
      "4.weight tensor(8.0115) tensor(-14.3331)\n",
      "4.bias tensor(4.5716) tensor(-9.8000)\n",
      "epoch: 2 iter: 0 reconn_loss: 11318.626953125 kl_loss: 1217.2713623046875\n",
      "epoch: 2 iter: 1 reconn_loss: 11215.458984375 kl_loss: 1137.5299072265625\n",
      "epoch: 2 iter: 2 reconn_loss: 11627.1435546875 kl_loss: 1167.2342529296875\n",
      "epoch: 2 iter: 3 reconn_loss: 11363.11328125 kl_loss: 1153.5599365234375\n",
      "epoch: 2 iter: 4 reconn_loss: 11267.041015625 kl_loss: 1208.386962890625\n",
      "epoch: 2 iter: 5 reconn_loss: 11633.775390625 kl_loss: 1215.8392333984375\n",
      "epoch: 2 iter: 6 reconn_loss: 11737.4248046875 kl_loss: 1187.1435546875\n",
      "epoch: 2 iter: 7 reconn_loss: 11883.3515625 kl_loss: 1136.41357421875\n",
      "epoch: 2 iter: 8 reconn_loss: 11279.37109375 kl_loss: 1235.0135498046875\n",
      "epoch: 2 iter: 9 reconn_loss: 11665.455078125 kl_loss: 1250.5614013671875\n",
      "epoch: 2 iter: 10 reconn_loss: 11622.1806640625 kl_loss: 1146.257568359375\n",
      "epoch: 2 iter: 11 reconn_loss: 11738.109375 kl_loss: 1148.7867431640625\n",
      "epoch: 2 iter: 12 reconn_loss: 11430.310546875 kl_loss: 1140.6298828125\n",
      "epoch: 2 iter: 13 reconn_loss: 11637.107421875 kl_loss: 1257.81298828125\n",
      "epoch: 2 iter: 14 reconn_loss: 11019.416015625 kl_loss: 1114.06689453125\n",
      "epoch: 2 iter: 15 reconn_loss: 11596.626953125 kl_loss: 1121.692138671875\n",
      "epoch: 2 iter: 16 reconn_loss: 11520.93359375 kl_loss: 1213.1690673828125\n",
      "epoch: 2 iter: 17 reconn_loss: 11506.552734375 kl_loss: 1217.568603515625\n",
      "epoch: 2 iter: 18 reconn_loss: 11534.767578125 kl_loss: 1089.845703125\n",
      "epoch: 2 iter: 19 reconn_loss: 11663.10546875 kl_loss: 1129.1817626953125\n",
      "epoch: 2 iter: 20 reconn_loss: 11579.9619140625 kl_loss: 1129.161376953125\n",
      "epoch: 2 iter: 21 reconn_loss: 11328.6650390625 kl_loss: 1093.115478515625\n",
      "epoch: 2 iter: 22 reconn_loss: 11598.95703125 kl_loss: 1120.578125\n",
      "epoch: 2 iter: 23 reconn_loss: 11341.4140625 kl_loss: 1185.955322265625\n",
      "epoch: 2 iter: 24 reconn_loss: 11537.0390625 kl_loss: 1153.500244140625\n",
      "epoch: 2 iter: 25 reconn_loss: 11435.8486328125 kl_loss: 1146.3837890625\n",
      "epoch: 2 iter: 26 reconn_loss: 11555.1337890625 kl_loss: 1123.6568603515625\n",
      "epoch: 2 iter: 27 reconn_loss: 11648.09765625 kl_loss: 1191.6737060546875\n",
      "epoch: 2 iter: 28 reconn_loss: 11415.4765625 kl_loss: 1115.4957275390625\n",
      "epoch: 2 iter: 29 reconn_loss: 11695.248046875 kl_loss: 1152.852294921875\n",
      "epoch: 2 iter: 30 reconn_loss: 11806.3671875 kl_loss: 1212.7266845703125\n",
      "epoch: 2 iter: 31 reconn_loss: 11720.3125 kl_loss: 1162.90380859375\n",
      "epoch: 2 iter: 32 reconn_loss: 11791.998046875 kl_loss: 1204.2550048828125\n",
      "epoch: 2 iter: 33 reconn_loss: 11488.0400390625 kl_loss: 1187.998779296875\n",
      "epoch: 2 iter: 34 reconn_loss: 11392.9453125 kl_loss: 1075.9896240234375\n",
      "epoch: 2 iter: 35 reconn_loss: 11480.6669921875 kl_loss: 1144.9896240234375\n",
      "epoch: 2 iter: 36 reconn_loss: 11827.5771484375 kl_loss: 1198.889892578125\n",
      "epoch: 2 iter: 37 reconn_loss: 11570.2626953125 kl_loss: 1177.301513671875\n",
      "epoch: 2 iter: 38 reconn_loss: 11705.578125 kl_loss: 1177.1329345703125\n",
      "epoch: 2 iter: 39 reconn_loss: 11542.9931640625 kl_loss: 1217.8896484375\n",
      "epoch: 2 iter: 40 reconn_loss: 11807.0068359375 kl_loss: 1221.146240234375\n",
      "epoch: 2 iter: 41 reconn_loss: 11218.0380859375 kl_loss: 1149.9703369140625\n",
      "epoch: 2 iter: 42 reconn_loss: 11528.65625 kl_loss: 1131.873046875\n",
      "epoch: 2 iter: 43 reconn_loss: 11741.93359375 kl_loss: 1246.8961181640625\n",
      "epoch: 2 iter: 44 reconn_loss: 11490.42578125 kl_loss: 1170.7042236328125\n",
      "epoch: 2 iter: 45 reconn_loss: 11411.76171875 kl_loss: 1134.27587890625\n",
      "epoch: 2 iter: 46 reconn_loss: 11453.5029296875 kl_loss: 1181.4124755859375\n",
      "epoch: 2 iter: 47 reconn_loss: 11551.734375 kl_loss: 1147.36474609375\n",
      "epoch: 2 iter: 48 reconn_loss: 11217.9931640625 kl_loss: 1191.942138671875\n",
      "epoch: 2 iter: 49 reconn_loss: 11066.724609375 kl_loss: 1177.7327880859375\n",
      "epoch: 2 iter: 50 reconn_loss: 10994.421875 kl_loss: 1174.82568359375\n",
      "epoch: 2 iter: 51 reconn_loss: 11336.58984375 kl_loss: 1204.355712890625\n",
      "epoch: 2 iter: 52 reconn_loss: 12060.8544921875 kl_loss: 1160.225341796875\n",
      "epoch: 2 iter: 53 reconn_loss: 11265.310546875 kl_loss: 1190.342529296875\n",
      "epoch: 2 iter: 54 reconn_loss: 11880.626953125 kl_loss: 1133.5841064453125\n",
      "epoch: 2 iter: 55 reconn_loss: 11243.990234375 kl_loss: 1147.8817138671875\n",
      "epoch: 2 iter: 56 reconn_loss: 11826.974609375 kl_loss: 1229.55517578125\n",
      "epoch: 2 iter: 57 reconn_loss: 11921.787109375 kl_loss: 1130.36962890625\n",
      "epoch: 2 iter: 58 reconn_loss: 11186.240234375 kl_loss: 1173.721435546875\n",
      "epoch: 2 iter: 59 reconn_loss: 11355.861328125 kl_loss: 1136.0933837890625\n",
      "epoch: 2 iter: 60 reconn_loss: 11367.958984375 kl_loss: 1159.3037109375\n",
      "epoch: 2 iter: 61 reconn_loss: 11371.134765625 kl_loss: 1265.00244140625\n",
      "epoch: 2 iter: 62 reconn_loss: 11675.6123046875 kl_loss: 1188.420166015625\n",
      "epoch: 2 iter: 63 reconn_loss: 12039.2763671875 kl_loss: 1245.1875\n",
      "epoch: 2 iter: 64 reconn_loss: 11640.4794921875 kl_loss: 1286.3387451171875\n",
      "epoch: 2 iter: 65 reconn_loss: 11427.8828125 kl_loss: 1262.648681640625\n",
      "epoch: 2 iter: 66 reconn_loss: 11144.8828125 kl_loss: 1190.0556640625\n",
      "epoch: 2 iter: 67 reconn_loss: 11572.638671875 kl_loss: 1148.7257080078125\n",
      "epoch: 2 iter: 68 reconn_loss: 11291.2939453125 kl_loss: 1171.7877197265625\n",
      "epoch: 2 iter: 69 reconn_loss: 11519.8203125 kl_loss: 1179.240478515625\n",
      "epoch: 2 iter: 70 reconn_loss: 11096.15625 kl_loss: 1115.210205078125\n",
      "epoch: 2 iter: 71 reconn_loss: 11413.978515625 kl_loss: 1128.8287353515625\n",
      "epoch: 2 iter: 72 reconn_loss: 11452.73046875 kl_loss: 1191.9842529296875\n",
      "epoch: 2 iter: 73 reconn_loss: 11607.302734375 kl_loss: 1212.03369140625\n",
      "epoch: 2 iter: 74 reconn_loss: 11223.11328125 kl_loss: 1183.458251953125\n",
      "epoch: 2 iter: 75 reconn_loss: 11479.046875 kl_loss: 1197.2281494140625\n",
      "epoch: 2 iter: 76 reconn_loss: 11857.9755859375 kl_loss: 1237.0146484375\n",
      "epoch: 2 iter: 77 reconn_loss: 11699.0859375 kl_loss: 1126.9349365234375\n",
      "epoch: 2 iter: 78 reconn_loss: 11585.1298828125 kl_loss: 1259.4544677734375\n",
      "epoch: 2 iter: 79 reconn_loss: 11380.0087890625 kl_loss: 1127.439697265625\n",
      "epoch: 2 iter: 80 reconn_loss: 11638.1494140625 kl_loss: 1146.145263671875\n",
      "epoch: 2 iter: 81 reconn_loss: 11317.3095703125 kl_loss: 1200.559814453125\n",
      "epoch: 2 iter: 82 reconn_loss: 11491.8603515625 kl_loss: 1150.0601806640625\n",
      "epoch: 2 iter: 83 reconn_loss: 11769.978515625 kl_loss: 1154.42333984375\n",
      "epoch: 2 iter: 84 reconn_loss: 11651.2197265625 kl_loss: 1155.1669921875\n",
      "epoch: 2 iter: 85 reconn_loss: 11641.0283203125 kl_loss: 1154.9542236328125\n",
      "epoch: 2 iter: 86 reconn_loss: 11300.5703125 kl_loss: 1198.951416015625\n",
      "epoch: 2 iter: 87 reconn_loss: 11685.4658203125 kl_loss: 1182.0888671875\n",
      "epoch: 2 iter: 88 reconn_loss: 11173.05078125 kl_loss: 1210.99365234375\n",
      "epoch: 2 iter: 89 reconn_loss: 11290.6484375 kl_loss: 1201.02978515625\n",
      "epoch: 2 iter: 90 reconn_loss: 11474.26171875 kl_loss: 1145.1788330078125\n",
      "epoch: 2 iter: 91 reconn_loss: 11174.0126953125 kl_loss: 1158.923583984375\n",
      "epoch: 2 iter: 92 reconn_loss: 11578.3173828125 kl_loss: 1127.521240234375\n",
      "epoch: 2 iter: 93 reconn_loss: 11362.1689453125 kl_loss: 1181.375\n",
      "epoch: 2 iter: 94 reconn_loss: 11425.0625 kl_loss: 1306.947509765625\n",
      "epoch: 2 iter: 95 reconn_loss: 11649.8447265625 kl_loss: 1125.2393798828125\n",
      "epoch: 2 iter: 96 reconn_loss: 11465.357421875 kl_loss: 1427.863525390625\n",
      "epoch: 2 iter: 97 reconn_loss: 11243.654296875 kl_loss: 1166.6165771484375\n",
      "epoch: 2 iter: 98 reconn_loss: 11720.806640625 kl_loss: 1230.16845703125\n",
      "epoch: 2 iter: 99 reconn_loss: 11545.19921875 kl_loss: 1231.7122802734375\n",
      "epoch: 2 iter: 100 reconn_loss: 11258.482421875 kl_loss: 1209.4598388671875\n",
      "epoch: 2 iter: 101 reconn_loss: 11367.953125 kl_loss: 1291.9716796875\n",
      "epoch: 2 iter: 102 reconn_loss: 10842.708984375 kl_loss: 1133.39697265625\n",
      "epoch: 2 iter: 103 reconn_loss: 11243.41796875 kl_loss: 1175.9637451171875\n",
      "epoch: 2 iter: 104 reconn_loss: 11365.6279296875 kl_loss: 1199.060791015625\n",
      "epoch: 2 iter: 105 reconn_loss: 11255.3671875 kl_loss: 1265.93408203125\n",
      "epoch: 2 iter: 106 reconn_loss: 11330.080078125 kl_loss: 1287.6151123046875\n",
      "epoch: 2 iter: 107 reconn_loss: 11157.0341796875 kl_loss: 1295.421630859375\n",
      "epoch: 2 iter: 108 reconn_loss: 11441.33984375 kl_loss: 1177.261962890625\n",
      "epoch: 2 iter: 109 reconn_loss: 11369.611328125 kl_loss: 1183.822509765625\n",
      "epoch: 2 iter: 110 reconn_loss: 11585.8984375 kl_loss: 1282.24365234375\n",
      "epoch: 2 iter: 111 reconn_loss: 11130.9736328125 kl_loss: 1272.0611572265625\n",
      "epoch: 2 iter: 112 reconn_loss: 11214.693359375 kl_loss: 1269.00341796875\n",
      "epoch: 2 iter: 113 reconn_loss: 11430.857421875 kl_loss: 1185.2974853515625\n",
      "epoch: 2 iter: 114 reconn_loss: 11163.208984375 kl_loss: 1183.5517578125\n",
      "epoch: 2 iter: 115 reconn_loss: 11482.2119140625 kl_loss: 1221.832763671875\n",
      "epoch: 2 iter: 116 reconn_loss: 11621.3271484375 kl_loss: 1165.8548583984375\n",
      "epoch: 2 iter: 117 reconn_loss: 11486.5078125 kl_loss: 1147.7822265625\n",
      "epoch: 2 iter: 118 reconn_loss: 11394.916015625 kl_loss: 1131.783203125\n",
      "epoch: 2 iter: 119 reconn_loss: 11345.4658203125 kl_loss: 1134.7005615234375\n",
      "epoch: 2 iter: 120 reconn_loss: 10958.728515625 kl_loss: 1215.9224853515625\n",
      "epoch: 2 iter: 121 reconn_loss: 11498.3564453125 kl_loss: 1135.0467529296875\n",
      "epoch: 2 iter: 122 reconn_loss: 11545.384765625 kl_loss: 1096.3187255859375\n",
      "epoch: 2 iter: 123 reconn_loss: 11426.1025390625 kl_loss: 1278.6341552734375\n",
      "epoch: 2 iter: 124 reconn_loss: 11566.763671875 kl_loss: 1217.12548828125\n",
      "epoch: 2 iter: 125 reconn_loss: 11003.3603515625 kl_loss: 1296.3564453125\n",
      "epoch: 2 iter: 126 reconn_loss: 11677.3623046875 kl_loss: 1235.610595703125\n",
      "epoch: 2 iter: 127 reconn_loss: 11029.095703125 kl_loss: 1269.77197265625\n",
      "epoch: 2 iter: 128 reconn_loss: 11294.9853515625 kl_loss: 1241.9385986328125\n",
      "epoch: 2 iter: 129 reconn_loss: 11382.1357421875 kl_loss: 1213.9227294921875\n",
      "epoch: 2 iter: 130 reconn_loss: 11591.0576171875 kl_loss: 1225.9095458984375\n",
      "epoch: 2 iter: 131 reconn_loss: 11532.91015625 kl_loss: 1234.4088134765625\n",
      "epoch: 2 iter: 132 reconn_loss: 11833.4375 kl_loss: 1256.92431640625\n",
      "epoch: 2 iter: 133 reconn_loss: 11283.109375 kl_loss: 1282.7572021484375\n",
      "epoch: 2 iter: 134 reconn_loss: 11363.1953125 kl_loss: 1245.3330078125\n",
      "epoch: 2 iter: 135 reconn_loss: 11370.748046875 kl_loss: 1245.4945068359375\n",
      "epoch: 2 iter: 136 reconn_loss: 11292.13671875 kl_loss: 1254.7886962890625\n",
      "epoch: 2 iter: 137 reconn_loss: 11386.587890625 kl_loss: 1196.9031982421875\n",
      "epoch: 2 iter: 138 reconn_loss: 11157.671875 kl_loss: 1194.3497314453125\n",
      "epoch: 2 iter: 139 reconn_loss: 11370.5830078125 kl_loss: 1198.4189453125\n",
      "epoch: 2 iter: 140 reconn_loss: 11364.486328125 kl_loss: 1203.9949951171875\n",
      "epoch: 2 iter: 141 reconn_loss: 11739.3369140625 kl_loss: 1235.721435546875\n",
      "epoch: 2 iter: 142 reconn_loss: 11410.71875 kl_loss: 1213.5255126953125\n",
      "epoch: 2 iter: 143 reconn_loss: 11290.4853515625 kl_loss: 1240.7786865234375\n",
      "epoch: 2 iter: 144 reconn_loss: 11487.1494140625 kl_loss: 1151.153564453125\n",
      "epoch: 2 iter: 145 reconn_loss: 11268.349609375 kl_loss: 1155.451904296875\n",
      "epoch: 2 iter: 146 reconn_loss: 11506.16796875 kl_loss: 1195.8961181640625\n",
      "epoch: 2 iter: 147 reconn_loss: 11210.001953125 kl_loss: 1248.619384765625\n",
      "epoch: 2 iter: 148 reconn_loss: 11132.9189453125 kl_loss: 1132.231201171875\n",
      "epoch: 2 iter: 149 reconn_loss: 11308.2138671875 kl_loss: 1261.7203369140625\n",
      "epoch: 2 iter: 150 reconn_loss: 11591.287109375 kl_loss: 1180.92626953125\n",
      "epoch: 2 iter: 151 reconn_loss: 11167.7783203125 kl_loss: 1262.34375\n",
      "epoch: 2 iter: 152 reconn_loss: 11585.3115234375 kl_loss: 1228.4761962890625\n",
      "epoch: 2 iter: 153 reconn_loss: 11339.9208984375 kl_loss: 1196.471435546875\n",
      "epoch: 2 iter: 154 reconn_loss: 11419.1171875 kl_loss: 1143.97998046875\n",
      "epoch: 2 iter: 155 reconn_loss: 11341.5322265625 kl_loss: 1149.0140380859375\n",
      "epoch: 2 iter: 156 reconn_loss: 11524.2265625 kl_loss: 1216.101318359375\n",
      "epoch: 2 iter: 157 reconn_loss: 11462.623046875 kl_loss: 1311.52978515625\n",
      "epoch: 2 iter: 158 reconn_loss: 11613.00390625 kl_loss: 1283.207275390625\n",
      "epoch: 2 iter: 159 reconn_loss: 11339.6767578125 kl_loss: 1304.307373046875\n",
      "epoch: 2 iter: 160 reconn_loss: 11339.4345703125 kl_loss: 1223.916748046875\n",
      "epoch: 2 iter: 161 reconn_loss: 11514.111328125 kl_loss: 1336.331298828125\n",
      "epoch: 2 iter: 162 reconn_loss: 11350.505859375 kl_loss: 1266.8370361328125\n",
      "epoch: 2 iter: 163 reconn_loss: 11710.978515625 kl_loss: 1307.6954345703125\n",
      "epoch: 2 iter: 164 reconn_loss: 11255.490234375 kl_loss: 1215.10302734375\n",
      "epoch: 2 iter: 165 reconn_loss: 11013.02734375 kl_loss: 1272.809326171875\n",
      "epoch: 2 iter: 166 reconn_loss: 11886.2333984375 kl_loss: 1212.493408203125\n",
      "epoch: 2 iter: 167 reconn_loss: 11236.189453125 kl_loss: 1234.0799560546875\n",
      "epoch: 2 iter: 168 reconn_loss: 11157.404296875 kl_loss: 1280.4541015625\n",
      "epoch: 2 iter: 169 reconn_loss: 11066.6171875 kl_loss: 1270.7657470703125\n",
      "epoch: 2 iter: 170 reconn_loss: 11253.0263671875 kl_loss: 1257.014404296875\n",
      "epoch: 2 iter: 171 reconn_loss: 11722.2119140625 kl_loss: 1260.172607421875\n",
      "epoch: 2 iter: 172 reconn_loss: 11598.4990234375 kl_loss: 1230.040771484375\n",
      "epoch: 2 iter: 173 reconn_loss: 11401.8876953125 kl_loss: 1196.6434326171875\n",
      "epoch: 2 iter: 174 reconn_loss: 11330.1025390625 kl_loss: 1260.4566650390625\n",
      "epoch: 2 iter: 175 reconn_loss: 11350.380859375 kl_loss: 1216.2523193359375\n",
      "epoch: 2 iter: 176 reconn_loss: 11268.7578125 kl_loss: 1321.11474609375\n",
      "epoch: 2 iter: 177 reconn_loss: 11746.390625 kl_loss: 1239.20263671875\n",
      "epoch: 2 iter: 178 reconn_loss: 11172.20703125 kl_loss: 1316.18798828125\n",
      "epoch: 2 iter: 179 reconn_loss: 11246.5048828125 kl_loss: 1165.2193603515625\n",
      "epoch: 2 iter: 180 reconn_loss: 11309.9609375 kl_loss: 1250.010009765625\n",
      "epoch: 2 iter: 181 reconn_loss: 11125.6943359375 kl_loss: 1204.3524169921875\n",
      "epoch: 2 iter: 182 reconn_loss: 11281.5654296875 kl_loss: 1206.524169921875\n",
      "epoch: 2 iter: 183 reconn_loss: 11069.2119140625 kl_loss: 1139.344970703125\n",
      "epoch: 2 iter: 184 reconn_loss: 11426.1787109375 kl_loss: 1220.245361328125\n",
      "epoch: 2 iter: 185 reconn_loss: 11542.5908203125 kl_loss: 1188.0572509765625\n",
      "epoch: 2 iter: 186 reconn_loss: 10982.462890625 kl_loss: 1189.30712890625\n",
      "epoch: 2 iter: 187 reconn_loss: 11685.982421875 kl_loss: 1169.4188232421875\n",
      "epoch: 2 iter: 188 reconn_loss: 11880.810546875 kl_loss: 1281.884765625\n",
      "epoch: 2 iter: 189 reconn_loss: 11499.0185546875 kl_loss: 1219.1591796875\n",
      "epoch: 2 iter: 190 reconn_loss: 11399.2265625 kl_loss: 1212.79833984375\n",
      "epoch: 2 iter: 191 reconn_loss: 11325.1796875 kl_loss: 1297.72509765625\n",
      "epoch: 2 iter: 192 reconn_loss: 11290.421875 kl_loss: 1177.7943115234375\n",
      "epoch: 2 iter: 193 reconn_loss: 11579.40625 kl_loss: 1295.76318359375\n",
      "epoch: 2 iter: 194 reconn_loss: 11325.931640625 kl_loss: 1301.107421875\n",
      "epoch: 2 iter: 195 reconn_loss: 11002.232421875 kl_loss: 1303.784912109375\n",
      "epoch: 2 iter: 196 reconn_loss: 11728.234375 kl_loss: 1317.0908203125\n",
      "epoch: 2 iter: 197 reconn_loss: 11690.5791015625 kl_loss: 1385.381591796875\n",
      "epoch: 2 iter: 198 reconn_loss: 11843.818359375 kl_loss: 1351.468505859375\n",
      "epoch: 2 iter: 199 reconn_loss: 11424.25390625 kl_loss: 1323.1009521484375\n",
      "epoch: 2 iter: 200 reconn_loss: 11254.0234375 kl_loss: 1239.70458984375\n",
      "epoch: 2 iter: 201 reconn_loss: 11286.32421875 kl_loss: 1284.228271484375\n",
      "epoch: 2 iter: 202 reconn_loss: 11398.548828125 kl_loss: 1357.8751220703125\n",
      "epoch: 2 iter: 203 reconn_loss: 11185.5625 kl_loss: 1246.52197265625\n",
      "epoch: 2 iter: 204 reconn_loss: 11359.0498046875 kl_loss: 1211.2197265625\n",
      "epoch: 2 iter: 205 reconn_loss: 11266.2060546875 kl_loss: 1191.3018798828125\n",
      "epoch: 2 iter: 206 reconn_loss: 10723.955078125 kl_loss: 1341.8448486328125\n",
      "epoch: 2 iter: 207 reconn_loss: 11335.3173828125 kl_loss: 1310.110595703125\n",
      "epoch: 2 iter: 208 reconn_loss: 11400.3115234375 kl_loss: 1262.3922119140625\n",
      "epoch: 2 iter: 209 reconn_loss: 11324.158203125 kl_loss: 1222.1058349609375\n",
      "epoch: 2 iter: 210 reconn_loss: 11383.9482421875 kl_loss: 1253.307861328125\n",
      "epoch: 2 iter: 211 reconn_loss: 11208.4677734375 kl_loss: 1245.6455078125\n",
      "epoch: 2 iter: 212 reconn_loss: 11249.240234375 kl_loss: 1292.26220703125\n",
      "epoch: 2 iter: 213 reconn_loss: 11307.66015625 kl_loss: 1191.6719970703125\n",
      "epoch: 2 iter: 214 reconn_loss: 11084.12109375 kl_loss: 1252.4801025390625\n",
      "epoch: 2 iter: 215 reconn_loss: 11441.03125 kl_loss: 1214.4669189453125\n",
      "epoch: 2 iter: 216 reconn_loss: 11387.009765625 kl_loss: 1260.806396484375\n",
      "epoch: 2 iter: 217 reconn_loss: 11514.51171875 kl_loss: 1322.53369140625\n",
      "epoch: 2 iter: 218 reconn_loss: 11102.818359375 kl_loss: 1242.75634765625\n",
      "epoch: 2 iter: 219 reconn_loss: 11722.751953125 kl_loss: 1294.9805908203125\n",
      "epoch: 2 iter: 220 reconn_loss: 11308.69921875 kl_loss: 1303.08837890625\n",
      "epoch: 2 iter: 221 reconn_loss: 11825.212890625 kl_loss: 1176.87548828125\n",
      "epoch: 2 iter: 222 reconn_loss: 7575.279296875 kl_loss: 796.721923828125\n",
      "0.weight tensor(53.1730) tensor(-51.3508)\n",
      "0.bias tensor(17.5488) tensor(-34.6648)\n",
      "2.weight tensor(13.4572) tensor(-13.9517)\n",
      "2.bias tensor(6.0048) tensor(-6.3301)\n",
      "4.weight tensor(7.0716) tensor(-7.4387)\n",
      "4.bias tensor(5.2690) tensor(-5.7666)\n",
      "epoch: 3 iter: 0 reconn_loss: 11460.8076171875 kl_loss: 1207.3170166015625\n",
      "epoch: 3 iter: 1 reconn_loss: 11564.9912109375 kl_loss: 1166.200439453125\n",
      "epoch: 3 iter: 2 reconn_loss: 11043.59765625 kl_loss: 1197.9599609375\n",
      "epoch: 3 iter: 3 reconn_loss: 11345.91796875 kl_loss: 1271.0257568359375\n",
      "epoch: 3 iter: 4 reconn_loss: 11579.6767578125 kl_loss: 1230.733642578125\n",
      "epoch: 3 iter: 5 reconn_loss: 11377.1220703125 kl_loss: 1207.6685791015625\n",
      "epoch: 3 iter: 6 reconn_loss: 10922.279296875 kl_loss: 1245.50341796875\n",
      "epoch: 3 iter: 7 reconn_loss: 11285.5595703125 kl_loss: 1237.8135986328125\n",
      "epoch: 3 iter: 8 reconn_loss: 11517.896484375 kl_loss: 1315.336181640625\n",
      "epoch: 3 iter: 9 reconn_loss: 11265.3154296875 kl_loss: 1223.520751953125\n",
      "epoch: 3 iter: 10 reconn_loss: 10865.0751953125 kl_loss: 1309.463134765625\n",
      "epoch: 3 iter: 11 reconn_loss: 11435.763671875 kl_loss: 1313.172119140625\n",
      "epoch: 3 iter: 12 reconn_loss: 10966.1455078125 kl_loss: 1267.0404052734375\n",
      "epoch: 3 iter: 13 reconn_loss: 11019.98828125 kl_loss: 1381.79443359375\n",
      "epoch: 3 iter: 14 reconn_loss: 11443.5673828125 kl_loss: 1437.1610107421875\n",
      "epoch: 3 iter: 15 reconn_loss: 11306.6240234375 kl_loss: 1329.1378173828125\n",
      "epoch: 3 iter: 16 reconn_loss: 10952.1044921875 kl_loss: 1295.745849609375\n",
      "epoch: 3 iter: 17 reconn_loss: 11415.1904296875 kl_loss: 1320.942626953125\n",
      "epoch: 3 iter: 18 reconn_loss: 11357.8251953125 kl_loss: 1399.2431640625\n",
      "epoch: 3 iter: 19 reconn_loss: 11438.083984375 kl_loss: 1271.9073486328125\n",
      "epoch: 3 iter: 20 reconn_loss: 11259.384765625 kl_loss: 1267.7890625\n",
      "epoch: 3 iter: 21 reconn_loss: 11298.68359375 kl_loss: 1215.887939453125\n",
      "epoch: 3 iter: 22 reconn_loss: 11579.669921875 kl_loss: 1288.5289306640625\n",
      "epoch: 3 iter: 23 reconn_loss: 11146.8466796875 kl_loss: 1241.62109375\n",
      "epoch: 3 iter: 24 reconn_loss: 11211.119140625 kl_loss: 1237.989990234375\n",
      "epoch: 3 iter: 25 reconn_loss: 11174.541015625 kl_loss: 1261.317138671875\n",
      "epoch: 3 iter: 26 reconn_loss: 11113.037109375 kl_loss: 1337.941650390625\n",
      "epoch: 3 iter: 27 reconn_loss: 11403.0185546875 kl_loss: 1240.3311767578125\n",
      "epoch: 3 iter: 28 reconn_loss: 11463.662109375 kl_loss: 1303.78271484375\n",
      "epoch: 3 iter: 29 reconn_loss: 11341.4111328125 kl_loss: 1312.8726806640625\n",
      "epoch: 3 iter: 30 reconn_loss: 11338.216796875 kl_loss: 1267.8916015625\n",
      "epoch: 3 iter: 31 reconn_loss: 11533.6982421875 kl_loss: 1277.475341796875\n",
      "epoch: 3 iter: 32 reconn_loss: 11587.29296875 kl_loss: 1270.0135498046875\n",
      "epoch: 3 iter: 33 reconn_loss: 11195.9130859375 kl_loss: 1331.3837890625\n",
      "epoch: 3 iter: 34 reconn_loss: 11147.7744140625 kl_loss: 1369.4869384765625\n",
      "epoch: 3 iter: 35 reconn_loss: 11403.986328125 kl_loss: 1352.15478515625\n",
      "epoch: 3 iter: 36 reconn_loss: 11360.8505859375 kl_loss: 1341.099609375\n",
      "epoch: 3 iter: 37 reconn_loss: 11168.3046875 kl_loss: 1300.5216064453125\n",
      "epoch: 3 iter: 38 reconn_loss: 10867.025390625 kl_loss: 1307.70556640625\n",
      "epoch: 3 iter: 39 reconn_loss: 11446.263671875 kl_loss: 1239.0169677734375\n",
      "epoch: 3 iter: 40 reconn_loss: 11049.97265625 kl_loss: 1376.78759765625\n",
      "epoch: 3 iter: 41 reconn_loss: 11236.4921875 kl_loss: 1156.784912109375\n",
      "epoch: 3 iter: 42 reconn_loss: 11304.880859375 kl_loss: 1274.4747314453125\n",
      "epoch: 3 iter: 43 reconn_loss: 11452.5703125 kl_loss: 1244.47119140625\n",
      "epoch: 3 iter: 44 reconn_loss: 10715.3203125 kl_loss: 1267.215087890625\n",
      "epoch: 3 iter: 45 reconn_loss: 11530.48046875 kl_loss: 1324.7518310546875\n",
      "epoch: 3 iter: 46 reconn_loss: 11202.25 kl_loss: 1266.8594970703125\n",
      "epoch: 3 iter: 47 reconn_loss: 11345.5625 kl_loss: 1212.2645263671875\n",
      "epoch: 3 iter: 48 reconn_loss: 11151.107421875 kl_loss: 1314.41162109375\n",
      "epoch: 3 iter: 49 reconn_loss: 11141.580078125 kl_loss: 1150.05322265625\n",
      "epoch: 3 iter: 50 reconn_loss: 11391.3857421875 kl_loss: 1216.268798828125\n",
      "epoch: 3 iter: 51 reconn_loss: 10625.9296875 kl_loss: 1316.04443359375\n",
      "epoch: 3 iter: 52 reconn_loss: 11442.7353515625 kl_loss: 1234.649169921875\n",
      "epoch: 3 iter: 53 reconn_loss: 10909.5126953125 kl_loss: 1326.294189453125\n",
      "epoch: 3 iter: 54 reconn_loss: 11508.0556640625 kl_loss: 1347.5267333984375\n",
      "epoch: 3 iter: 55 reconn_loss: 11195.548828125 kl_loss: 1258.44287109375\n",
      "epoch: 3 iter: 56 reconn_loss: 11337.697265625 kl_loss: 1286.28076171875\n",
      "epoch: 3 iter: 57 reconn_loss: 11325.818359375 kl_loss: 1307.1842041015625\n",
      "epoch: 3 iter: 58 reconn_loss: 11436.0048828125 kl_loss: 1226.9063720703125\n",
      "epoch: 3 iter: 59 reconn_loss: 11453.474609375 kl_loss: 1187.6866455078125\n",
      "epoch: 3 iter: 60 reconn_loss: 11118.8095703125 kl_loss: 1293.696044921875\n",
      "epoch: 3 iter: 61 reconn_loss: 11295.779296875 kl_loss: 1315.9996337890625\n",
      "epoch: 3 iter: 62 reconn_loss: 11293.7265625 kl_loss: 1273.603515625\n",
      "epoch: 3 iter: 63 reconn_loss: 11332.1630859375 kl_loss: 1238.3795166015625\n",
      "epoch: 3 iter: 64 reconn_loss: 11165.9580078125 kl_loss: 1267.4271240234375\n",
      "epoch: 3 iter: 65 reconn_loss: 11180.564453125 kl_loss: 1261.2091064453125\n",
      "epoch: 3 iter: 66 reconn_loss: 11049.484375 kl_loss: 1254.33740234375\n",
      "epoch: 3 iter: 67 reconn_loss: 11434.8779296875 kl_loss: 1346.923095703125\n",
      "epoch: 3 iter: 68 reconn_loss: 11332.703125 kl_loss: 1286.63134765625\n",
      "epoch: 3 iter: 69 reconn_loss: 11245.4921875 kl_loss: 1247.8359375\n",
      "epoch: 3 iter: 70 reconn_loss: 11068.2861328125 kl_loss: 1329.2647705078125\n",
      "epoch: 3 iter: 71 reconn_loss: 11256.78515625 kl_loss: 1376.9842529296875\n",
      "epoch: 3 iter: 72 reconn_loss: 11426.7919921875 kl_loss: 1194.1002197265625\n",
      "epoch: 3 iter: 73 reconn_loss: 11245.330078125 kl_loss: 1288.925048828125\n",
      "epoch: 3 iter: 74 reconn_loss: 10941.7646484375 kl_loss: 1293.148681640625\n",
      "epoch: 3 iter: 75 reconn_loss: 10973.9775390625 kl_loss: 1317.858154296875\n",
      "epoch: 3 iter: 76 reconn_loss: 11133.537109375 kl_loss: 1279.39111328125\n",
      "epoch: 3 iter: 77 reconn_loss: 11480.029296875 kl_loss: 1346.2890625\n",
      "epoch: 3 iter: 78 reconn_loss: 11410.2705078125 kl_loss: 1320.853271484375\n",
      "epoch: 3 iter: 79 reconn_loss: 11283.87109375 kl_loss: 1314.39404296875\n",
      "epoch: 3 iter: 80 reconn_loss: 11341.2880859375 kl_loss: 1400.943359375\n",
      "epoch: 3 iter: 81 reconn_loss: 11119.1259765625 kl_loss: 1250.2222900390625\n",
      "epoch: 3 iter: 82 reconn_loss: 11324.4384765625 kl_loss: 1282.2528076171875\n",
      "epoch: 3 iter: 83 reconn_loss: 11142.0341796875 kl_loss: 1223.089599609375\n",
      "epoch: 3 iter: 84 reconn_loss: 10831.146484375 kl_loss: 1274.3031005859375\n",
      "epoch: 3 iter: 85 reconn_loss: 11245.568359375 kl_loss: 1258.32666015625\n",
      "epoch: 3 iter: 86 reconn_loss: 11443.669921875 kl_loss: 1282.864990234375\n",
      "epoch: 3 iter: 87 reconn_loss: 10837.51953125 kl_loss: 1280.232666015625\n",
      "epoch: 3 iter: 88 reconn_loss: 11629.591796875 kl_loss: 1320.00634765625\n",
      "epoch: 3 iter: 89 reconn_loss: 11308.552734375 kl_loss: 1257.9744873046875\n",
      "epoch: 3 iter: 90 reconn_loss: 10916.1806640625 kl_loss: 1331.4970703125\n",
      "epoch: 3 iter: 91 reconn_loss: 11197.53125 kl_loss: 1306.950439453125\n",
      "epoch: 3 iter: 92 reconn_loss: 11041.6640625 kl_loss: 1367.697265625\n",
      "epoch: 3 iter: 93 reconn_loss: 11535.7021484375 kl_loss: 1322.6239013671875\n",
      "epoch: 3 iter: 94 reconn_loss: 11651.3740234375 kl_loss: 1306.1356201171875\n",
      "epoch: 3 iter: 95 reconn_loss: 10853.787109375 kl_loss: 1272.1961669921875\n",
      "epoch: 3 iter: 96 reconn_loss: 11502.1455078125 kl_loss: 1353.8001708984375\n",
      "epoch: 3 iter: 97 reconn_loss: 11305.314453125 kl_loss: 1234.818359375\n",
      "epoch: 3 iter: 98 reconn_loss: 11212.4375 kl_loss: 1381.016845703125\n",
      "epoch: 3 iter: 99 reconn_loss: 10964.703125 kl_loss: 1361.9739990234375\n",
      "epoch: 3 iter: 100 reconn_loss: 11205.9189453125 kl_loss: 1351.5777587890625\n",
      "epoch: 3 iter: 101 reconn_loss: 11205.4345703125 kl_loss: 1349.8748779296875\n",
      "epoch: 3 iter: 102 reconn_loss: 11195.30078125 kl_loss: 1267.3026123046875\n",
      "epoch: 3 iter: 103 reconn_loss: 11123.6640625 kl_loss: 1279.98974609375\n",
      "epoch: 3 iter: 104 reconn_loss: 11452.158203125 kl_loss: 1289.2734375\n",
      "epoch: 3 iter: 105 reconn_loss: 11095.2958984375 kl_loss: 1285.036865234375\n",
      "epoch: 3 iter: 106 reconn_loss: 11146.4912109375 kl_loss: 1274.270751953125\n",
      "epoch: 3 iter: 107 reconn_loss: 11053.623046875 kl_loss: 1266.44970703125\n",
      "epoch: 3 iter: 108 reconn_loss: 11139.65234375 kl_loss: 1336.3720703125\n",
      "epoch: 3 iter: 109 reconn_loss: 10963.466796875 kl_loss: 1227.440185546875\n",
      "epoch: 3 iter: 110 reconn_loss: 11099.021484375 kl_loss: 1305.4193115234375\n",
      "epoch: 3 iter: 111 reconn_loss: 11134.04296875 kl_loss: 1261.3616943359375\n",
      "epoch: 3 iter: 112 reconn_loss: 11193.4189453125 kl_loss: 1341.0003662109375\n",
      "epoch: 3 iter: 113 reconn_loss: 11455.265625 kl_loss: 1271.10986328125\n",
      "epoch: 3 iter: 114 reconn_loss: 11103.4619140625 kl_loss: 1314.1783447265625\n",
      "epoch: 3 iter: 115 reconn_loss: 11264.845703125 kl_loss: 1190.67626953125\n",
      "epoch: 3 iter: 116 reconn_loss: 11040.6865234375 kl_loss: 1218.010986328125\n",
      "epoch: 3 iter: 117 reconn_loss: 11495.759765625 kl_loss: 1380.27490234375\n",
      "epoch: 3 iter: 118 reconn_loss: 10865.748046875 kl_loss: 1360.0909423828125\n",
      "epoch: 3 iter: 119 reconn_loss: 11166.046875 kl_loss: 1292.8681640625\n",
      "epoch: 3 iter: 120 reconn_loss: 10839.12890625 kl_loss: 1240.5516357421875\n",
      "epoch: 3 iter: 121 reconn_loss: 10856.798828125 kl_loss: 1327.3653564453125\n",
      "epoch: 3 iter: 122 reconn_loss: 11399.0546875 kl_loss: 1207.646484375\n",
      "epoch: 3 iter: 123 reconn_loss: 11434.2978515625 kl_loss: 1349.5718994140625\n",
      "epoch: 3 iter: 124 reconn_loss: 10883.3974609375 kl_loss: 1341.2327880859375\n",
      "epoch: 3 iter: 125 reconn_loss: 11188.7431640625 kl_loss: 1342.5648193359375\n",
      "epoch: 3 iter: 126 reconn_loss: 11127.75 kl_loss: 1275.4259033203125\n",
      "epoch: 3 iter: 127 reconn_loss: 10884.931640625 kl_loss: 1338.689208984375\n",
      "epoch: 3 iter: 128 reconn_loss: 11419.263671875 kl_loss: 1291.6988525390625\n",
      "epoch: 3 iter: 129 reconn_loss: 10998.6328125 kl_loss: 1354.613525390625\n",
      "epoch: 3 iter: 130 reconn_loss: 11399.1669921875 kl_loss: 1425.4857177734375\n",
      "epoch: 3 iter: 131 reconn_loss: 11062.541015625 kl_loss: 1234.377197265625\n",
      "epoch: 3 iter: 132 reconn_loss: 11043.2353515625 kl_loss: 1345.571533203125\n",
      "epoch: 3 iter: 133 reconn_loss: 11231.77734375 kl_loss: 1329.468994140625\n",
      "epoch: 3 iter: 134 reconn_loss: 11287.99609375 kl_loss: 1377.119384765625\n",
      "epoch: 3 iter: 135 reconn_loss: 11624.580078125 kl_loss: 1272.5911865234375\n",
      "epoch: 3 iter: 136 reconn_loss: 11122.7392578125 kl_loss: 1355.1549072265625\n",
      "epoch: 3 iter: 137 reconn_loss: 11341.458984375 kl_loss: 1346.625244140625\n",
      "epoch: 3 iter: 138 reconn_loss: 11322.1904296875 kl_loss: 1273.1488037109375\n",
      "epoch: 3 iter: 139 reconn_loss: 11185.17578125 kl_loss: 1256.312744140625\n",
      "epoch: 3 iter: 140 reconn_loss: 11459.0712890625 kl_loss: 1401.432861328125\n",
      "epoch: 3 iter: 141 reconn_loss: 10971.12109375 kl_loss: 1389.875\n",
      "epoch: 3 iter: 142 reconn_loss: 11210.896484375 kl_loss: 1316.6727294921875\n",
      "epoch: 3 iter: 143 reconn_loss: 11453.044921875 kl_loss: 1301.69677734375\n",
      "epoch: 3 iter: 144 reconn_loss: 11036.099609375 kl_loss: 1290.6334228515625\n",
      "epoch: 3 iter: 145 reconn_loss: 11211.58203125 kl_loss: 1359.034912109375\n",
      "epoch: 3 iter: 146 reconn_loss: 11133.796875 kl_loss: 1293.881103515625\n",
      "epoch: 3 iter: 147 reconn_loss: 11396.423828125 kl_loss: 1287.6044921875\n",
      "epoch: 3 iter: 148 reconn_loss: 11223.474609375 kl_loss: 1331.25244140625\n",
      "epoch: 3 iter: 149 reconn_loss: 11220.0234375 kl_loss: 1300.97705078125\n",
      "epoch: 3 iter: 150 reconn_loss: 10876.37890625 kl_loss: 1240.20458984375\n",
      "epoch: 3 iter: 151 reconn_loss: 11385.005859375 kl_loss: 1344.76611328125\n",
      "epoch: 3 iter: 152 reconn_loss: 11360.9140625 kl_loss: 1339.8819580078125\n",
      "epoch: 3 iter: 153 reconn_loss: 11347.9326171875 kl_loss: 1318.8759765625\n",
      "epoch: 3 iter: 154 reconn_loss: 10850.5146484375 kl_loss: 1291.0458984375\n",
      "epoch: 3 iter: 155 reconn_loss: 11046.0771484375 kl_loss: 1278.8404541015625\n",
      "epoch: 3 iter: 156 reconn_loss: 11262.318359375 kl_loss: 1188.9256591796875\n",
      "epoch: 3 iter: 157 reconn_loss: 11528.759765625 kl_loss: 1348.14697265625\n",
      "epoch: 3 iter: 158 reconn_loss: 11171.400390625 kl_loss: 1310.12841796875\n",
      "epoch: 3 iter: 159 reconn_loss: 11322.427734375 kl_loss: 1393.95849609375\n",
      "epoch: 3 iter: 160 reconn_loss: 11421.671875 kl_loss: 1350.876953125\n",
      "epoch: 3 iter: 161 reconn_loss: 10817.0693359375 kl_loss: 1392.0201416015625\n",
      "epoch: 3 iter: 162 reconn_loss: 11407.4765625 kl_loss: 1414.8330078125\n",
      "epoch: 3 iter: 163 reconn_loss: 10928.8759765625 kl_loss: 1330.114501953125\n",
      "epoch: 3 iter: 164 reconn_loss: 10791.501953125 kl_loss: 1313.2838134765625\n",
      "epoch: 3 iter: 165 reconn_loss: 11145.98828125 kl_loss: 1304.650146484375\n",
      "epoch: 3 iter: 166 reconn_loss: 11199.279296875 kl_loss: 1280.55029296875\n",
      "epoch: 3 iter: 167 reconn_loss: 11109.853515625 kl_loss: 1225.40087890625\n",
      "epoch: 3 iter: 168 reconn_loss: 11098.1416015625 kl_loss: 1317.3233642578125\n",
      "epoch: 3 iter: 169 reconn_loss: 11318.95703125 kl_loss: 1309.41552734375\n",
      "epoch: 3 iter: 170 reconn_loss: 11088.494140625 kl_loss: 1413.584228515625\n",
      "epoch: 3 iter: 171 reconn_loss: 11222.84375 kl_loss: 1334.16455078125\n",
      "epoch: 3 iter: 172 reconn_loss: 11195.94140625 kl_loss: 1396.12646484375\n",
      "epoch: 3 iter: 173 reconn_loss: 11288.0908203125 kl_loss: 1286.141357421875\n",
      "epoch: 3 iter: 174 reconn_loss: 11236.265625 kl_loss: 1350.478759765625\n",
      "epoch: 3 iter: 175 reconn_loss: 11589.6669921875 kl_loss: 1255.5675048828125\n",
      "epoch: 3 iter: 176 reconn_loss: 11151.7548828125 kl_loss: 1287.4364013671875\n",
      "epoch: 3 iter: 177 reconn_loss: 11126.3564453125 kl_loss: 1398.8978271484375\n",
      "epoch: 3 iter: 178 reconn_loss: 11128.0712890625 kl_loss: 1315.472412109375\n",
      "epoch: 3 iter: 179 reconn_loss: 11042.0 kl_loss: 1369.82861328125\n",
      "epoch: 3 iter: 180 reconn_loss: 10669.853515625 kl_loss: 1307.90869140625\n",
      "epoch: 3 iter: 181 reconn_loss: 11026.0361328125 kl_loss: 1297.2684326171875\n",
      "epoch: 3 iter: 182 reconn_loss: 10570.388671875 kl_loss: 1353.9215087890625\n",
      "epoch: 3 iter: 183 reconn_loss: 11461.974609375 kl_loss: 1360.26318359375\n",
      "epoch: 3 iter: 184 reconn_loss: 11277.3525390625 kl_loss: 1348.6143798828125\n",
      "epoch: 3 iter: 185 reconn_loss: 11247.548828125 kl_loss: 1355.9140625\n",
      "epoch: 3 iter: 186 reconn_loss: 10840.6298828125 kl_loss: 1387.527099609375\n",
      "epoch: 3 iter: 187 reconn_loss: 11261.537109375 kl_loss: 1282.47607421875\n",
      "epoch: 3 iter: 188 reconn_loss: 11374.578125 kl_loss: 1355.166259765625\n",
      "epoch: 3 iter: 189 reconn_loss: 11429.9072265625 kl_loss: 1311.116943359375\n",
      "epoch: 3 iter: 190 reconn_loss: 11246.958984375 kl_loss: 1386.102783203125\n",
      "epoch: 3 iter: 191 reconn_loss: 10787.9580078125 kl_loss: 1334.53125\n",
      "epoch: 3 iter: 192 reconn_loss: 11270.46484375 kl_loss: 1390.18994140625\n",
      "epoch: 3 iter: 193 reconn_loss: 11071.9765625 kl_loss: 1380.102294921875\n",
      "epoch: 3 iter: 194 reconn_loss: 10895.865234375 kl_loss: 1337.6158447265625\n",
      "epoch: 3 iter: 195 reconn_loss: 10944.15234375 kl_loss: 1261.7572021484375\n",
      "epoch: 3 iter: 196 reconn_loss: 11009.0234375 kl_loss: 1337.13232421875\n",
      "epoch: 3 iter: 197 reconn_loss: 11136.853515625 kl_loss: 1296.577392578125\n",
      "epoch: 3 iter: 198 reconn_loss: 11098.2802734375 kl_loss: 1326.0233154296875\n",
      "epoch: 3 iter: 199 reconn_loss: 10892.45703125 kl_loss: 1358.523193359375\n",
      "epoch: 3 iter: 200 reconn_loss: 11210.052734375 kl_loss: 1348.15771484375\n",
      "epoch: 3 iter: 201 reconn_loss: 11075.7529296875 kl_loss: 1298.8486328125\n",
      "epoch: 3 iter: 202 reconn_loss: 11075.263671875 kl_loss: 1342.0872802734375\n",
      "epoch: 3 iter: 203 reconn_loss: 11115.25 kl_loss: 1283.415771484375\n",
      "epoch: 3 iter: 204 reconn_loss: 11243.716796875 kl_loss: 1328.849365234375\n",
      "epoch: 3 iter: 205 reconn_loss: 10891.4697265625 kl_loss: 1306.6356201171875\n",
      "epoch: 3 iter: 206 reconn_loss: 10724.748046875 kl_loss: 1311.3184814453125\n",
      "epoch: 3 iter: 207 reconn_loss: 11284.6748046875 kl_loss: 1363.737548828125\n",
      "epoch: 3 iter: 208 reconn_loss: 10785.03515625 kl_loss: 1378.3927001953125\n",
      "epoch: 3 iter: 209 reconn_loss: 11570.7529296875 kl_loss: 1448.7410888671875\n",
      "epoch: 3 iter: 210 reconn_loss: 11200.14453125 kl_loss: 1424.857421875\n",
      "epoch: 3 iter: 211 reconn_loss: 10796.076171875 kl_loss: 1321.251708984375\n",
      "epoch: 3 iter: 212 reconn_loss: 11527.548828125 kl_loss: 1365.4683837890625\n",
      "epoch: 3 iter: 213 reconn_loss: 10823.6494140625 kl_loss: 1345.2041015625\n",
      "epoch: 3 iter: 214 reconn_loss: 10854.48046875 kl_loss: 1313.9381103515625\n",
      "epoch: 3 iter: 215 reconn_loss: 11217.943359375 kl_loss: 1367.827880859375\n",
      "epoch: 3 iter: 216 reconn_loss: 10913.60546875 kl_loss: 1250.07568359375\n",
      "epoch: 3 iter: 217 reconn_loss: 11014.40234375 kl_loss: 1348.869140625\n",
      "epoch: 3 iter: 218 reconn_loss: 11220.447265625 kl_loss: 1243.5296630859375\n",
      "epoch: 3 iter: 219 reconn_loss: 10966.9521484375 kl_loss: 1341.0589599609375\n",
      "epoch: 3 iter: 220 reconn_loss: 11135.578125 kl_loss: 1322.999755859375\n",
      "epoch: 3 iter: 221 reconn_loss: 11230.8173828125 kl_loss: 1350.533935546875\n",
      "epoch: 3 iter: 222 reconn_loss: 7339.33740234375 kl_loss: 900.3536987304688\n",
      "0.weight tensor(50.4083) tensor(-61.0525)\n",
      "0.bias tensor(13.3663) tensor(-33.2898)\n",
      "2.weight tensor(20.0113) tensor(-10.0484)\n",
      "2.bias tensor(12.5433) tensor(-6.3336)\n",
      "4.weight tensor(10.1235) tensor(-7.7917)\n",
      "4.bias tensor(4.1575) tensor(-4.8244)\n",
      "epoch: 4 iter: 0 reconn_loss: 10976.0390625 kl_loss: 1461.57763671875\n",
      "epoch: 4 iter: 1 reconn_loss: 10823.845703125 kl_loss: 1340.85400390625\n",
      "epoch: 4 iter: 2 reconn_loss: 11611.6328125 kl_loss: 1433.860595703125\n",
      "epoch: 4 iter: 3 reconn_loss: 11230.6533203125 kl_loss: 1453.8560791015625\n",
      "epoch: 4 iter: 4 reconn_loss: 11108.615234375 kl_loss: 1302.3585205078125\n",
      "epoch: 4 iter: 5 reconn_loss: 10770.412109375 kl_loss: 1275.2833251953125\n",
      "epoch: 4 iter: 6 reconn_loss: 11092.8642578125 kl_loss: 1344.5660400390625\n",
      "epoch: 4 iter: 7 reconn_loss: 10991.0625 kl_loss: 1283.5697021484375\n",
      "epoch: 4 iter: 8 reconn_loss: 10728.1533203125 kl_loss: 1347.7901611328125\n",
      "epoch: 4 iter: 9 reconn_loss: 11048.033203125 kl_loss: 1291.7786865234375\n",
      "epoch: 4 iter: 10 reconn_loss: 10986.4716796875 kl_loss: 1380.8388671875\n",
      "epoch: 4 iter: 11 reconn_loss: 10409.8740234375 kl_loss: 1373.8134765625\n",
      "epoch: 4 iter: 12 reconn_loss: 11047.650390625 kl_loss: 1399.79443359375\n",
      "epoch: 4 iter: 13 reconn_loss: 11172.673828125 kl_loss: 1279.193115234375\n",
      "epoch: 4 iter: 14 reconn_loss: 11208.7236328125 kl_loss: 1442.5955810546875\n",
      "epoch: 4 iter: 15 reconn_loss: 11084.5986328125 kl_loss: 1328.203125\n",
      "epoch: 4 iter: 16 reconn_loss: 11180.8232421875 kl_loss: 1298.0528564453125\n",
      "epoch: 4 iter: 17 reconn_loss: 11292.81640625 kl_loss: 1316.2574462890625\n",
      "epoch: 4 iter: 18 reconn_loss: 10925.84765625 kl_loss: 1372.1700439453125\n",
      "epoch: 4 iter: 19 reconn_loss: 11092.4912109375 kl_loss: 1255.9700927734375\n",
      "epoch: 4 iter: 20 reconn_loss: 10828.6552734375 kl_loss: 1318.981201171875\n",
      "epoch: 4 iter: 21 reconn_loss: 10940.71875 kl_loss: 1305.783447265625\n",
      "epoch: 4 iter: 22 reconn_loss: 10937.4775390625 kl_loss: 1303.1763916015625\n",
      "epoch: 4 iter: 23 reconn_loss: 11174.873046875 kl_loss: 1364.2471923828125\n",
      "epoch: 4 iter: 24 reconn_loss: 10748.833984375 kl_loss: 1411.20458984375\n",
      "epoch: 4 iter: 25 reconn_loss: 10731.4296875 kl_loss: 1383.2730712890625\n",
      "epoch: 4 iter: 26 reconn_loss: 10857.0771484375 kl_loss: 1387.7568359375\n",
      "epoch: 4 iter: 27 reconn_loss: 10900.0517578125 kl_loss: 1324.5146484375\n",
      "epoch: 4 iter: 28 reconn_loss: 10932.341796875 kl_loss: 1442.538818359375\n",
      "epoch: 4 iter: 29 reconn_loss: 11408.2978515625 kl_loss: 1306.114013671875\n",
      "epoch: 4 iter: 30 reconn_loss: 11323.8408203125 kl_loss: 1310.1243896484375\n",
      "epoch: 4 iter: 31 reconn_loss: 11243.693359375 kl_loss: 1303.8509521484375\n",
      "epoch: 4 iter: 32 reconn_loss: 10913.4814453125 kl_loss: 1358.1556396484375\n",
      "epoch: 4 iter: 33 reconn_loss: 11191.841796875 kl_loss: 1344.099365234375\n",
      "epoch: 4 iter: 34 reconn_loss: 11105.5732421875 kl_loss: 1360.188720703125\n",
      "epoch: 4 iter: 35 reconn_loss: 11008.79296875 kl_loss: 1349.87255859375\n",
      "epoch: 4 iter: 36 reconn_loss: 10810.78125 kl_loss: 1325.520263671875\n",
      "epoch: 4 iter: 37 reconn_loss: 11244.939453125 kl_loss: 1364.8912353515625\n",
      "epoch: 4 iter: 38 reconn_loss: 10949.68359375 kl_loss: 1393.07958984375\n",
      "epoch: 4 iter: 39 reconn_loss: 11251.865234375 kl_loss: 1337.642822265625\n",
      "epoch: 4 iter: 40 reconn_loss: 11019.767578125 kl_loss: 1294.850341796875\n",
      "epoch: 4 iter: 41 reconn_loss: 10962.0146484375 kl_loss: 1310.8563232421875\n",
      "epoch: 4 iter: 42 reconn_loss: 11194.6875 kl_loss: 1298.645263671875\n",
      "epoch: 4 iter: 43 reconn_loss: 10980.3017578125 kl_loss: 1391.117919921875\n",
      "epoch: 4 iter: 44 reconn_loss: 11196.4833984375 kl_loss: 1285.544921875\n",
      "epoch: 4 iter: 45 reconn_loss: 11058.93359375 kl_loss: 1391.3226318359375\n",
      "epoch: 4 iter: 46 reconn_loss: 11084.669921875 kl_loss: 1349.6728515625\n",
      "epoch: 4 iter: 47 reconn_loss: 11357.212890625 kl_loss: 1340.169189453125\n",
      "epoch: 4 iter: 48 reconn_loss: 11074.728515625 kl_loss: 1414.9615478515625\n",
      "epoch: 4 iter: 49 reconn_loss: 11219.923828125 kl_loss: 1311.6483154296875\n",
      "epoch: 4 iter: 50 reconn_loss: 11358.583984375 kl_loss: 1326.17529296875\n",
      "epoch: 4 iter: 51 reconn_loss: 10969.2578125 kl_loss: 1352.761474609375\n",
      "epoch: 4 iter: 52 reconn_loss: 10985.0400390625 kl_loss: 1375.8992919921875\n",
      "epoch: 4 iter: 53 reconn_loss: 10927.150390625 kl_loss: 1395.1773681640625\n",
      "epoch: 4 iter: 54 reconn_loss: 11087.5390625 kl_loss: 1360.6063232421875\n",
      "epoch: 4 iter: 55 reconn_loss: 10574.0400390625 kl_loss: 1369.6983642578125\n",
      "epoch: 4 iter: 56 reconn_loss: 11025.109375 kl_loss: 1408.072021484375\n",
      "epoch: 4 iter: 57 reconn_loss: 10899.6474609375 kl_loss: 1324.811767578125\n",
      "epoch: 4 iter: 58 reconn_loss: 11270.3330078125 kl_loss: 1421.8118896484375\n",
      "epoch: 4 iter: 59 reconn_loss: 10932.0029296875 kl_loss: 1410.850341796875\n",
      "epoch: 4 iter: 60 reconn_loss: 10955.0693359375 kl_loss: 1420.3433837890625\n",
      "epoch: 4 iter: 61 reconn_loss: 10830.26171875 kl_loss: 1344.81396484375\n",
      "epoch: 4 iter: 62 reconn_loss: 10513.3203125 kl_loss: 1397.8660888671875\n",
      "epoch: 4 iter: 63 reconn_loss: 10894.587890625 kl_loss: 1434.478515625\n",
      "epoch: 4 iter: 64 reconn_loss: 10991.4140625 kl_loss: 1331.756103515625\n",
      "epoch: 4 iter: 65 reconn_loss: 11103.396484375 kl_loss: 1252.816650390625\n",
      "epoch: 4 iter: 66 reconn_loss: 11182.578125 kl_loss: 1318.264404296875\n",
      "epoch: 4 iter: 67 reconn_loss: 10757.44140625 kl_loss: 1382.8477783203125\n",
      "epoch: 4 iter: 68 reconn_loss: 11409.380859375 kl_loss: 1317.79443359375\n",
      "epoch: 4 iter: 69 reconn_loss: 11024.1025390625 kl_loss: 1282.796875\n",
      "epoch: 4 iter: 70 reconn_loss: 10929.908203125 kl_loss: 1254.081787109375\n",
      "epoch: 4 iter: 71 reconn_loss: 10906.7578125 kl_loss: 1382.1793212890625\n",
      "epoch: 4 iter: 72 reconn_loss: 10951.19921875 kl_loss: 1384.439697265625\n",
      "epoch: 4 iter: 73 reconn_loss: 11329.5537109375 kl_loss: 1387.602783203125\n",
      "epoch: 4 iter: 74 reconn_loss: 11277.427734375 kl_loss: 1417.5205078125\n",
      "epoch: 4 iter: 75 reconn_loss: 11260.220703125 kl_loss: 1425.9794921875\n",
      "epoch: 4 iter: 76 reconn_loss: 10884.607421875 kl_loss: 1405.3726806640625\n",
      "epoch: 4 iter: 77 reconn_loss: 11384.1025390625 kl_loss: 1394.245361328125\n",
      "epoch: 4 iter: 78 reconn_loss: 11263.0361328125 kl_loss: 1464.156982421875\n",
      "epoch: 4 iter: 79 reconn_loss: 11021.701171875 kl_loss: 1362.61572265625\n",
      "epoch: 4 iter: 80 reconn_loss: 10597.8662109375 kl_loss: 1449.488525390625\n",
      "epoch: 4 iter: 81 reconn_loss: 10780.3125 kl_loss: 1259.5946044921875\n",
      "epoch: 4 iter: 82 reconn_loss: 10823.6875 kl_loss: 1314.0438232421875\n",
      "epoch: 4 iter: 83 reconn_loss: 10900.3701171875 kl_loss: 1340.46484375\n",
      "epoch: 4 iter: 84 reconn_loss: 10914.4267578125 kl_loss: 1448.134521484375\n",
      "epoch: 4 iter: 85 reconn_loss: 11069.58984375 kl_loss: 1415.377197265625\n",
      "epoch: 4 iter: 86 reconn_loss: 10827.2666015625 kl_loss: 1403.6036376953125\n",
      "epoch: 4 iter: 87 reconn_loss: 11143.8349609375 kl_loss: 1429.432373046875\n",
      "epoch: 4 iter: 88 reconn_loss: 10842.431640625 kl_loss: 1333.18408203125\n",
      "epoch: 4 iter: 89 reconn_loss: 11089.10546875 kl_loss: 1407.5599365234375\n",
      "epoch: 4 iter: 90 reconn_loss: 10787.3857421875 kl_loss: 1408.671630859375\n",
      "epoch: 4 iter: 91 reconn_loss: 11084.845703125 kl_loss: 1265.89111328125\n",
      "epoch: 4 iter: 92 reconn_loss: 11140.80078125 kl_loss: 1383.609619140625\n",
      "epoch: 4 iter: 93 reconn_loss: 10761.404296875 kl_loss: 1367.373046875\n",
      "epoch: 4 iter: 94 reconn_loss: 11273.4755859375 kl_loss: 1399.9017333984375\n",
      "epoch: 4 iter: 95 reconn_loss: 11268.7060546875 kl_loss: 1355.7098388671875\n",
      "epoch: 4 iter: 96 reconn_loss: 10693.0244140625 kl_loss: 1382.2593994140625\n",
      "epoch: 4 iter: 97 reconn_loss: 11438.7919921875 kl_loss: 1362.6214599609375\n",
      "epoch: 4 iter: 98 reconn_loss: 11205.1162109375 kl_loss: 1452.835693359375\n",
      "epoch: 4 iter: 99 reconn_loss: 11138.7578125 kl_loss: 1464.5765380859375\n",
      "epoch: 4 iter: 100 reconn_loss: 11143.65234375 kl_loss: 1419.9814453125\n",
      "epoch: 4 iter: 101 reconn_loss: 10927.1826171875 kl_loss: 1339.889404296875\n",
      "epoch: 4 iter: 102 reconn_loss: 11413.1943359375 kl_loss: 1336.091064453125\n",
      "epoch: 4 iter: 103 reconn_loss: 11162.716796875 kl_loss: 1442.802001953125\n",
      "epoch: 4 iter: 104 reconn_loss: 11074.6123046875 kl_loss: 1329.2261962890625\n",
      "epoch: 4 iter: 105 reconn_loss: 10936.611328125 kl_loss: 1364.4962158203125\n",
      "epoch: 4 iter: 106 reconn_loss: 10886.8212890625 kl_loss: 1425.041748046875\n",
      "epoch: 4 iter: 107 reconn_loss: 11088.7783203125 kl_loss: 1470.8656005859375\n",
      "epoch: 4 iter: 108 reconn_loss: 10760.609375 kl_loss: 1454.484619140625\n",
      "epoch: 4 iter: 109 reconn_loss: 10944.1884765625 kl_loss: 1409.8406982421875\n",
      "epoch: 4 iter: 110 reconn_loss: 11164.48828125 kl_loss: 1554.7435302734375\n",
      "epoch: 4 iter: 111 reconn_loss: 11297.9736328125 kl_loss: 1440.8447265625\n",
      "epoch: 4 iter: 112 reconn_loss: 10920.943359375 kl_loss: 1403.18408203125\n",
      "epoch: 4 iter: 113 reconn_loss: 10883.3564453125 kl_loss: 1367.044189453125\n",
      "epoch: 4 iter: 114 reconn_loss: 10928.498046875 kl_loss: 1484.859375\n",
      "epoch: 4 iter: 115 reconn_loss: 10929.5517578125 kl_loss: 1396.4466552734375\n",
      "epoch: 4 iter: 116 reconn_loss: 11057.552734375 kl_loss: 1397.7935791015625\n",
      "epoch: 4 iter: 117 reconn_loss: 10890.193359375 kl_loss: 1351.67529296875\n",
      "epoch: 4 iter: 118 reconn_loss: 11230.310546875 kl_loss: 1412.1756591796875\n",
      "epoch: 4 iter: 119 reconn_loss: 10998.1484375 kl_loss: 1367.2366943359375\n",
      "epoch: 4 iter: 120 reconn_loss: 10693.6591796875 kl_loss: 1384.8878173828125\n",
      "epoch: 4 iter: 121 reconn_loss: 11381.607421875 kl_loss: 1346.865234375\n",
      "epoch: 4 iter: 122 reconn_loss: 11022.0 kl_loss: 1369.57861328125\n",
      "epoch: 4 iter: 123 reconn_loss: 11206.068359375 kl_loss: 1410.8828125\n",
      "epoch: 4 iter: 124 reconn_loss: 10826.3701171875 kl_loss: 1309.4560546875\n",
      "epoch: 4 iter: 125 reconn_loss: 10693.65234375 kl_loss: 1336.9227294921875\n",
      "epoch: 4 iter: 126 reconn_loss: 11055.060546875 kl_loss: 1349.91064453125\n",
      "epoch: 4 iter: 127 reconn_loss: 11335.94921875 kl_loss: 1292.8121337890625\n",
      "epoch: 4 iter: 128 reconn_loss: 10590.580078125 kl_loss: 1354.3438720703125\n",
      "epoch: 4 iter: 129 reconn_loss: 11041.1806640625 kl_loss: 1407.8095703125\n",
      "epoch: 4 iter: 130 reconn_loss: 10825.9912109375 kl_loss: 1455.537353515625\n",
      "epoch: 4 iter: 131 reconn_loss: 10892.1474609375 kl_loss: 1300.0328369140625\n",
      "epoch: 4 iter: 132 reconn_loss: 11040.58203125 kl_loss: 1415.0546875\n",
      "epoch: 4 iter: 133 reconn_loss: 11060.60546875 kl_loss: 1351.615234375\n",
      "epoch: 4 iter: 134 reconn_loss: 11237.49609375 kl_loss: 1352.8577880859375\n",
      "epoch: 4 iter: 135 reconn_loss: 11154.4287109375 kl_loss: 1464.549560546875\n",
      "epoch: 4 iter: 136 reconn_loss: 10919.2353515625 kl_loss: 1442.8482666015625\n",
      "epoch: 4 iter: 137 reconn_loss: 11242.5810546875 kl_loss: 1353.1883544921875\n",
      "epoch: 4 iter: 138 reconn_loss: 11053.087890625 kl_loss: 1415.420654296875\n",
      "epoch: 4 iter: 139 reconn_loss: 10995.2177734375 kl_loss: 1435.3302001953125\n",
      "epoch: 4 iter: 140 reconn_loss: 10674.322265625 kl_loss: 1376.489990234375\n",
      "epoch: 4 iter: 141 reconn_loss: 11073.4580078125 kl_loss: 1479.660400390625\n",
      "epoch: 4 iter: 142 reconn_loss: 11022.5107421875 kl_loss: 1392.4306640625\n",
      "epoch: 4 iter: 143 reconn_loss: 10726.638671875 kl_loss: 1452.29931640625\n",
      "epoch: 4 iter: 144 reconn_loss: 10718.40234375 kl_loss: 1438.5552978515625\n",
      "epoch: 4 iter: 145 reconn_loss: 10925.935546875 kl_loss: 1454.3175048828125\n",
      "epoch: 4 iter: 146 reconn_loss: 10493.0068359375 kl_loss: 1384.874755859375\n",
      "epoch: 4 iter: 147 reconn_loss: 11000.98828125 kl_loss: 1456.946533203125\n",
      "epoch: 4 iter: 148 reconn_loss: 10867.443359375 kl_loss: 1351.7996826171875\n",
      "epoch: 4 iter: 149 reconn_loss: 10614.9052734375 kl_loss: 1382.383544921875\n",
      "epoch: 4 iter: 150 reconn_loss: 10762.6787109375 kl_loss: 1463.461181640625\n",
      "epoch: 4 iter: 151 reconn_loss: 11098.9892578125 kl_loss: 1332.353759765625\n",
      "epoch: 4 iter: 152 reconn_loss: 11282.546875 kl_loss: 1433.536376953125\n",
      "epoch: 4 iter: 153 reconn_loss: 11057.537109375 kl_loss: 1468.8656005859375\n",
      "epoch: 4 iter: 154 reconn_loss: 11234.412109375 kl_loss: 1496.486083984375\n",
      "epoch: 4 iter: 155 reconn_loss: 11090.5654296875 kl_loss: 1417.019775390625\n",
      "epoch: 4 iter: 156 reconn_loss: 11085.9296875 kl_loss: 1358.394775390625\n",
      "epoch: 4 iter: 157 reconn_loss: 10609.732421875 kl_loss: 1395.49755859375\n",
      "epoch: 4 iter: 158 reconn_loss: 10576.716796875 kl_loss: 1361.618896484375\n",
      "epoch: 4 iter: 159 reconn_loss: 10828.787109375 kl_loss: 1370.79345703125\n",
      "epoch: 4 iter: 160 reconn_loss: 11004.7880859375 kl_loss: 1412.5997314453125\n",
      "epoch: 4 iter: 161 reconn_loss: 10950.130859375 kl_loss: 1376.048583984375\n",
      "epoch: 4 iter: 162 reconn_loss: 10638.185546875 kl_loss: 1332.2415771484375\n",
      "epoch: 4 iter: 163 reconn_loss: 10684.896484375 kl_loss: 1384.556884765625\n",
      "epoch: 4 iter: 164 reconn_loss: 10790.6025390625 kl_loss: 1425.3878173828125\n",
      "epoch: 4 iter: 165 reconn_loss: 10831.37109375 kl_loss: 1350.2808837890625\n",
      "epoch: 4 iter: 166 reconn_loss: 10725.65234375 kl_loss: 1373.4005126953125\n",
      "epoch: 4 iter: 167 reconn_loss: 10916.509765625 kl_loss: 1438.0703125\n",
      "epoch: 4 iter: 168 reconn_loss: 10622.826171875 kl_loss: 1495.9979248046875\n",
      "epoch: 4 iter: 169 reconn_loss: 10729.119140625 kl_loss: 1475.3314208984375\n",
      "epoch: 4 iter: 170 reconn_loss: 11111.67578125 kl_loss: 1399.83544921875\n",
      "epoch: 4 iter: 171 reconn_loss: 10643.23046875 kl_loss: 1366.8729248046875\n",
      "epoch: 4 iter: 172 reconn_loss: 11049.9228515625 kl_loss: 1524.0264892578125\n",
      "epoch: 4 iter: 173 reconn_loss: 11233.98046875 kl_loss: 1395.5687255859375\n",
      "epoch: 4 iter: 174 reconn_loss: 10762.958984375 kl_loss: 1339.7286376953125\n",
      "epoch: 4 iter: 175 reconn_loss: 10653.23046875 kl_loss: 1321.1043701171875\n",
      "epoch: 4 iter: 176 reconn_loss: 10241.2841796875 kl_loss: 1447.9423828125\n",
      "epoch: 4 iter: 177 reconn_loss: 11064.2861328125 kl_loss: 1408.7796630859375\n",
      "epoch: 4 iter: 178 reconn_loss: 10931.0244140625 kl_loss: 1435.5089111328125\n",
      "epoch: 4 iter: 179 reconn_loss: 10833.1865234375 kl_loss: 1349.822509765625\n",
      "epoch: 4 iter: 180 reconn_loss: 10879.7724609375 kl_loss: 1460.642822265625\n",
      "epoch: 4 iter: 181 reconn_loss: 10805.083984375 kl_loss: 1397.0152587890625\n",
      "epoch: 4 iter: 182 reconn_loss: 11104.5615234375 kl_loss: 1387.686279296875\n",
      "epoch: 4 iter: 183 reconn_loss: 10651.7607421875 kl_loss: 1419.6011962890625\n",
      "epoch: 4 iter: 184 reconn_loss: 10867.8544921875 kl_loss: 1473.721435546875\n",
      "epoch: 4 iter: 185 reconn_loss: 10977.97265625 kl_loss: 1468.0174560546875\n",
      "epoch: 4 iter: 186 reconn_loss: 10625.6083984375 kl_loss: 1410.600830078125\n",
      "epoch: 4 iter: 187 reconn_loss: 10969.2236328125 kl_loss: 1432.2109375\n",
      "epoch: 4 iter: 188 reconn_loss: 11155.466796875 kl_loss: 1495.8458251953125\n",
      "epoch: 4 iter: 189 reconn_loss: 10817.5634765625 kl_loss: 1375.5064697265625\n",
      "epoch: 4 iter: 190 reconn_loss: 11057.2060546875 kl_loss: 1436.2845458984375\n",
      "epoch: 4 iter: 191 reconn_loss: 11119.630859375 kl_loss: 1313.368896484375\n",
      "epoch: 4 iter: 192 reconn_loss: 10996.650390625 kl_loss: 1369.6630859375\n",
      "epoch: 4 iter: 193 reconn_loss: 10907.9384765625 kl_loss: 1404.9215087890625\n",
      "epoch: 4 iter: 194 reconn_loss: 10414.3291015625 kl_loss: 1464.735107421875\n",
      "epoch: 4 iter: 195 reconn_loss: 10683.2060546875 kl_loss: 1447.599365234375\n",
      "epoch: 4 iter: 196 reconn_loss: 10728.509765625 kl_loss: 1378.98388671875\n",
      "epoch: 4 iter: 197 reconn_loss: 11113.9189453125 kl_loss: 1494.885986328125\n",
      "epoch: 4 iter: 198 reconn_loss: 10838.04296875 kl_loss: 1295.928466796875\n",
      "epoch: 4 iter: 199 reconn_loss: 10931.158203125 kl_loss: 1374.1265869140625\n",
      "epoch: 4 iter: 200 reconn_loss: 10422.220703125 kl_loss: 1400.986328125\n",
      "epoch: 4 iter: 201 reconn_loss: 10923.1044921875 kl_loss: 1442.1060791015625\n",
      "epoch: 4 iter: 202 reconn_loss: 10823.3837890625 kl_loss: 1346.7540283203125\n",
      "epoch: 4 iter: 203 reconn_loss: 10900.5615234375 kl_loss: 1416.6136474609375\n",
      "epoch: 4 iter: 204 reconn_loss: 10591.6572265625 kl_loss: 1344.788330078125\n",
      "epoch: 4 iter: 205 reconn_loss: 10879.302734375 kl_loss: 1354.948974609375\n",
      "epoch: 4 iter: 206 reconn_loss: 11146.75 kl_loss: 1448.8980712890625\n",
      "epoch: 4 iter: 207 reconn_loss: 11220.1123046875 kl_loss: 1432.192626953125\n",
      "epoch: 4 iter: 208 reconn_loss: 11010.912109375 kl_loss: 1363.955810546875\n",
      "epoch: 4 iter: 209 reconn_loss: 10607.1416015625 kl_loss: 1489.6390380859375\n",
      "epoch: 4 iter: 210 reconn_loss: 10549.654296875 kl_loss: 1443.828369140625\n",
      "epoch: 4 iter: 211 reconn_loss: 10712.0263671875 kl_loss: 1361.903564453125\n",
      "epoch: 4 iter: 212 reconn_loss: 10700.728515625 kl_loss: 1393.19970703125\n",
      "epoch: 4 iter: 213 reconn_loss: 10919.39453125 kl_loss: 1424.6773681640625\n",
      "epoch: 4 iter: 214 reconn_loss: 10847.1513671875 kl_loss: 1453.4420166015625\n",
      "epoch: 4 iter: 215 reconn_loss: 10930.3916015625 kl_loss: 1432.6378173828125\n",
      "epoch: 4 iter: 216 reconn_loss: 10730.5400390625 kl_loss: 1392.9927978515625\n",
      "epoch: 4 iter: 217 reconn_loss: 10503.80078125 kl_loss: 1545.404052734375\n",
      "epoch: 4 iter: 218 reconn_loss: 11236.9443359375 kl_loss: 1465.8427734375\n",
      "epoch: 4 iter: 219 reconn_loss: 10807.166015625 kl_loss: 1509.90625\n",
      "epoch: 4 iter: 220 reconn_loss: 10693.140625 kl_loss: 1490.73828125\n",
      "epoch: 4 iter: 221 reconn_loss: 10872.96484375 kl_loss: 1479.72802734375\n",
      "epoch: 4 iter: 222 reconn_loss: 7263.74462890625 kl_loss: 911.2105712890625\n",
      "0.weight tensor(88.5438) tensor(-64.9292)\n",
      "0.bias tensor(38.5321) tensor(-43.5587)\n",
      "2.weight tensor(21.0164) tensor(-15.2286)\n",
      "2.bias tensor(10.3153) tensor(-9.6085)\n",
      "4.weight tensor(11.3668) tensor(-9.0459)\n",
      "4.bias tensor(7.4511) tensor(-6.3308)\n",
      "epoch: 5 iter: 0 reconn_loss: 10791.736328125 kl_loss: 1368.7908935546875\n",
      "epoch: 5 iter: 1 reconn_loss: 10755.58203125 kl_loss: 1369.8406982421875\n",
      "epoch: 5 iter: 2 reconn_loss: 10855.6279296875 kl_loss: 1405.9212646484375\n",
      "epoch: 5 iter: 3 reconn_loss: 11026.451171875 kl_loss: 1516.2958984375\n",
      "epoch: 5 iter: 4 reconn_loss: 10742.791015625 kl_loss: 1484.400390625\n",
      "epoch: 5 iter: 5 reconn_loss: 10722.4013671875 kl_loss: 1474.021240234375\n",
      "epoch: 5 iter: 6 reconn_loss: 10758.7802734375 kl_loss: 1473.7247314453125\n",
      "epoch: 5 iter: 7 reconn_loss: 10845.056640625 kl_loss: 1372.0941162109375\n",
      "epoch: 5 iter: 8 reconn_loss: 10725.8330078125 kl_loss: 1447.7213134765625\n",
      "epoch: 5 iter: 9 reconn_loss: 10731.62109375 kl_loss: 1428.437744140625\n",
      "epoch: 5 iter: 10 reconn_loss: 10647.9091796875 kl_loss: 1469.7406005859375\n",
      "epoch: 5 iter: 11 reconn_loss: 11501.298828125 kl_loss: 1485.710693359375\n",
      "epoch: 5 iter: 12 reconn_loss: 10529.03515625 kl_loss: 1395.6734619140625\n",
      "epoch: 5 iter: 13 reconn_loss: 11158.318359375 kl_loss: 1389.296875\n",
      "epoch: 5 iter: 14 reconn_loss: 10617.7919921875 kl_loss: 1364.385009765625\n",
      "epoch: 5 iter: 15 reconn_loss: 10723.8544921875 kl_loss: 1503.865478515625\n",
      "epoch: 5 iter: 16 reconn_loss: 10913.4853515625 kl_loss: 1386.4979248046875\n",
      "epoch: 5 iter: 17 reconn_loss: 10952.0791015625 kl_loss: 1463.1812744140625\n",
      "epoch: 5 iter: 18 reconn_loss: 10632.537109375 kl_loss: 1475.27099609375\n",
      "epoch: 5 iter: 19 reconn_loss: 11066.978515625 kl_loss: 1420.5633544921875\n",
      "epoch: 5 iter: 20 reconn_loss: 10851.1025390625 kl_loss: 1447.9615478515625\n",
      "epoch: 5 iter: 21 reconn_loss: 10809.638671875 kl_loss: 1498.45703125\n",
      "epoch: 5 iter: 22 reconn_loss: 10775.1044921875 kl_loss: 1422.2080078125\n",
      "epoch: 5 iter: 23 reconn_loss: 10740.095703125 kl_loss: 1316.29833984375\n",
      "epoch: 5 iter: 24 reconn_loss: 10864.69140625 kl_loss: 1465.18798828125\n",
      "epoch: 5 iter: 25 reconn_loss: 11007.3251953125 kl_loss: 1482.2891845703125\n",
      "epoch: 5 iter: 26 reconn_loss: 10782.4541015625 kl_loss: 1429.712890625\n",
      "epoch: 5 iter: 27 reconn_loss: 10777.8251953125 kl_loss: 1450.127197265625\n",
      "epoch: 5 iter: 28 reconn_loss: 10508.84375 kl_loss: 1398.031494140625\n",
      "epoch: 5 iter: 29 reconn_loss: 10823.8798828125 kl_loss: 1395.8133544921875\n",
      "epoch: 5 iter: 30 reconn_loss: 11100.4765625 kl_loss: 1462.84765625\n",
      "epoch: 5 iter: 31 reconn_loss: 10365.2607421875 kl_loss: 1468.1119384765625\n",
      "epoch: 5 iter: 32 reconn_loss: 10894.52734375 kl_loss: 1354.23095703125\n",
      "epoch: 5 iter: 33 reconn_loss: 10682.44140625 kl_loss: 1442.241943359375\n",
      "epoch: 5 iter: 34 reconn_loss: 11069.453125 kl_loss: 1369.2830810546875\n",
      "epoch: 5 iter: 35 reconn_loss: 11132.0673828125 kl_loss: 1381.5933837890625\n",
      "epoch: 5 iter: 36 reconn_loss: 10704.0126953125 kl_loss: 1409.3184814453125\n",
      "epoch: 5 iter: 37 reconn_loss: 10712.251953125 kl_loss: 1497.7481689453125\n",
      "epoch: 5 iter: 38 reconn_loss: 10939.421875 kl_loss: 1466.533447265625\n",
      "epoch: 5 iter: 39 reconn_loss: 10675.9755859375 kl_loss: 1338.7491455078125\n",
      "epoch: 5 iter: 40 reconn_loss: 10991.9921875 kl_loss: 1415.4722900390625\n",
      "epoch: 5 iter: 41 reconn_loss: 10511.3037109375 kl_loss: 1380.345458984375\n",
      "epoch: 5 iter: 42 reconn_loss: 10767.0615234375 kl_loss: 1514.304443359375\n",
      "epoch: 5 iter: 43 reconn_loss: 10893.4248046875 kl_loss: 1381.221435546875\n",
      "epoch: 5 iter: 44 reconn_loss: 10743.333984375 kl_loss: 1448.6142578125\n",
      "epoch: 5 iter: 45 reconn_loss: 10697.056640625 kl_loss: 1488.3206787109375\n",
      "epoch: 5 iter: 46 reconn_loss: 10661.69921875 kl_loss: 1388.00048828125\n",
      "epoch: 5 iter: 47 reconn_loss: 10999.458984375 kl_loss: 1394.48681640625\n",
      "epoch: 5 iter: 48 reconn_loss: 10716.7294921875 kl_loss: 1381.1513671875\n",
      "epoch: 5 iter: 49 reconn_loss: 10520.2275390625 kl_loss: 1484.7452392578125\n",
      "epoch: 5 iter: 50 reconn_loss: 10964.6484375 kl_loss: 1497.10107421875\n",
      "epoch: 5 iter: 51 reconn_loss: 10777.033203125 kl_loss: 1486.974609375\n",
      "epoch: 5 iter: 52 reconn_loss: 10790.1376953125 kl_loss: 1463.37109375\n",
      "epoch: 5 iter: 53 reconn_loss: 11041.244140625 kl_loss: 1470.155029296875\n",
      "epoch: 5 iter: 54 reconn_loss: 10742.646484375 kl_loss: 1464.2449951171875\n",
      "epoch: 5 iter: 55 reconn_loss: 10500.13671875 kl_loss: 1419.799072265625\n",
      "epoch: 5 iter: 56 reconn_loss: 10788.8740234375 kl_loss: 1530.5968017578125\n",
      "epoch: 5 iter: 57 reconn_loss: 10713.2236328125 kl_loss: 1388.8797607421875\n",
      "epoch: 5 iter: 58 reconn_loss: 10860.5244140625 kl_loss: 1424.852294921875\n",
      "epoch: 5 iter: 59 reconn_loss: 10580.3046875 kl_loss: 1490.65771484375\n",
      "epoch: 5 iter: 60 reconn_loss: 10997.5205078125 kl_loss: 1455.6982421875\n",
      "epoch: 5 iter: 61 reconn_loss: 10516.650390625 kl_loss: 1407.90869140625\n",
      "epoch: 5 iter: 62 reconn_loss: 10810.6220703125 kl_loss: 1466.107177734375\n",
      "epoch: 5 iter: 63 reconn_loss: 10605.2294921875 kl_loss: 1487.033203125\n",
      "epoch: 5 iter: 64 reconn_loss: 10865.0419921875 kl_loss: 1554.7325439453125\n",
      "epoch: 5 iter: 65 reconn_loss: 10692.8505859375 kl_loss: 1417.080322265625\n",
      "epoch: 5 iter: 66 reconn_loss: 10727.451171875 kl_loss: 1471.9481201171875\n",
      "epoch: 5 iter: 67 reconn_loss: 10810.2197265625 kl_loss: 1447.4775390625\n",
      "epoch: 5 iter: 68 reconn_loss: 10291.68359375 kl_loss: 1412.265625\n",
      "epoch: 5 iter: 69 reconn_loss: 10813.607421875 kl_loss: 1419.9471435546875\n",
      "epoch: 5 iter: 70 reconn_loss: 10752.0361328125 kl_loss: 1484.1990966796875\n",
      "epoch: 5 iter: 71 reconn_loss: 10655.017578125 kl_loss: 1413.7052001953125\n",
      "epoch: 5 iter: 72 reconn_loss: 10668.095703125 kl_loss: 1364.82275390625\n",
      "epoch: 5 iter: 73 reconn_loss: 11184.40234375 kl_loss: 1399.452392578125\n",
      "epoch: 5 iter: 74 reconn_loss: 10775.2958984375 kl_loss: 1431.2982177734375\n",
      "epoch: 5 iter: 75 reconn_loss: 10913.724609375 kl_loss: 1384.9239501953125\n",
      "epoch: 5 iter: 76 reconn_loss: 10575.609375 kl_loss: 1469.63671875\n",
      "epoch: 5 iter: 77 reconn_loss: 10583.53125 kl_loss: 1449.2984619140625\n",
      "epoch: 5 iter: 78 reconn_loss: 10985.2890625 kl_loss: 1456.810791015625\n",
      "epoch: 5 iter: 79 reconn_loss: 11007.859375 kl_loss: 1555.899169921875\n",
      "epoch: 5 iter: 80 reconn_loss: 10880.3564453125 kl_loss: 1502.2979736328125\n",
      "epoch: 5 iter: 81 reconn_loss: 10931.138671875 kl_loss: 1414.779296875\n",
      "epoch: 5 iter: 82 reconn_loss: 10964.64453125 kl_loss: 1424.759033203125\n",
      "epoch: 5 iter: 83 reconn_loss: 10401.841796875 kl_loss: 1490.611328125\n",
      "epoch: 5 iter: 84 reconn_loss: 10590.228515625 kl_loss: 1452.328125\n",
      "epoch: 5 iter: 85 reconn_loss: 10558.3447265625 kl_loss: 1474.8560791015625\n",
      "epoch: 5 iter: 86 reconn_loss: 10775.076171875 kl_loss: 1519.02587890625\n",
      "epoch: 5 iter: 87 reconn_loss: 10987.8076171875 kl_loss: 1542.6009521484375\n",
      "epoch: 5 iter: 88 reconn_loss: 10714.5283203125 kl_loss: 1447.1593017578125\n",
      "epoch: 5 iter: 89 reconn_loss: 10875.8642578125 kl_loss: 1475.5775146484375\n",
      "epoch: 5 iter: 90 reconn_loss: 10746.7197265625 kl_loss: 1522.1453857421875\n",
      "epoch: 5 iter: 91 reconn_loss: 10675.1962890625 kl_loss: 1476.695556640625\n",
      "epoch: 5 iter: 92 reconn_loss: 10743.5615234375 kl_loss: 1447.4283447265625\n",
      "epoch: 5 iter: 93 reconn_loss: 10682.4541015625 kl_loss: 1482.780517578125\n",
      "epoch: 5 iter: 94 reconn_loss: 10738.41015625 kl_loss: 1467.19482421875\n",
      "epoch: 5 iter: 95 reconn_loss: 10646.5966796875 kl_loss: 1484.7235107421875\n",
      "epoch: 5 iter: 96 reconn_loss: 10909.49609375 kl_loss: 1522.8927001953125\n",
      "epoch: 5 iter: 97 reconn_loss: 11117.02734375 kl_loss: 1473.9901123046875\n",
      "epoch: 5 iter: 98 reconn_loss: 11184.8037109375 kl_loss: 1423.885498046875\n",
      "epoch: 5 iter: 99 reconn_loss: 10827.3291015625 kl_loss: 1461.4342041015625\n",
      "epoch: 5 iter: 100 reconn_loss: 10254.96484375 kl_loss: 1451.502685546875\n",
      "epoch: 5 iter: 101 reconn_loss: 10531.5751953125 kl_loss: 1429.132080078125\n",
      "epoch: 5 iter: 102 reconn_loss: 10551.0556640625 kl_loss: 1452.495361328125\n",
      "epoch: 5 iter: 103 reconn_loss: 10466.4765625 kl_loss: 1455.7828369140625\n",
      "epoch: 5 iter: 104 reconn_loss: 10823.138671875 kl_loss: 1497.390869140625\n",
      "epoch: 5 iter: 105 reconn_loss: 10889.853515625 kl_loss: 1444.5657958984375\n",
      "epoch: 5 iter: 106 reconn_loss: 10191.310546875 kl_loss: 1372.537109375\n",
      "epoch: 5 iter: 107 reconn_loss: 10463.4091796875 kl_loss: 1379.4156494140625\n",
      "epoch: 5 iter: 108 reconn_loss: 10854.224609375 kl_loss: 1465.27587890625\n",
      "epoch: 5 iter: 109 reconn_loss: 10692.583984375 kl_loss: 1406.2501220703125\n",
      "epoch: 5 iter: 110 reconn_loss: 10594.408203125 kl_loss: 1465.01904296875\n",
      "epoch: 5 iter: 111 reconn_loss: 10401.1455078125 kl_loss: 1385.3934326171875\n",
      "epoch: 5 iter: 112 reconn_loss: 10721.7744140625 kl_loss: 1480.619873046875\n",
      "epoch: 5 iter: 113 reconn_loss: 10635.08203125 kl_loss: 1491.6529541015625\n",
      "epoch: 5 iter: 114 reconn_loss: 10777.412109375 kl_loss: 1473.9820556640625\n",
      "epoch: 5 iter: 115 reconn_loss: 10735.48046875 kl_loss: 1524.230224609375\n",
      "epoch: 5 iter: 116 reconn_loss: 10235.771484375 kl_loss: 1439.0567626953125\n",
      "epoch: 5 iter: 117 reconn_loss: 10682.25390625 kl_loss: 1483.1368408203125\n",
      "epoch: 5 iter: 118 reconn_loss: 10452.83984375 kl_loss: 1406.3955078125\n",
      "epoch: 5 iter: 119 reconn_loss: 10543.720703125 kl_loss: 1551.100830078125\n",
      "epoch: 5 iter: 120 reconn_loss: 10895.166015625 kl_loss: 1459.560791015625\n",
      "epoch: 5 iter: 121 reconn_loss: 10874.3671875 kl_loss: 1393.27392578125\n",
      "epoch: 5 iter: 122 reconn_loss: 10761.947265625 kl_loss: 1411.6468505859375\n",
      "epoch: 5 iter: 123 reconn_loss: 10710.30078125 kl_loss: 1448.19091796875\n",
      "epoch: 5 iter: 124 reconn_loss: 10498.21875 kl_loss: 1511.9176025390625\n",
      "epoch: 5 iter: 125 reconn_loss: 10925.7763671875 kl_loss: 1445.3277587890625\n",
      "epoch: 5 iter: 126 reconn_loss: 10616.34375 kl_loss: 1451.0501708984375\n",
      "epoch: 5 iter: 127 reconn_loss: 10717.2353515625 kl_loss: 1503.0767822265625\n",
      "epoch: 5 iter: 128 reconn_loss: 11055.9619140625 kl_loss: 1513.194580078125\n",
      "epoch: 5 iter: 129 reconn_loss: 10612.328125 kl_loss: 1423.24072265625\n",
      "epoch: 5 iter: 130 reconn_loss: 10755.1435546875 kl_loss: 1402.768310546875\n",
      "epoch: 5 iter: 131 reconn_loss: 10310.5126953125 kl_loss: 1399.6502685546875\n",
      "epoch: 5 iter: 132 reconn_loss: 10949.7060546875 kl_loss: 1459.0390625\n",
      "epoch: 5 iter: 133 reconn_loss: 10563.83203125 kl_loss: 1485.852294921875\n",
      "epoch: 5 iter: 134 reconn_loss: 10586.162109375 kl_loss: 1468.5806884765625\n",
      "epoch: 5 iter: 135 reconn_loss: 10917.42578125 kl_loss: 1475.5406494140625\n",
      "epoch: 5 iter: 136 reconn_loss: 10510.720703125 kl_loss: 1455.35107421875\n",
      "epoch: 5 iter: 137 reconn_loss: 10778.26171875 kl_loss: 1442.4234619140625\n",
      "epoch: 5 iter: 138 reconn_loss: 10718.396484375 kl_loss: 1403.820068359375\n",
      "epoch: 5 iter: 139 reconn_loss: 10598.333984375 kl_loss: 1506.97509765625\n",
      "epoch: 5 iter: 140 reconn_loss: 10721.1953125 kl_loss: 1413.900146484375\n",
      "epoch: 5 iter: 141 reconn_loss: 10341.7275390625 kl_loss: 1411.537841796875\n",
      "epoch: 5 iter: 142 reconn_loss: 10713.1279296875 kl_loss: 1393.501953125\n",
      "epoch: 5 iter: 143 reconn_loss: 10679.501953125 kl_loss: 1389.5250244140625\n",
      "epoch: 5 iter: 144 reconn_loss: 10908.62890625 kl_loss: 1444.994873046875\n",
      "epoch: 5 iter: 145 reconn_loss: 10482.109375 kl_loss: 1420.06396484375\n",
      "epoch: 5 iter: 146 reconn_loss: 10467.09765625 kl_loss: 1489.53466796875\n",
      "epoch: 5 iter: 147 reconn_loss: 10773.1953125 kl_loss: 1493.0631103515625\n",
      "epoch: 5 iter: 148 reconn_loss: 10724.701171875 kl_loss: 1505.65380859375\n",
      "epoch: 5 iter: 149 reconn_loss: 10984.0791015625 kl_loss: 1485.4517822265625\n",
      "epoch: 5 iter: 150 reconn_loss: 10661.3134765625 kl_loss: 1551.2001953125\n",
      "epoch: 5 iter: 151 reconn_loss: 10624.056640625 kl_loss: 1526.630615234375\n",
      "epoch: 5 iter: 152 reconn_loss: 10714.34765625 kl_loss: 1475.35888671875\n",
      "epoch: 5 iter: 153 reconn_loss: 10630.8271484375 kl_loss: 1454.1553955078125\n",
      "epoch: 5 iter: 154 reconn_loss: 10433.44921875 kl_loss: 1506.1207275390625\n",
      "epoch: 5 iter: 155 reconn_loss: 11069.01171875 kl_loss: 1453.5030517578125\n",
      "epoch: 5 iter: 156 reconn_loss: 10474.79296875 kl_loss: 1503.27880859375\n",
      "epoch: 5 iter: 157 reconn_loss: 10462.978515625 kl_loss: 1595.673095703125\n",
      "epoch: 5 iter: 158 reconn_loss: 10777.40234375 kl_loss: 1450.78271484375\n",
      "epoch: 5 iter: 159 reconn_loss: 10638.251953125 kl_loss: 1480.590087890625\n",
      "epoch: 5 iter: 160 reconn_loss: 10673.529296875 kl_loss: 1521.1541748046875\n",
      "epoch: 5 iter: 161 reconn_loss: 10831.1171875 kl_loss: 1534.333984375\n",
      "epoch: 5 iter: 162 reconn_loss: 10329.703125 kl_loss: 1486.715087890625\n",
      "epoch: 5 iter: 163 reconn_loss: 10785.810546875 kl_loss: 1490.85009765625\n",
      "epoch: 5 iter: 164 reconn_loss: 10806.142578125 kl_loss: 1450.961669921875\n",
      "epoch: 5 iter: 165 reconn_loss: 10458.205078125 kl_loss: 1450.184326171875\n",
      "epoch: 5 iter: 166 reconn_loss: 10213.822265625 kl_loss: 1480.091552734375\n",
      "epoch: 5 iter: 167 reconn_loss: 10808.0546875 kl_loss: 1394.0645751953125\n",
      "epoch: 5 iter: 168 reconn_loss: 10418.556640625 kl_loss: 1464.733642578125\n",
      "epoch: 5 iter: 169 reconn_loss: 10573.9267578125 kl_loss: 1413.961669921875\n",
      "epoch: 5 iter: 170 reconn_loss: 10726.236328125 kl_loss: 1427.039306640625\n",
      "epoch: 5 iter: 171 reconn_loss: 10529.3798828125 kl_loss: 1447.220458984375\n",
      "epoch: 5 iter: 172 reconn_loss: 10008.966796875 kl_loss: 1456.763916015625\n",
      "epoch: 5 iter: 173 reconn_loss: 10529.986328125 kl_loss: 1513.4654541015625\n",
      "epoch: 5 iter: 174 reconn_loss: 10413.064453125 kl_loss: 1502.8455810546875\n",
      "epoch: 5 iter: 175 reconn_loss: 10558.73046875 kl_loss: 1452.67626953125\n",
      "epoch: 5 iter: 176 reconn_loss: 10294.1123046875 kl_loss: 1364.9420166015625\n",
      "epoch: 5 iter: 177 reconn_loss: 10575.6982421875 kl_loss: 1372.795166015625\n",
      "epoch: 5 iter: 178 reconn_loss: 10610.861328125 kl_loss: 1462.5888671875\n",
      "epoch: 5 iter: 179 reconn_loss: 10061.126953125 kl_loss: 1457.45947265625\n",
      "epoch: 5 iter: 180 reconn_loss: 10707.599609375 kl_loss: 1440.619384765625\n",
      "epoch: 5 iter: 181 reconn_loss: 10733.76171875 kl_loss: 1453.7071533203125\n",
      "epoch: 5 iter: 182 reconn_loss: 10923.91015625 kl_loss: 1467.81640625\n",
      "epoch: 5 iter: 183 reconn_loss: 10478.0390625 kl_loss: 1537.07861328125\n",
      "epoch: 5 iter: 184 reconn_loss: 10448.22265625 kl_loss: 1496.4208984375\n",
      "epoch: 5 iter: 185 reconn_loss: 10687.0546875 kl_loss: 1428.794921875\n",
      "epoch: 5 iter: 186 reconn_loss: 10496.744140625 kl_loss: 1553.5675048828125\n",
      "epoch: 5 iter: 187 reconn_loss: 10617.53515625 kl_loss: 1530.82080078125\n",
      "epoch: 5 iter: 188 reconn_loss: 10400.1328125 kl_loss: 1468.092041015625\n",
      "epoch: 5 iter: 189 reconn_loss: 10456.763671875 kl_loss: 1433.0845947265625\n",
      "epoch: 5 iter: 190 reconn_loss: 10488.4609375 kl_loss: 1517.447998046875\n",
      "epoch: 5 iter: 191 reconn_loss: 10640.1875 kl_loss: 1413.939697265625\n",
      "epoch: 5 iter: 192 reconn_loss: 10981.9921875 kl_loss: 1494.9534912109375\n",
      "epoch: 5 iter: 193 reconn_loss: 10466.013671875 kl_loss: 1422.395263671875\n",
      "epoch: 5 iter: 194 reconn_loss: 10381.3505859375 kl_loss: 1471.309814453125\n",
      "epoch: 5 iter: 195 reconn_loss: 10517.833984375 kl_loss: 1447.60302734375\n",
      "epoch: 5 iter: 196 reconn_loss: 10423.345703125 kl_loss: 1493.893310546875\n",
      "epoch: 5 iter: 197 reconn_loss: 10611.4453125 kl_loss: 1431.30419921875\n",
      "epoch: 5 iter: 198 reconn_loss: 10484.8017578125 kl_loss: 1478.9798583984375\n",
      "epoch: 5 iter: 199 reconn_loss: 10395.10546875 kl_loss: 1415.774658203125\n",
      "epoch: 5 iter: 200 reconn_loss: 10827.556640625 kl_loss: 1472.931396484375\n",
      "epoch: 5 iter: 201 reconn_loss: 10853.2763671875 kl_loss: 1519.9898681640625\n",
      "epoch: 5 iter: 202 reconn_loss: 10794.5625 kl_loss: 1466.4149169921875\n",
      "epoch: 5 iter: 203 reconn_loss: 10482.85546875 kl_loss: 1600.89794921875\n",
      "epoch: 5 iter: 204 reconn_loss: 10598.666015625 kl_loss: 1438.2327880859375\n",
      "epoch: 5 iter: 205 reconn_loss: 10754.9423828125 kl_loss: 1541.710205078125\n",
      "epoch: 5 iter: 206 reconn_loss: 10388.7607421875 kl_loss: 1474.9659423828125\n",
      "epoch: 5 iter: 207 reconn_loss: 10568.759765625 kl_loss: 1485.632568359375\n",
      "epoch: 5 iter: 208 reconn_loss: 10632.4951171875 kl_loss: 1434.388671875\n",
      "epoch: 5 iter: 209 reconn_loss: 10607.6083984375 kl_loss: 1477.9615478515625\n",
      "epoch: 5 iter: 210 reconn_loss: 10167.2958984375 kl_loss: 1642.867919921875\n",
      "epoch: 5 iter: 211 reconn_loss: 10612.142578125 kl_loss: 1530.0118408203125\n",
      "epoch: 5 iter: 212 reconn_loss: 10510.53125 kl_loss: 1498.728515625\n",
      "epoch: 5 iter: 213 reconn_loss: 10858.7744140625 kl_loss: 1467.3399658203125\n",
      "epoch: 5 iter: 214 reconn_loss: 10306.359375 kl_loss: 1431.367919921875\n",
      "epoch: 5 iter: 215 reconn_loss: 10824.298828125 kl_loss: 1446.9285888671875\n",
      "epoch: 5 iter: 216 reconn_loss: 10385.1748046875 kl_loss: 1489.3250732421875\n",
      "epoch: 5 iter: 217 reconn_loss: 10496.611328125 kl_loss: 1490.931640625\n",
      "epoch: 5 iter: 218 reconn_loss: 10262.4111328125 kl_loss: 1510.478515625\n",
      "epoch: 5 iter: 219 reconn_loss: 10840.58984375 kl_loss: 1590.3155517578125\n",
      "epoch: 5 iter: 220 reconn_loss: 10583.224609375 kl_loss: 1493.769287109375\n",
      "epoch: 5 iter: 221 reconn_loss: 10775.0634765625 kl_loss: 1464.8917236328125\n",
      "epoch: 5 iter: 222 reconn_loss: 7128.0185546875 kl_loss: 1009.3741455078125\n",
      "0.weight tensor(76.9717) tensor(-70.2373)\n",
      "0.bias tensor(32.5095) tensor(-26.4978)\n",
      "2.weight tensor(34.7030) tensor(-17.3950)\n",
      "2.bias tensor(12.4170) tensor(-10.0365)\n",
      "4.weight tensor(9.0878) tensor(-7.4584)\n",
      "4.bias tensor(4.9411) tensor(-4.6892)\n",
      "epoch: 6 iter: 0 reconn_loss: 10645.0126953125 kl_loss: 1597.7174072265625\n",
      "epoch: 6 iter: 1 reconn_loss: 10441.611328125 kl_loss: 1503.8426513671875\n",
      "epoch: 6 iter: 2 reconn_loss: 10731.7783203125 kl_loss: 1466.697998046875\n",
      "epoch: 6 iter: 3 reconn_loss: 10773.056640625 kl_loss: 1499.1114501953125\n",
      "epoch: 6 iter: 4 reconn_loss: 10721.5029296875 kl_loss: 1496.1571044921875\n",
      "epoch: 6 iter: 5 reconn_loss: 10483.0595703125 kl_loss: 1546.8387451171875\n",
      "epoch: 6 iter: 6 reconn_loss: 10542.6669921875 kl_loss: 1572.9022216796875\n",
      "epoch: 6 iter: 7 reconn_loss: 10866.2109375 kl_loss: 1519.8775634765625\n",
      "epoch: 6 iter: 8 reconn_loss: 10538.6240234375 kl_loss: 1501.793212890625\n",
      "epoch: 6 iter: 9 reconn_loss: 10777.228515625 kl_loss: 1564.2052001953125\n",
      "epoch: 6 iter: 10 reconn_loss: 10406.0537109375 kl_loss: 1430.9281005859375\n",
      "epoch: 6 iter: 11 reconn_loss: 10453.0986328125 kl_loss: 1480.4483642578125\n",
      "epoch: 6 iter: 12 reconn_loss: 10432.8466796875 kl_loss: 1453.6929931640625\n",
      "epoch: 6 iter: 13 reconn_loss: 10306.8203125 kl_loss: 1415.891845703125\n",
      "epoch: 6 iter: 14 reconn_loss: 10568.744140625 kl_loss: 1518.91552734375\n",
      "epoch: 6 iter: 15 reconn_loss: 10641.205078125 kl_loss: 1469.22705078125\n",
      "epoch: 6 iter: 16 reconn_loss: 10006.0947265625 kl_loss: 1526.377685546875\n",
      "epoch: 6 iter: 17 reconn_loss: 10402.521484375 kl_loss: 1408.53662109375\n",
      "epoch: 6 iter: 18 reconn_loss: 10726.181640625 kl_loss: 1420.164794921875\n",
      "epoch: 6 iter: 19 reconn_loss: 10391.787109375 kl_loss: 1426.248779296875\n",
      "epoch: 6 iter: 20 reconn_loss: 10332.9013671875 kl_loss: 1444.2099609375\n",
      "epoch: 6 iter: 21 reconn_loss: 10669.5419921875 kl_loss: 1519.6932373046875\n",
      "epoch: 6 iter: 22 reconn_loss: 10116.384765625 kl_loss: 1571.697998046875\n",
      "epoch: 6 iter: 23 reconn_loss: 10678.8623046875 kl_loss: 1548.0045166015625\n",
      "epoch: 6 iter: 24 reconn_loss: 10412.0263671875 kl_loss: 1514.4715576171875\n",
      "epoch: 6 iter: 25 reconn_loss: 10509.708984375 kl_loss: 1509.99169921875\n",
      "epoch: 6 iter: 26 reconn_loss: 10594.779296875 kl_loss: 1477.5277099609375\n",
      "epoch: 6 iter: 27 reconn_loss: 10258.587890625 kl_loss: 1496.048095703125\n",
      "epoch: 6 iter: 28 reconn_loss: 10561.6796875 kl_loss: 1516.8885498046875\n",
      "epoch: 6 iter: 29 reconn_loss: 10788.400390625 kl_loss: 1507.14404296875\n",
      "epoch: 6 iter: 30 reconn_loss: 10799.802734375 kl_loss: 1552.9302978515625\n",
      "epoch: 6 iter: 31 reconn_loss: 10475.513671875 kl_loss: 1467.603759765625\n",
      "epoch: 6 iter: 32 reconn_loss: 10478.9892578125 kl_loss: 1570.314208984375\n",
      "epoch: 6 iter: 33 reconn_loss: 10105.302734375 kl_loss: 1431.507080078125\n",
      "epoch: 6 iter: 34 reconn_loss: 10425.154296875 kl_loss: 1515.755615234375\n",
      "epoch: 6 iter: 35 reconn_loss: 10481.3388671875 kl_loss: 1521.028564453125\n",
      "epoch: 6 iter: 36 reconn_loss: 10645.439453125 kl_loss: 1502.517578125\n",
      "epoch: 6 iter: 37 reconn_loss: 10312.5322265625 kl_loss: 1534.2943115234375\n",
      "epoch: 6 iter: 38 reconn_loss: 10362.1005859375 kl_loss: 1484.783203125\n",
      "epoch: 6 iter: 39 reconn_loss: 10579.21484375 kl_loss: 1512.064453125\n",
      "epoch: 6 iter: 40 reconn_loss: 10397.873046875 kl_loss: 1417.400146484375\n",
      "epoch: 6 iter: 41 reconn_loss: 10039.7744140625 kl_loss: 1421.8521728515625\n",
      "epoch: 6 iter: 42 reconn_loss: 10395.4033203125 kl_loss: 1556.960693359375\n",
      "epoch: 6 iter: 43 reconn_loss: 10716.642578125 kl_loss: 1487.72265625\n",
      "epoch: 6 iter: 44 reconn_loss: 10354.779296875 kl_loss: 1475.914306640625\n",
      "epoch: 6 iter: 45 reconn_loss: 10418.2333984375 kl_loss: 1402.6533203125\n",
      "epoch: 6 iter: 46 reconn_loss: 10514.005859375 kl_loss: 1445.4959716796875\n",
      "epoch: 6 iter: 47 reconn_loss: 10766.1787109375 kl_loss: 1512.881103515625\n",
      "epoch: 6 iter: 48 reconn_loss: 10183.583984375 kl_loss: 1535.6949462890625\n",
      "epoch: 6 iter: 49 reconn_loss: 10549.0703125 kl_loss: 1530.2203369140625\n",
      "epoch: 6 iter: 50 reconn_loss: 10538.8759765625 kl_loss: 1506.594482421875\n",
      "epoch: 6 iter: 51 reconn_loss: 10484.171875 kl_loss: 1550.781494140625\n",
      "epoch: 6 iter: 52 reconn_loss: 10380.6865234375 kl_loss: 1566.413818359375\n",
      "epoch: 6 iter: 53 reconn_loss: 10823.115234375 kl_loss: 1467.3043212890625\n",
      "epoch: 6 iter: 54 reconn_loss: 10596.033203125 kl_loss: 1509.572021484375\n",
      "epoch: 6 iter: 55 reconn_loss: 10374.95703125 kl_loss: 1434.0372314453125\n",
      "epoch: 6 iter: 56 reconn_loss: 10263.2255859375 kl_loss: 1520.6512451171875\n",
      "epoch: 6 iter: 57 reconn_loss: 10384.8232421875 kl_loss: 1517.9246826171875\n",
      "epoch: 6 iter: 58 reconn_loss: 10392.36328125 kl_loss: 1534.629638671875\n",
      "epoch: 6 iter: 59 reconn_loss: 10106.2890625 kl_loss: 1480.85302734375\n",
      "epoch: 6 iter: 60 reconn_loss: 10762.76953125 kl_loss: 1534.177001953125\n",
      "epoch: 6 iter: 61 reconn_loss: 10472.529296875 kl_loss: 1590.83203125\n",
      "epoch: 6 iter: 62 reconn_loss: 10423.4462890625 kl_loss: 1596.427978515625\n",
      "epoch: 6 iter: 63 reconn_loss: 10370.115234375 kl_loss: 1559.32958984375\n",
      "epoch: 6 iter: 64 reconn_loss: 10439.4326171875 kl_loss: 1512.509521484375\n",
      "epoch: 6 iter: 65 reconn_loss: 10516.048828125 kl_loss: 1600.3946533203125\n",
      "epoch: 6 iter: 66 reconn_loss: 10314.0205078125 kl_loss: 1589.009765625\n",
      "epoch: 6 iter: 67 reconn_loss: 10296.80859375 kl_loss: 1582.113037109375\n",
      "epoch: 6 iter: 68 reconn_loss: 10267.732421875 kl_loss: 1471.322265625\n",
      "epoch: 6 iter: 69 reconn_loss: 10377.6259765625 kl_loss: 1563.154296875\n",
      "epoch: 6 iter: 70 reconn_loss: 10125.0244140625 kl_loss: 1575.641845703125\n",
      "epoch: 6 iter: 71 reconn_loss: 10530.8271484375 kl_loss: 1446.4852294921875\n",
      "epoch: 6 iter: 72 reconn_loss: 10787.9375 kl_loss: 1501.6959228515625\n",
      "epoch: 6 iter: 73 reconn_loss: 10173.5224609375 kl_loss: 1540.4608154296875\n",
      "epoch: 6 iter: 74 reconn_loss: 10375.447265625 kl_loss: 1436.44287109375\n",
      "epoch: 6 iter: 75 reconn_loss: 10623.935546875 kl_loss: 1508.3172607421875\n",
      "epoch: 6 iter: 76 reconn_loss: 10810.12890625 kl_loss: 1504.3984375\n",
      "epoch: 6 iter: 77 reconn_loss: 10416.267578125 kl_loss: 1505.44091796875\n",
      "epoch: 6 iter: 78 reconn_loss: 10653.064453125 kl_loss: 1544.3546142578125\n",
      "epoch: 6 iter: 79 reconn_loss: 10192.7373046875 kl_loss: 1480.498779296875\n",
      "epoch: 6 iter: 80 reconn_loss: 10530.3779296875 kl_loss: 1556.308837890625\n",
      "epoch: 6 iter: 81 reconn_loss: 10289.234375 kl_loss: 1527.9527587890625\n",
      "epoch: 6 iter: 82 reconn_loss: 10453.9677734375 kl_loss: 1608.85546875\n",
      "epoch: 6 iter: 83 reconn_loss: 10696.103515625 kl_loss: 1589.6204833984375\n",
      "epoch: 6 iter: 84 reconn_loss: 10375.833984375 kl_loss: 1534.15087890625\n",
      "epoch: 6 iter: 85 reconn_loss: 10472.7783203125 kl_loss: 1520.4107666015625\n",
      "epoch: 6 iter: 86 reconn_loss: 10156.958984375 kl_loss: 1546.222900390625\n",
      "epoch: 6 iter: 87 reconn_loss: 10720.744140625 kl_loss: 1478.376708984375\n",
      "epoch: 6 iter: 88 reconn_loss: 10244.029296875 kl_loss: 1496.7498779296875\n",
      "epoch: 6 iter: 89 reconn_loss: 10233.5029296875 kl_loss: 1545.079833984375\n",
      "epoch: 6 iter: 90 reconn_loss: 10649.189453125 kl_loss: 1487.9337158203125\n",
      "epoch: 6 iter: 91 reconn_loss: 10323.1650390625 kl_loss: 1466.4915771484375\n",
      "epoch: 6 iter: 92 reconn_loss: 10627.625 kl_loss: 1555.7601318359375\n",
      "epoch: 6 iter: 93 reconn_loss: 10100.3515625 kl_loss: 1511.39501953125\n",
      "epoch: 6 iter: 94 reconn_loss: 10383.3896484375 kl_loss: 1468.5269775390625\n",
      "epoch: 6 iter: 95 reconn_loss: 10637.8173828125 kl_loss: 1497.959716796875\n",
      "epoch: 6 iter: 96 reconn_loss: 10647.7998046875 kl_loss: 1523.3577880859375\n",
      "epoch: 6 iter: 97 reconn_loss: 10445.5439453125 kl_loss: 1508.150390625\n",
      "epoch: 6 iter: 98 reconn_loss: 10533.287109375 kl_loss: 1498.459228515625\n",
      "epoch: 6 iter: 99 reconn_loss: 10302.435546875 kl_loss: 1480.3294677734375\n",
      "epoch: 6 iter: 100 reconn_loss: 9996.271484375 kl_loss: 1404.9730224609375\n",
      "epoch: 6 iter: 101 reconn_loss: 10138.38671875 kl_loss: 1491.39208984375\n",
      "epoch: 6 iter: 102 reconn_loss: 10362.7900390625 kl_loss: 1424.455078125\n",
      "epoch: 6 iter: 103 reconn_loss: 10370.171875 kl_loss: 1503.4482421875\n",
      "epoch: 6 iter: 104 reconn_loss: 10508.861328125 kl_loss: 1477.613037109375\n",
      "epoch: 6 iter: 105 reconn_loss: 10384.767578125 kl_loss: 1533.59619140625\n",
      "epoch: 6 iter: 106 reconn_loss: 10570.298828125 kl_loss: 1458.490966796875\n",
      "epoch: 6 iter: 107 reconn_loss: 10320.828125 kl_loss: 1468.318603515625\n",
      "epoch: 6 iter: 108 reconn_loss: 10079.66796875 kl_loss: 1482.884765625\n",
      "epoch: 6 iter: 109 reconn_loss: 10563.330078125 kl_loss: 1576.845703125\n",
      "epoch: 6 iter: 110 reconn_loss: 10205.32421875 kl_loss: 1546.9091796875\n",
      "epoch: 6 iter: 111 reconn_loss: 10198.3955078125 kl_loss: 1578.1844482421875\n",
      "epoch: 6 iter: 112 reconn_loss: 10455.4853515625 kl_loss: 1486.0399169921875\n",
      "epoch: 6 iter: 113 reconn_loss: 10276.3037109375 kl_loss: 1513.159423828125\n",
      "epoch: 6 iter: 114 reconn_loss: 10325.3193359375 kl_loss: 1629.763671875\n",
      "epoch: 6 iter: 115 reconn_loss: 10369.642578125 kl_loss: 1570.1011962890625\n",
      "epoch: 6 iter: 116 reconn_loss: 10694.490234375 kl_loss: 1599.47412109375\n",
      "epoch: 6 iter: 117 reconn_loss: 10252.2109375 kl_loss: 1564.32275390625\n",
      "epoch: 6 iter: 118 reconn_loss: 10454.8515625 kl_loss: 1526.98876953125\n",
      "epoch: 6 iter: 119 reconn_loss: 10151.166015625 kl_loss: 1544.7216796875\n",
      "epoch: 6 iter: 120 reconn_loss: 10438.2177734375 kl_loss: 1550.8798828125\n",
      "epoch: 6 iter: 121 reconn_loss: 10282.4873046875 kl_loss: 1543.643310546875\n",
      "epoch: 6 iter: 122 reconn_loss: 10396.83984375 kl_loss: 1495.578857421875\n",
      "epoch: 6 iter: 123 reconn_loss: 10736.9814453125 kl_loss: 1534.115966796875\n",
      "epoch: 6 iter: 124 reconn_loss: 10429.6875 kl_loss: 1545.1922607421875\n",
      "epoch: 6 iter: 125 reconn_loss: 10169.029296875 kl_loss: 1465.37060546875\n",
      "epoch: 6 iter: 126 reconn_loss: 10215.673828125 kl_loss: 1484.955078125\n",
      "epoch: 6 iter: 127 reconn_loss: 10194.1484375 kl_loss: 1527.5673828125\n",
      "epoch: 6 iter: 128 reconn_loss: 10551.7412109375 kl_loss: 1500.1368408203125\n",
      "epoch: 6 iter: 129 reconn_loss: 10428.197265625 kl_loss: 1507.5814208984375\n",
      "epoch: 6 iter: 130 reconn_loss: 10608.83984375 kl_loss: 1444.9080810546875\n",
      "epoch: 6 iter: 131 reconn_loss: 10205.9736328125 kl_loss: 1438.013427734375\n",
      "epoch: 6 iter: 132 reconn_loss: 10023.1689453125 kl_loss: 1497.0369873046875\n",
      "epoch: 6 iter: 133 reconn_loss: 10085.8740234375 kl_loss: 1590.793701171875\n",
      "epoch: 6 iter: 134 reconn_loss: 10373.0859375 kl_loss: 1487.7066650390625\n",
      "epoch: 6 iter: 135 reconn_loss: 10289.9599609375 kl_loss: 1496.7713623046875\n",
      "epoch: 6 iter: 136 reconn_loss: 10418.6884765625 kl_loss: 1488.095458984375\n",
      "epoch: 6 iter: 137 reconn_loss: 10221.763671875 kl_loss: 1505.5189208984375\n",
      "epoch: 6 iter: 138 reconn_loss: 10682.212890625 kl_loss: 1504.9912109375\n",
      "epoch: 6 iter: 139 reconn_loss: 10348.80078125 kl_loss: 1473.28369140625\n",
      "epoch: 6 iter: 140 reconn_loss: 10236.5068359375 kl_loss: 1537.0264892578125\n",
      "epoch: 6 iter: 141 reconn_loss: 10298.1552734375 kl_loss: 1540.082763671875\n",
      "epoch: 6 iter: 142 reconn_loss: 10410.6064453125 kl_loss: 1508.9150390625\n",
      "epoch: 6 iter: 143 reconn_loss: 10517.8095703125 kl_loss: 1513.752685546875\n",
      "epoch: 6 iter: 144 reconn_loss: 10225.9970703125 kl_loss: 1626.6077880859375\n",
      "epoch: 6 iter: 145 reconn_loss: 10352.3212890625 kl_loss: 1542.7076416015625\n",
      "epoch: 6 iter: 146 reconn_loss: 10169.3193359375 kl_loss: 1552.3067626953125\n",
      "epoch: 6 iter: 147 reconn_loss: 10327.0458984375 kl_loss: 1492.5927734375\n",
      "epoch: 6 iter: 148 reconn_loss: 10509.01953125 kl_loss: 1517.25439453125\n",
      "epoch: 6 iter: 149 reconn_loss: 10104.611328125 kl_loss: 1542.326416015625\n",
      "epoch: 6 iter: 150 reconn_loss: 10185.36328125 kl_loss: 1571.57568359375\n",
      "epoch: 6 iter: 151 reconn_loss: 10183.416015625 kl_loss: 1473.215087890625\n",
      "epoch: 6 iter: 152 reconn_loss: 10414.361328125 kl_loss: 1505.8568115234375\n",
      "epoch: 6 iter: 153 reconn_loss: 10523.009765625 kl_loss: 1546.4879150390625\n",
      "epoch: 6 iter: 154 reconn_loss: 10245.6318359375 kl_loss: 1563.8509521484375\n",
      "epoch: 6 iter: 155 reconn_loss: 10606.2021484375 kl_loss: 1567.0386962890625\n",
      "epoch: 6 iter: 156 reconn_loss: 10793.2509765625 kl_loss: 1562.4071044921875\n",
      "epoch: 6 iter: 157 reconn_loss: 10207.896484375 kl_loss: 1519.98779296875\n",
      "epoch: 6 iter: 158 reconn_loss: 10377.546875 kl_loss: 1495.566650390625\n",
      "epoch: 6 iter: 159 reconn_loss: 10804.095703125 kl_loss: 1506.37451171875\n",
      "epoch: 6 iter: 160 reconn_loss: 10429.658203125 kl_loss: 1434.648193359375\n",
      "epoch: 6 iter: 161 reconn_loss: 10040.904296875 kl_loss: 1525.29150390625\n",
      "epoch: 6 iter: 162 reconn_loss: 10272.4306640625 kl_loss: 1508.8779296875\n",
      "epoch: 6 iter: 163 reconn_loss: 10516.216796875 kl_loss: 1530.55615234375\n",
      "epoch: 6 iter: 164 reconn_loss: 10258.7373046875 kl_loss: 1593.5947265625\n",
      "epoch: 6 iter: 165 reconn_loss: 10234.9208984375 kl_loss: 1585.564697265625\n",
      "epoch: 6 iter: 166 reconn_loss: 10408.9873046875 kl_loss: 1531.692138671875\n",
      "epoch: 6 iter: 167 reconn_loss: 10584.66796875 kl_loss: 1536.384033203125\n",
      "epoch: 6 iter: 168 reconn_loss: 10408.5029296875 kl_loss: 1534.0343017578125\n",
      "epoch: 6 iter: 169 reconn_loss: 10329.6767578125 kl_loss: 1476.9840087890625\n",
      "epoch: 6 iter: 170 reconn_loss: 10488.0830078125 kl_loss: 1589.4674072265625\n",
      "epoch: 6 iter: 171 reconn_loss: 10215.2041015625 kl_loss: 1508.981201171875\n",
      "epoch: 6 iter: 172 reconn_loss: 10238.0673828125 kl_loss: 1488.327880859375\n",
      "epoch: 6 iter: 173 reconn_loss: 10290.71875 kl_loss: 1535.70166015625\n",
      "epoch: 6 iter: 174 reconn_loss: 10455.4638671875 kl_loss: 1541.734375\n",
      "epoch: 6 iter: 175 reconn_loss: 10516.9677734375 kl_loss: 1561.294677734375\n",
      "epoch: 6 iter: 176 reconn_loss: 10339.396484375 kl_loss: 1503.0048828125\n",
      "epoch: 6 iter: 177 reconn_loss: 10471.482421875 kl_loss: 1443.4461669921875\n",
      "epoch: 6 iter: 178 reconn_loss: 10366.23828125 kl_loss: 1530.4183349609375\n",
      "epoch: 6 iter: 179 reconn_loss: 10388.5517578125 kl_loss: 1480.3695068359375\n",
      "epoch: 6 iter: 180 reconn_loss: 10367.080078125 kl_loss: 1535.561767578125\n",
      "epoch: 6 iter: 181 reconn_loss: 10467.2880859375 kl_loss: 1484.4052734375\n",
      "epoch: 6 iter: 182 reconn_loss: 10232.052734375 kl_loss: 1640.10888671875\n",
      "epoch: 6 iter: 183 reconn_loss: 10045.7734375 kl_loss: 1600.50048828125\n",
      "epoch: 6 iter: 184 reconn_loss: 10360.02734375 kl_loss: 1538.7041015625\n",
      "epoch: 6 iter: 185 reconn_loss: 10404.734375 kl_loss: 1526.6212158203125\n",
      "epoch: 6 iter: 186 reconn_loss: 10230.6708984375 kl_loss: 1549.456787109375\n",
      "epoch: 6 iter: 187 reconn_loss: 10335.4208984375 kl_loss: 1476.655029296875\n",
      "epoch: 6 iter: 188 reconn_loss: 10279.408203125 kl_loss: 1539.160888671875\n",
      "epoch: 6 iter: 189 reconn_loss: 10118.1083984375 kl_loss: 1550.117919921875\n",
      "epoch: 6 iter: 190 reconn_loss: 10617.568359375 kl_loss: 1467.04345703125\n",
      "epoch: 6 iter: 191 reconn_loss: 10343.302734375 kl_loss: 1564.68017578125\n",
      "epoch: 6 iter: 192 reconn_loss: 10655.25 kl_loss: 1481.15185546875\n",
      "epoch: 6 iter: 193 reconn_loss: 10333.486328125 kl_loss: 1547.5111083984375\n",
      "epoch: 6 iter: 194 reconn_loss: 10316.7333984375 kl_loss: 1553.9580078125\n",
      "epoch: 6 iter: 195 reconn_loss: 9864.951171875 kl_loss: 1586.341552734375\n",
      "epoch: 6 iter: 196 reconn_loss: 10418.92578125 kl_loss: 1537.0906982421875\n",
      "epoch: 6 iter: 197 reconn_loss: 10520.8212890625 kl_loss: 1549.976318359375\n",
      "epoch: 6 iter: 198 reconn_loss: 10594.9990234375 kl_loss: 1575.071044921875\n",
      "epoch: 6 iter: 199 reconn_loss: 10096.025390625 kl_loss: 1500.47900390625\n",
      "epoch: 6 iter: 200 reconn_loss: 9977.0595703125 kl_loss: 1495.5482177734375\n",
      "epoch: 6 iter: 201 reconn_loss: 10323.951171875 kl_loss: 1497.412353515625\n",
      "epoch: 6 iter: 202 reconn_loss: 10371.75390625 kl_loss: 1552.5416259765625\n",
      "epoch: 6 iter: 203 reconn_loss: 10481.1474609375 kl_loss: 1491.957763671875\n",
      "epoch: 6 iter: 204 reconn_loss: 9946.4169921875 kl_loss: 1514.3563232421875\n",
      "epoch: 6 iter: 205 reconn_loss: 10297.3994140625 kl_loss: 1553.0196533203125\n",
      "epoch: 6 iter: 206 reconn_loss: 10571.6806640625 kl_loss: 1478.5677490234375\n",
      "epoch: 6 iter: 207 reconn_loss: 10362.724609375 kl_loss: 1534.08349609375\n",
      "epoch: 6 iter: 208 reconn_loss: 10293.6796875 kl_loss: 1559.07275390625\n",
      "epoch: 6 iter: 209 reconn_loss: 10436.81640625 kl_loss: 1532.687744140625\n",
      "epoch: 6 iter: 210 reconn_loss: 10311.7939453125 kl_loss: 1512.7342529296875\n",
      "epoch: 6 iter: 211 reconn_loss: 10403.3896484375 kl_loss: 1502.3843994140625\n",
      "epoch: 6 iter: 212 reconn_loss: 10503.98046875 kl_loss: 1542.23388671875\n",
      "epoch: 6 iter: 213 reconn_loss: 10039.541015625 kl_loss: 1548.2333984375\n",
      "epoch: 6 iter: 214 reconn_loss: 10118.947265625 kl_loss: 1542.56396484375\n",
      "epoch: 6 iter: 215 reconn_loss: 10315.79296875 kl_loss: 1560.5179443359375\n",
      "epoch: 6 iter: 216 reconn_loss: 10114.388671875 kl_loss: 1594.97607421875\n",
      "epoch: 6 iter: 217 reconn_loss: 9972.3212890625 kl_loss: 1524.141845703125\n",
      "epoch: 6 iter: 218 reconn_loss: 9768.0576171875 kl_loss: 1584.113525390625\n",
      "epoch: 6 iter: 219 reconn_loss: 10262.537109375 kl_loss: 1514.0570068359375\n",
      "epoch: 6 iter: 220 reconn_loss: 10434.7919921875 kl_loss: 1568.567138671875\n",
      "epoch: 6 iter: 221 reconn_loss: 10474.3720703125 kl_loss: 1560.008544921875\n",
      "epoch: 6 iter: 222 reconn_loss: 6997.8994140625 kl_loss: 988.2608032226562\n",
      "0.weight tensor(59.5233) tensor(-57.7904)\n",
      "0.bias tensor(21.5165) tensor(-34.0470)\n",
      "2.weight tensor(9.8436) tensor(-15.8815)\n",
      "2.bias tensor(7.1523) tensor(-8.5078)\n",
      "4.weight tensor(6.9213) tensor(-8.6850)\n",
      "4.bias tensor(4.6218) tensor(-5.2738)\n",
      "epoch: 7 iter: 0 reconn_loss: 10173.734375 kl_loss: 1578.169921875\n",
      "epoch: 7 iter: 1 reconn_loss: 10175.443359375 kl_loss: 1584.88330078125\n",
      "epoch: 7 iter: 2 reconn_loss: 10340.12109375 kl_loss: 1621.21240234375\n",
      "epoch: 7 iter: 3 reconn_loss: 10376.5810546875 kl_loss: 1559.016845703125\n",
      "epoch: 7 iter: 4 reconn_loss: 10268.548828125 kl_loss: 1565.2906494140625\n",
      "epoch: 7 iter: 5 reconn_loss: 10204.44921875 kl_loss: 1546.254150390625\n",
      "epoch: 7 iter: 6 reconn_loss: 10028.08984375 kl_loss: 1587.493896484375\n",
      "epoch: 7 iter: 7 reconn_loss: 10469.72265625 kl_loss: 1654.6695556640625\n",
      "epoch: 7 iter: 8 reconn_loss: 10353.8681640625 kl_loss: 1585.2952880859375\n",
      "epoch: 7 iter: 9 reconn_loss: 9896.88671875 kl_loss: 1584.3924560546875\n",
      "epoch: 7 iter: 10 reconn_loss: 10369.751953125 kl_loss: 1587.1251220703125\n",
      "epoch: 7 iter: 11 reconn_loss: 10302.7822265625 kl_loss: 1540.2855224609375\n",
      "epoch: 7 iter: 12 reconn_loss: 10308.6220703125 kl_loss: 1575.61328125\n",
      "epoch: 7 iter: 13 reconn_loss: 10202.8056640625 kl_loss: 1554.475341796875\n",
      "epoch: 7 iter: 14 reconn_loss: 9990.6806640625 kl_loss: 1488.428466796875\n",
      "epoch: 7 iter: 15 reconn_loss: 10381.51171875 kl_loss: 1585.55322265625\n",
      "epoch: 7 iter: 16 reconn_loss: 10041.89453125 kl_loss: 1543.9678955078125\n",
      "epoch: 7 iter: 17 reconn_loss: 10199.908203125 kl_loss: 1481.0352783203125\n",
      "epoch: 7 iter: 18 reconn_loss: 10408.9990234375 kl_loss: 1535.893798828125\n",
      "epoch: 7 iter: 19 reconn_loss: 10428.19140625 kl_loss: 1623.4425048828125\n",
      "epoch: 7 iter: 20 reconn_loss: 10569.23828125 kl_loss: 1574.195068359375\n",
      "epoch: 7 iter: 21 reconn_loss: 10211.791015625 kl_loss: 1486.8570556640625\n",
      "epoch: 7 iter: 22 reconn_loss: 10211.216796875 kl_loss: 1653.5289306640625\n",
      "epoch: 7 iter: 23 reconn_loss: 10370.83984375 kl_loss: 1530.379638671875\n",
      "epoch: 7 iter: 24 reconn_loss: 10307.900390625 kl_loss: 1549.693603515625\n",
      "epoch: 7 iter: 25 reconn_loss: 10009.765625 kl_loss: 1573.0318603515625\n",
      "epoch: 7 iter: 26 reconn_loss: 10124.0283203125 kl_loss: 1531.4508056640625\n",
      "epoch: 7 iter: 27 reconn_loss: 10228.5 kl_loss: 1520.0687255859375\n",
      "epoch: 7 iter: 28 reconn_loss: 9932.517578125 kl_loss: 1503.13525390625\n",
      "epoch: 7 iter: 29 reconn_loss: 10226.490234375 kl_loss: 1585.748046875\n",
      "epoch: 7 iter: 30 reconn_loss: 10448.25390625 kl_loss: 1596.47119140625\n",
      "epoch: 7 iter: 31 reconn_loss: 10068.6748046875 kl_loss: 1628.5595703125\n",
      "epoch: 7 iter: 32 reconn_loss: 10099.4609375 kl_loss: 1605.27783203125\n",
      "epoch: 7 iter: 33 reconn_loss: 10050.3798828125 kl_loss: 1585.0889892578125\n",
      "epoch: 7 iter: 34 reconn_loss: 10073.650390625 kl_loss: 1530.304931640625\n",
      "epoch: 7 iter: 35 reconn_loss: 10130.15625 kl_loss: 1536.3724365234375\n",
      "epoch: 7 iter: 36 reconn_loss: 9794.31640625 kl_loss: 1518.4189453125\n",
      "epoch: 7 iter: 37 reconn_loss: 10336.982421875 kl_loss: 1508.83544921875\n",
      "epoch: 7 iter: 38 reconn_loss: 10324.798828125 kl_loss: 1573.79345703125\n",
      "epoch: 7 iter: 39 reconn_loss: 9921.947265625 kl_loss: 1612.6722412109375\n",
      "epoch: 7 iter: 40 reconn_loss: 10380.41796875 kl_loss: 1576.01611328125\n",
      "epoch: 7 iter: 41 reconn_loss: 10250.5908203125 kl_loss: 1488.3836669921875\n",
      "epoch: 7 iter: 42 reconn_loss: 10263.671875 kl_loss: 1584.95654296875\n",
      "epoch: 7 iter: 43 reconn_loss: 10287.2265625 kl_loss: 1575.12158203125\n",
      "epoch: 7 iter: 44 reconn_loss: 10356.44921875 kl_loss: 1566.26513671875\n",
      "epoch: 7 iter: 45 reconn_loss: 10035.205078125 kl_loss: 1540.5224609375\n",
      "epoch: 7 iter: 46 reconn_loss: 9970.6640625 kl_loss: 1548.0015869140625\n",
      "epoch: 7 iter: 47 reconn_loss: 10274.3544921875 kl_loss: 1592.3197021484375\n",
      "epoch: 7 iter: 48 reconn_loss: 10415.294921875 kl_loss: 1580.3533935546875\n",
      "epoch: 7 iter: 49 reconn_loss: 9982.1435546875 kl_loss: 1560.385986328125\n",
      "epoch: 7 iter: 50 reconn_loss: 10079.0927734375 kl_loss: 1552.1455078125\n",
      "epoch: 7 iter: 51 reconn_loss: 10053.4951171875 kl_loss: 1567.5335693359375\n",
      "epoch: 7 iter: 52 reconn_loss: 10206.2958984375 kl_loss: 1567.795654296875\n",
      "epoch: 7 iter: 53 reconn_loss: 10050.7626953125 kl_loss: 1503.122314453125\n",
      "epoch: 7 iter: 54 reconn_loss: 10459.314453125 kl_loss: 1476.39501953125\n",
      "epoch: 7 iter: 55 reconn_loss: 10140.365234375 kl_loss: 1566.9884033203125\n",
      "epoch: 7 iter: 56 reconn_loss: 10334.921875 kl_loss: 1552.11669921875\n",
      "epoch: 7 iter: 57 reconn_loss: 10278.7119140625 kl_loss: 1563.9420166015625\n",
      "epoch: 7 iter: 58 reconn_loss: 10019.7724609375 kl_loss: 1566.4185791015625\n",
      "epoch: 7 iter: 59 reconn_loss: 10435.947265625 kl_loss: 1521.68017578125\n",
      "epoch: 7 iter: 60 reconn_loss: 9792.091796875 kl_loss: 1605.978515625\n",
      "epoch: 7 iter: 61 reconn_loss: 9859.423828125 kl_loss: 1547.5\n",
      "epoch: 7 iter: 62 reconn_loss: 10312.361328125 kl_loss: 1586.99951171875\n",
      "epoch: 7 iter: 63 reconn_loss: 9991.314453125 kl_loss: 1542.4449462890625\n",
      "epoch: 7 iter: 64 reconn_loss: 10104.703125 kl_loss: 1487.7674560546875\n",
      "epoch: 7 iter: 65 reconn_loss: 10293.091796875 kl_loss: 1559.219482421875\n",
      "epoch: 7 iter: 66 reconn_loss: 10430.908203125 kl_loss: 1531.3983154296875\n",
      "epoch: 7 iter: 67 reconn_loss: 10324.98828125 kl_loss: 1563.5406494140625\n",
      "epoch: 7 iter: 68 reconn_loss: 10268.5185546875 kl_loss: 1527.059326171875\n",
      "epoch: 7 iter: 69 reconn_loss: 10118.6484375 kl_loss: 1497.147216796875\n",
      "epoch: 7 iter: 70 reconn_loss: 10092.03515625 kl_loss: 1529.2576904296875\n",
      "epoch: 7 iter: 71 reconn_loss: 9999.5615234375 kl_loss: 1539.005126953125\n",
      "epoch: 7 iter: 72 reconn_loss: 10342.7197265625 kl_loss: 1541.115966796875\n",
      "epoch: 7 iter: 73 reconn_loss: 10238.6484375 kl_loss: 1503.87646484375\n",
      "epoch: 7 iter: 74 reconn_loss: 10081.224609375 kl_loss: 1561.12158203125\n",
      "epoch: 7 iter: 75 reconn_loss: 10021.572265625 kl_loss: 1531.88232421875\n",
      "epoch: 7 iter: 76 reconn_loss: 9907.7490234375 kl_loss: 1583.8609619140625\n",
      "epoch: 7 iter: 77 reconn_loss: 10072.109375 kl_loss: 1657.4437255859375\n",
      "epoch: 7 iter: 78 reconn_loss: 10097.7080078125 kl_loss: 1573.7156982421875\n",
      "epoch: 7 iter: 79 reconn_loss: 10181.9892578125 kl_loss: 1575.9600830078125\n",
      "epoch: 7 iter: 80 reconn_loss: 9981.671875 kl_loss: 1601.8486328125\n",
      "epoch: 7 iter: 81 reconn_loss: 9609.1064453125 kl_loss: 1580.185302734375\n",
      "epoch: 7 iter: 82 reconn_loss: 10039.8837890625 kl_loss: 1543.0318603515625\n",
      "epoch: 7 iter: 83 reconn_loss: 9779.939453125 kl_loss: 1568.7232666015625\n",
      "epoch: 7 iter: 84 reconn_loss: 9959.5673828125 kl_loss: 1570.652099609375\n",
      "epoch: 7 iter: 85 reconn_loss: 10247.197265625 kl_loss: 1482.22216796875\n",
      "epoch: 7 iter: 86 reconn_loss: 9986.669921875 kl_loss: 1568.168212890625\n",
      "epoch: 7 iter: 87 reconn_loss: 10075.25 kl_loss: 1513.7427978515625\n",
      "epoch: 7 iter: 88 reconn_loss: 10135.8427734375 kl_loss: 1499.333740234375\n",
      "epoch: 7 iter: 89 reconn_loss: 10184.6796875 kl_loss: 1499.8912353515625\n",
      "epoch: 7 iter: 90 reconn_loss: 10396.1240234375 kl_loss: 1512.637451171875\n",
      "epoch: 7 iter: 91 reconn_loss: 10264.8115234375 kl_loss: 1547.909423828125\n",
      "epoch: 7 iter: 92 reconn_loss: 10225.75 kl_loss: 1620.772216796875\n",
      "epoch: 7 iter: 93 reconn_loss: 9972.3447265625 kl_loss: 1651.2994384765625\n",
      "epoch: 7 iter: 94 reconn_loss: 9756.8388671875 kl_loss: 1651.6356201171875\n",
      "epoch: 7 iter: 95 reconn_loss: 10204.4951171875 kl_loss: 1661.998291015625\n",
      "epoch: 7 iter: 96 reconn_loss: 9912.0693359375 kl_loss: 1510.869873046875\n",
      "epoch: 7 iter: 97 reconn_loss: 9787.7060546875 kl_loss: 1610.5093994140625\n",
      "epoch: 7 iter: 98 reconn_loss: 9942.3017578125 kl_loss: 1617.013427734375\n",
      "epoch: 7 iter: 99 reconn_loss: 9748.69921875 kl_loss: 1519.689208984375\n",
      "epoch: 7 iter: 100 reconn_loss: 10438.0458984375 kl_loss: 1541.3299560546875\n",
      "epoch: 7 iter: 101 reconn_loss: 10027.123046875 kl_loss: 1554.01513671875\n",
      "epoch: 7 iter: 102 reconn_loss: 10248.8359375 kl_loss: 1568.4046630859375\n",
      "epoch: 7 iter: 103 reconn_loss: 9902.9296875 kl_loss: 1640.004150390625\n",
      "epoch: 7 iter: 104 reconn_loss: 9991.41796875 kl_loss: 1536.1051025390625\n",
      "epoch: 7 iter: 105 reconn_loss: 10051.6767578125 kl_loss: 1696.4903564453125\n",
      "epoch: 7 iter: 106 reconn_loss: 10256.3701171875 kl_loss: 1579.1336669921875\n",
      "epoch: 7 iter: 107 reconn_loss: 10225.5673828125 kl_loss: 1625.964599609375\n",
      "epoch: 7 iter: 108 reconn_loss: 10040.94140625 kl_loss: 1565.52392578125\n",
      "epoch: 7 iter: 109 reconn_loss: 9988.5576171875 kl_loss: 1630.2371826171875\n",
      "epoch: 7 iter: 110 reconn_loss: 10253.40234375 kl_loss: 1540.90673828125\n",
      "epoch: 7 iter: 111 reconn_loss: 10032.7255859375 kl_loss: 1580.0390625\n",
      "epoch: 7 iter: 112 reconn_loss: 10109.875 kl_loss: 1578.61181640625\n",
      "epoch: 7 iter: 113 reconn_loss: 9795.404296875 kl_loss: 1570.6356201171875\n",
      "epoch: 7 iter: 114 reconn_loss: 10224.4208984375 kl_loss: 1559.1737060546875\n",
      "epoch: 7 iter: 115 reconn_loss: 9685.7685546875 kl_loss: 1562.718505859375\n",
      "epoch: 7 iter: 116 reconn_loss: 10281.1005859375 kl_loss: 1562.6317138671875\n",
      "epoch: 7 iter: 117 reconn_loss: 9815.3642578125 kl_loss: 1564.938720703125\n",
      "epoch: 7 iter: 118 reconn_loss: 10255.1923828125 kl_loss: 1541.2066650390625\n",
      "epoch: 7 iter: 119 reconn_loss: 10447.705078125 kl_loss: 1570.8873291015625\n",
      "epoch: 7 iter: 120 reconn_loss: 10191.0400390625 kl_loss: 1572.2371826171875\n",
      "epoch: 7 iter: 121 reconn_loss: 10536.6142578125 kl_loss: 1523.7535400390625\n",
      "epoch: 7 iter: 122 reconn_loss: 10123.8828125 kl_loss: 1535.2198486328125\n",
      "epoch: 7 iter: 123 reconn_loss: 10000.072265625 kl_loss: 1583.58642578125\n",
      "epoch: 7 iter: 124 reconn_loss: 9972.751953125 kl_loss: 1579.734619140625\n",
      "epoch: 7 iter: 125 reconn_loss: 9983.376953125 kl_loss: 1672.4722900390625\n",
      "epoch: 7 iter: 126 reconn_loss: 9758.474609375 kl_loss: 1581.00048828125\n",
      "epoch: 7 iter: 127 reconn_loss: 10445.048828125 kl_loss: 1619.00439453125\n",
      "epoch: 7 iter: 128 reconn_loss: 9966.1455078125 kl_loss: 1598.305419921875\n",
      "epoch: 7 iter: 129 reconn_loss: 9904.2080078125 kl_loss: 1559.9495849609375\n",
      "epoch: 7 iter: 130 reconn_loss: 9713.130859375 kl_loss: 1585.9085693359375\n",
      "epoch: 7 iter: 131 reconn_loss: 9997.416015625 kl_loss: 1623.47705078125\n",
      "epoch: 7 iter: 132 reconn_loss: 10079.8076171875 kl_loss: 1657.078125\n",
      "epoch: 7 iter: 133 reconn_loss: 10389.767578125 kl_loss: 1583.88232421875\n",
      "epoch: 7 iter: 134 reconn_loss: 10009.4306640625 kl_loss: 1607.454345703125\n",
      "epoch: 7 iter: 135 reconn_loss: 10084.1298828125 kl_loss: 1602.3382568359375\n",
      "epoch: 7 iter: 136 reconn_loss: 10224.431640625 kl_loss: 1527.3226318359375\n",
      "epoch: 7 iter: 137 reconn_loss: 9980.693359375 kl_loss: 1558.1199951171875\n",
      "epoch: 7 iter: 138 reconn_loss: 9632.298828125 kl_loss: 1560.8265380859375\n",
      "epoch: 7 iter: 139 reconn_loss: 10086.2578125 kl_loss: 1518.80322265625\n",
      "epoch: 7 iter: 140 reconn_loss: 9830.3701171875 kl_loss: 1536.468017578125\n",
      "epoch: 7 iter: 141 reconn_loss: 9788.880859375 kl_loss: 1526.870849609375\n",
      "epoch: 7 iter: 142 reconn_loss: 10066.986328125 kl_loss: 1646.63525390625\n",
      "epoch: 7 iter: 143 reconn_loss: 10500.4931640625 kl_loss: 1657.163818359375\n",
      "epoch: 7 iter: 144 reconn_loss: 10579.330078125 kl_loss: 1587.840576171875\n",
      "epoch: 7 iter: 145 reconn_loss: 9782.896484375 kl_loss: 1564.742431640625\n",
      "epoch: 7 iter: 146 reconn_loss: 10419.251953125 kl_loss: 1622.61376953125\n",
      "epoch: 7 iter: 147 reconn_loss: 10080.19921875 kl_loss: 1671.2542724609375\n",
      "epoch: 7 iter: 148 reconn_loss: 10091.568359375 kl_loss: 1630.485107421875\n",
      "epoch: 7 iter: 149 reconn_loss: 10080.8037109375 kl_loss: 1581.7459716796875\n",
      "epoch: 7 iter: 150 reconn_loss: 10035.6982421875 kl_loss: 1569.495361328125\n",
      "epoch: 7 iter: 151 reconn_loss: 9847.9990234375 kl_loss: 1545.161376953125\n",
      "epoch: 7 iter: 152 reconn_loss: 10151.26171875 kl_loss: 1600.156982421875\n",
      "epoch: 7 iter: 153 reconn_loss: 9847.53125 kl_loss: 1590.8927001953125\n",
      "epoch: 7 iter: 154 reconn_loss: 10025.830078125 kl_loss: 1545.9957275390625\n",
      "epoch: 7 iter: 155 reconn_loss: 10391.3427734375 kl_loss: 1654.3353271484375\n",
      "epoch: 7 iter: 156 reconn_loss: 10182.6650390625 kl_loss: 1565.357421875\n",
      "epoch: 7 iter: 157 reconn_loss: 9722.357421875 kl_loss: 1528.860595703125\n",
      "epoch: 7 iter: 158 reconn_loss: 10216.4609375 kl_loss: 1585.2144775390625\n",
      "epoch: 7 iter: 159 reconn_loss: 10125.3935546875 kl_loss: 1640.0655517578125\n",
      "epoch: 7 iter: 160 reconn_loss: 10028.5517578125 kl_loss: 1490.3433837890625\n",
      "epoch: 7 iter: 161 reconn_loss: 9674.861328125 kl_loss: 1595.61181640625\n",
      "epoch: 7 iter: 162 reconn_loss: 10377.796875 kl_loss: 1553.3046875\n",
      "epoch: 7 iter: 163 reconn_loss: 9933.4765625 kl_loss: 1610.228515625\n",
      "epoch: 7 iter: 164 reconn_loss: 10179.7392578125 kl_loss: 1562.9892578125\n",
      "epoch: 7 iter: 165 reconn_loss: 10285.42578125 kl_loss: 1577.1209716796875\n",
      "epoch: 7 iter: 166 reconn_loss: 10627.7275390625 kl_loss: 1607.7794189453125\n",
      "epoch: 7 iter: 167 reconn_loss: 10216.8525390625 kl_loss: 1545.4083251953125\n",
      "epoch: 7 iter: 168 reconn_loss: 9961.9697265625 kl_loss: 1589.609619140625\n",
      "epoch: 7 iter: 169 reconn_loss: 10225.3173828125 kl_loss: 1563.916748046875\n",
      "epoch: 7 iter: 170 reconn_loss: 10130.75390625 kl_loss: 1562.76806640625\n",
      "epoch: 7 iter: 171 reconn_loss: 10231.9609375 kl_loss: 1582.317138671875\n",
      "epoch: 7 iter: 172 reconn_loss: 9890.90625 kl_loss: 1588.4951171875\n",
      "epoch: 7 iter: 173 reconn_loss: 9880.5361328125 kl_loss: 1617.0771484375\n",
      "epoch: 7 iter: 174 reconn_loss: 9905.1005859375 kl_loss: 1622.135498046875\n",
      "epoch: 7 iter: 175 reconn_loss: 9924.759765625 kl_loss: 1597.97705078125\n",
      "epoch: 7 iter: 176 reconn_loss: 10202.8671875 kl_loss: 1558.801025390625\n",
      "epoch: 7 iter: 177 reconn_loss: 9667.9130859375 kl_loss: 1562.572021484375\n",
      "epoch: 7 iter: 178 reconn_loss: 10533.220703125 kl_loss: 1584.31982421875\n",
      "epoch: 7 iter: 179 reconn_loss: 10289.0302734375 kl_loss: 1578.5733642578125\n",
      "epoch: 7 iter: 180 reconn_loss: 10402.1103515625 kl_loss: 1568.0965576171875\n",
      "epoch: 7 iter: 181 reconn_loss: 9553.44140625 kl_loss: 1596.4501953125\n",
      "epoch: 7 iter: 182 reconn_loss: 10255.3134765625 kl_loss: 1554.1668701171875\n",
      "epoch: 7 iter: 183 reconn_loss: 10212.6240234375 kl_loss: 1563.7021484375\n",
      "epoch: 7 iter: 184 reconn_loss: 9921.703125 kl_loss: 1661.162841796875\n",
      "epoch: 7 iter: 185 reconn_loss: 10159.3310546875 kl_loss: 1572.819091796875\n",
      "epoch: 7 iter: 186 reconn_loss: 9888.30078125 kl_loss: 1559.88916015625\n",
      "epoch: 7 iter: 187 reconn_loss: 10282.4609375 kl_loss: 1595.4072265625\n",
      "epoch: 7 iter: 188 reconn_loss: 10124.669921875 kl_loss: 1594.64892578125\n",
      "epoch: 7 iter: 189 reconn_loss: 9831.220703125 kl_loss: 1582.365966796875\n",
      "epoch: 7 iter: 190 reconn_loss: 10424.583984375 kl_loss: 1551.5704345703125\n",
      "epoch: 7 iter: 191 reconn_loss: 9864.81640625 kl_loss: 1581.31689453125\n",
      "epoch: 7 iter: 192 reconn_loss: 9847.2255859375 kl_loss: 1587.383056640625\n",
      "epoch: 7 iter: 193 reconn_loss: 10190.86328125 kl_loss: 1591.22998046875\n",
      "epoch: 7 iter: 194 reconn_loss: 10424.666015625 kl_loss: 1588.843505859375\n",
      "epoch: 7 iter: 195 reconn_loss: 9688.162109375 kl_loss: 1600.4097900390625\n",
      "epoch: 7 iter: 196 reconn_loss: 10351.208984375 kl_loss: 1550.535888671875\n",
      "epoch: 7 iter: 197 reconn_loss: 10106.171875 kl_loss: 1734.58837890625\n",
      "epoch: 7 iter: 198 reconn_loss: 10109.2958984375 kl_loss: 1609.7728271484375\n",
      "epoch: 7 iter: 199 reconn_loss: 10444.240234375 kl_loss: 1646.60009765625\n",
      "epoch: 7 iter: 200 reconn_loss: 10069.1865234375 kl_loss: 1638.479248046875\n",
      "epoch: 7 iter: 201 reconn_loss: 10237.9658203125 kl_loss: 1606.3287353515625\n",
      "epoch: 7 iter: 202 reconn_loss: 10088.5498046875 kl_loss: 1587.2486572265625\n",
      "epoch: 7 iter: 203 reconn_loss: 10385.205078125 kl_loss: 1553.9376220703125\n",
      "epoch: 7 iter: 204 reconn_loss: 10148.1015625 kl_loss: 1605.28271484375\n",
      "epoch: 7 iter: 205 reconn_loss: 9964.34375 kl_loss: 1585.6246337890625\n",
      "epoch: 7 iter: 206 reconn_loss: 10052.3759765625 kl_loss: 1662.611083984375\n",
      "epoch: 7 iter: 207 reconn_loss: 10037.375 kl_loss: 1616.86474609375\n",
      "epoch: 7 iter: 208 reconn_loss: 10183.11328125 kl_loss: 1616.2113037109375\n",
      "epoch: 7 iter: 209 reconn_loss: 9728.2548828125 kl_loss: 1624.1488037109375\n",
      "epoch: 7 iter: 210 reconn_loss: 10248.49609375 kl_loss: 1635.931396484375\n",
      "epoch: 7 iter: 211 reconn_loss: 9934.9033203125 kl_loss: 1644.6083984375\n",
      "epoch: 7 iter: 212 reconn_loss: 10308.3515625 kl_loss: 1583.947265625\n",
      "epoch: 7 iter: 213 reconn_loss: 9991.90234375 kl_loss: 1616.95654296875\n",
      "epoch: 7 iter: 214 reconn_loss: 10083.88671875 kl_loss: 1614.2738037109375\n",
      "epoch: 7 iter: 215 reconn_loss: 10162.0146484375 kl_loss: 1567.3321533203125\n",
      "epoch: 7 iter: 216 reconn_loss: 10203.27734375 kl_loss: 1574.59912109375\n",
      "epoch: 7 iter: 217 reconn_loss: 10015.66796875 kl_loss: 1608.1031494140625\n",
      "epoch: 7 iter: 218 reconn_loss: 9861.6181640625 kl_loss: 1559.3367919921875\n",
      "epoch: 7 iter: 219 reconn_loss: 9689.091796875 kl_loss: 1644.1143798828125\n",
      "epoch: 7 iter: 220 reconn_loss: 9844.6015625 kl_loss: 1625.7205810546875\n",
      "epoch: 7 iter: 221 reconn_loss: 10410.69140625 kl_loss: 1622.7362060546875\n",
      "epoch: 7 iter: 222 reconn_loss: 6574.4248046875 kl_loss: 1065.681640625\n",
      "0.weight tensor(61.2687) tensor(-59.7193)\n",
      "0.bias tensor(31.3649) tensor(-29.8201)\n",
      "2.weight tensor(16.1833) tensor(-21.4112)\n",
      "2.bias tensor(6.4249) tensor(-10.6567)\n",
      "4.weight tensor(6.6743) tensor(-10.4803)\n",
      "4.bias tensor(4.0286) tensor(-6.8896)\n",
      "epoch: 8 iter: 0 reconn_loss: 9846.1748046875 kl_loss: 1580.0751953125\n",
      "epoch: 8 iter: 1 reconn_loss: 10140.099609375 kl_loss: 1667.38134765625\n",
      "epoch: 8 iter: 2 reconn_loss: 9851.345703125 kl_loss: 1572.02490234375\n",
      "epoch: 8 iter: 3 reconn_loss: 9938.0234375 kl_loss: 1666.4344482421875\n",
      "epoch: 8 iter: 4 reconn_loss: 10027.3642578125 kl_loss: 1624.499267578125\n",
      "epoch: 8 iter: 5 reconn_loss: 9922.091796875 kl_loss: 1636.128662109375\n",
      "epoch: 8 iter: 6 reconn_loss: 9760.4951171875 kl_loss: 1563.6494140625\n",
      "epoch: 8 iter: 7 reconn_loss: 9525.94921875 kl_loss: 1650.68115234375\n",
      "epoch: 8 iter: 8 reconn_loss: 10215.01171875 kl_loss: 1619.16455078125\n",
      "epoch: 8 iter: 9 reconn_loss: 10251.7978515625 kl_loss: 1672.475341796875\n",
      "epoch: 8 iter: 10 reconn_loss: 9915.833984375 kl_loss: 1651.0107421875\n",
      "epoch: 8 iter: 11 reconn_loss: 10043.328125 kl_loss: 1568.1768798828125\n",
      "epoch: 8 iter: 12 reconn_loss: 10129.6845703125 kl_loss: 1547.065673828125\n",
      "epoch: 8 iter: 13 reconn_loss: 10080.2412109375 kl_loss: 1669.4361572265625\n",
      "epoch: 8 iter: 14 reconn_loss: 10004.36328125 kl_loss: 1557.4505615234375\n",
      "epoch: 8 iter: 15 reconn_loss: 9925.0419921875 kl_loss: 1635.1492919921875\n",
      "epoch: 8 iter: 16 reconn_loss: 9749.3642578125 kl_loss: 1639.192138671875\n",
      "epoch: 8 iter: 17 reconn_loss: 10140.212890625 kl_loss: 1674.8509521484375\n",
      "epoch: 8 iter: 18 reconn_loss: 10128.41015625 kl_loss: 1600.78662109375\n",
      "epoch: 8 iter: 19 reconn_loss: 10002.1318359375 kl_loss: 1569.130615234375\n",
      "epoch: 8 iter: 20 reconn_loss: 10142.068359375 kl_loss: 1662.697021484375\n",
      "epoch: 8 iter: 21 reconn_loss: 10201.265625 kl_loss: 1691.9422607421875\n",
      "epoch: 8 iter: 22 reconn_loss: 9819.740234375 kl_loss: 1614.640869140625\n",
      "epoch: 8 iter: 23 reconn_loss: 10135.34375 kl_loss: 1675.3170166015625\n",
      "epoch: 8 iter: 24 reconn_loss: 10072.140625 kl_loss: 1628.68798828125\n",
      "epoch: 8 iter: 25 reconn_loss: 9968.3154296875 kl_loss: 1682.666015625\n",
      "epoch: 8 iter: 26 reconn_loss: 9860.41015625 kl_loss: 1605.55712890625\n",
      "epoch: 8 iter: 27 reconn_loss: 10041.4296875 kl_loss: 1716.941162109375\n",
      "epoch: 8 iter: 28 reconn_loss: 9781.0498046875 kl_loss: 1593.5015869140625\n",
      "epoch: 8 iter: 29 reconn_loss: 10471.5859375 kl_loss: 1612.693603515625\n",
      "epoch: 8 iter: 30 reconn_loss: 10328.2490234375 kl_loss: 1599.6748046875\n",
      "epoch: 8 iter: 31 reconn_loss: 10257.1611328125 kl_loss: 1603.201416015625\n",
      "epoch: 8 iter: 32 reconn_loss: 10006.248046875 kl_loss: 1637.36767578125\n",
      "epoch: 8 iter: 33 reconn_loss: 9912.4501953125 kl_loss: 1610.535888671875\n",
      "epoch: 8 iter: 34 reconn_loss: 9806.435546875 kl_loss: 1647.57861328125\n",
      "epoch: 8 iter: 35 reconn_loss: 9969.6318359375 kl_loss: 1641.0889892578125\n",
      "epoch: 8 iter: 36 reconn_loss: 9632.8310546875 kl_loss: 1620.6575927734375\n",
      "epoch: 8 iter: 37 reconn_loss: 9953.21484375 kl_loss: 1622.4881591796875\n",
      "epoch: 8 iter: 38 reconn_loss: 10213.62890625 kl_loss: 1507.4610595703125\n",
      "epoch: 8 iter: 39 reconn_loss: 9846.9453125 kl_loss: 1528.101806640625\n",
      "epoch: 8 iter: 40 reconn_loss: 9889.34375 kl_loss: 1630.137939453125\n",
      "epoch: 8 iter: 41 reconn_loss: 9663.7998046875 kl_loss: 1617.5262451171875\n",
      "epoch: 8 iter: 42 reconn_loss: 10115.580078125 kl_loss: 1545.3291015625\n",
      "epoch: 8 iter: 43 reconn_loss: 10057.2060546875 kl_loss: 1608.8416748046875\n",
      "epoch: 8 iter: 44 reconn_loss: 9719.990234375 kl_loss: 1596.3870849609375\n",
      "epoch: 8 iter: 45 reconn_loss: 9556.36328125 kl_loss: 1544.66357421875\n",
      "epoch: 8 iter: 46 reconn_loss: 10345.3642578125 kl_loss: 1563.2998046875\n",
      "epoch: 8 iter: 47 reconn_loss: 10181.8154296875 kl_loss: 1655.2506103515625\n",
      "epoch: 8 iter: 48 reconn_loss: 9941.0224609375 kl_loss: 1618.5892333984375\n",
      "epoch: 8 iter: 49 reconn_loss: 10377.671875 kl_loss: 1617.3515625\n",
      "epoch: 8 iter: 50 reconn_loss: 9990.83984375 kl_loss: 1643.153076171875\n",
      "epoch: 8 iter: 51 reconn_loss: 10107.875 kl_loss: 1682.42138671875\n",
      "epoch: 8 iter: 52 reconn_loss: 10093.98828125 kl_loss: 1655.001708984375\n",
      "epoch: 8 iter: 53 reconn_loss: 9542.947265625 kl_loss: 1657.7177734375\n",
      "epoch: 8 iter: 54 reconn_loss: 10018.693359375 kl_loss: 1546.2191162109375\n",
      "epoch: 8 iter: 55 reconn_loss: 9965.3466796875 kl_loss: 1591.5792236328125\n",
      "epoch: 8 iter: 56 reconn_loss: 10065.603515625 kl_loss: 1642.81103515625\n",
      "epoch: 8 iter: 57 reconn_loss: 10203.9384765625 kl_loss: 1660.2681884765625\n",
      "epoch: 8 iter: 58 reconn_loss: 9903.162109375 kl_loss: 1605.1951904296875\n",
      "epoch: 8 iter: 59 reconn_loss: 10038.4541015625 kl_loss: 1639.376953125\n",
      "epoch: 8 iter: 60 reconn_loss: 9503.8720703125 kl_loss: 1554.4300537109375\n",
      "epoch: 8 iter: 61 reconn_loss: 9824.5556640625 kl_loss: 1618.965576171875\n",
      "epoch: 8 iter: 62 reconn_loss: 9432.85546875 kl_loss: 1609.805419921875\n",
      "epoch: 8 iter: 63 reconn_loss: 9883.7451171875 kl_loss: 1587.284912109375\n",
      "epoch: 8 iter: 64 reconn_loss: 9734.6787109375 kl_loss: 1654.868408203125\n",
      "epoch: 8 iter: 65 reconn_loss: 9832.53125 kl_loss: 1588.2999267578125\n",
      "epoch: 8 iter: 66 reconn_loss: 9958.578125 kl_loss: 1566.006591796875\n",
      "epoch: 8 iter: 67 reconn_loss: 10025.5029296875 kl_loss: 1600.8582763671875\n",
      "epoch: 8 iter: 68 reconn_loss: 9782.1787109375 kl_loss: 1556.5029296875\n",
      "epoch: 8 iter: 69 reconn_loss: 10303.203125 kl_loss: 1606.9219970703125\n",
      "epoch: 8 iter: 70 reconn_loss: 9925.9091796875 kl_loss: 1600.0164794921875\n",
      "epoch: 8 iter: 71 reconn_loss: 9829.2314453125 kl_loss: 1616.175537109375\n",
      "epoch: 8 iter: 72 reconn_loss: 10101.1708984375 kl_loss: 1670.3736572265625\n",
      "epoch: 8 iter: 73 reconn_loss: 9902.03125 kl_loss: 1708.4228515625\n",
      "epoch: 8 iter: 74 reconn_loss: 10011.4755859375 kl_loss: 1699.3272705078125\n",
      "epoch: 8 iter: 75 reconn_loss: 10255.931640625 kl_loss: 1654.383056640625\n",
      "epoch: 8 iter: 76 reconn_loss: 9609.92578125 kl_loss: 1658.2310791015625\n",
      "epoch: 8 iter: 77 reconn_loss: 9882.568359375 kl_loss: 1645.8837890625\n",
      "epoch: 8 iter: 78 reconn_loss: 9963.544921875 kl_loss: 1675.27978515625\n",
      "epoch: 8 iter: 79 reconn_loss: 9576.515625 kl_loss: 1770.471435546875\n",
      "epoch: 8 iter: 80 reconn_loss: 9943.26953125 kl_loss: 1763.0133056640625\n",
      "epoch: 8 iter: 81 reconn_loss: 10061.0751953125 kl_loss: 1727.556396484375\n",
      "epoch: 8 iter: 82 reconn_loss: 9913.6845703125 kl_loss: 1648.4697265625\n",
      "epoch: 8 iter: 83 reconn_loss: 9799.64453125 kl_loss: 1627.25927734375\n",
      "epoch: 8 iter: 84 reconn_loss: 10110.10546875 kl_loss: 1657.241455078125\n",
      "epoch: 8 iter: 85 reconn_loss: 10114.2734375 kl_loss: 1671.8121337890625\n",
      "epoch: 8 iter: 86 reconn_loss: 9692.0810546875 kl_loss: 1563.9202880859375\n",
      "epoch: 8 iter: 87 reconn_loss: 10198.626953125 kl_loss: 1638.361083984375\n",
      "epoch: 8 iter: 88 reconn_loss: 9769.962890625 kl_loss: 1614.696533203125\n",
      "epoch: 8 iter: 89 reconn_loss: 9573.935546875 kl_loss: 1646.48193359375\n",
      "epoch: 8 iter: 90 reconn_loss: 9912.275390625 kl_loss: 1573.53466796875\n",
      "epoch: 8 iter: 91 reconn_loss: 9802.958984375 kl_loss: 1578.297119140625\n",
      "epoch: 8 iter: 92 reconn_loss: 9876.1025390625 kl_loss: 1561.2679443359375\n",
      "epoch: 8 iter: 93 reconn_loss: 9971.3349609375 kl_loss: 1567.0787353515625\n",
      "epoch: 8 iter: 94 reconn_loss: 9811.947265625 kl_loss: 1588.5770263671875\n",
      "epoch: 8 iter: 95 reconn_loss: 9736.9375 kl_loss: 1649.1883544921875\n",
      "epoch: 8 iter: 96 reconn_loss: 9639.025390625 kl_loss: 1592.55224609375\n",
      "epoch: 8 iter: 97 reconn_loss: 9907.2099609375 kl_loss: 1609.8505859375\n",
      "epoch: 8 iter: 98 reconn_loss: 9975.8310546875 kl_loss: 1592.2637939453125\n",
      "epoch: 8 iter: 99 reconn_loss: 10006.3828125 kl_loss: 1655.288818359375\n",
      "epoch: 8 iter: 100 reconn_loss: 9838.6494140625 kl_loss: 1590.0653076171875\n",
      "epoch: 8 iter: 101 reconn_loss: 9906.7578125 kl_loss: 1596.121337890625\n",
      "epoch: 8 iter: 102 reconn_loss: 9905.220703125 kl_loss: 1579.712158203125\n",
      "epoch: 8 iter: 103 reconn_loss: 9838.455078125 kl_loss: 1649.1160888671875\n",
      "epoch: 8 iter: 104 reconn_loss: 10006.79296875 kl_loss: 1686.82470703125\n",
      "epoch: 8 iter: 105 reconn_loss: 9720.728515625 kl_loss: 1726.923583984375\n",
      "epoch: 8 iter: 106 reconn_loss: 9566.509765625 kl_loss: 1663.6334228515625\n",
      "epoch: 8 iter: 107 reconn_loss: 9700.849609375 kl_loss: 1676.3623046875\n",
      "epoch: 8 iter: 108 reconn_loss: 9960.3125 kl_loss: 1664.602294921875\n",
      "epoch: 8 iter: 109 reconn_loss: 9925.4462890625 kl_loss: 1626.6063232421875\n",
      "epoch: 8 iter: 110 reconn_loss: 10180.0048828125 kl_loss: 1654.8092041015625\n",
      "epoch: 8 iter: 111 reconn_loss: 9720.55859375 kl_loss: 1592.411376953125\n",
      "epoch: 8 iter: 112 reconn_loss: 9794.4521484375 kl_loss: 1631.721923828125\n",
      "epoch: 8 iter: 113 reconn_loss: 10176.0654296875 kl_loss: 1695.368408203125\n",
      "epoch: 8 iter: 114 reconn_loss: 10073.12890625 kl_loss: 1604.202880859375\n",
      "epoch: 8 iter: 115 reconn_loss: 9661.5322265625 kl_loss: 1571.530517578125\n",
      "epoch: 8 iter: 116 reconn_loss: 9759.3935546875 kl_loss: 1628.741943359375\n",
      "epoch: 8 iter: 117 reconn_loss: 9810.4951171875 kl_loss: 1615.068603515625\n",
      "epoch: 8 iter: 118 reconn_loss: 9985.1171875 kl_loss: 1652.924072265625\n",
      "epoch: 8 iter: 119 reconn_loss: 9979.697265625 kl_loss: 1604.3074951171875\n",
      "epoch: 8 iter: 120 reconn_loss: 9790.5703125 kl_loss: 1596.5137939453125\n",
      "epoch: 8 iter: 121 reconn_loss: 9976.30859375 kl_loss: 1657.106689453125\n",
      "epoch: 8 iter: 122 reconn_loss: 9543.8759765625 kl_loss: 1680.4532470703125\n",
      "epoch: 8 iter: 123 reconn_loss: 9704.1923828125 kl_loss: 1582.180908203125\n",
      "epoch: 8 iter: 124 reconn_loss: 9723.1044921875 kl_loss: 1508.3035888671875\n",
      "epoch: 8 iter: 125 reconn_loss: 10014.1474609375 kl_loss: 1651.4970703125\n",
      "epoch: 8 iter: 126 reconn_loss: 9529.556640625 kl_loss: 1558.1571044921875\n",
      "epoch: 8 iter: 127 reconn_loss: 9891.095703125 kl_loss: 1575.6243896484375\n",
      "epoch: 8 iter: 128 reconn_loss: 9880.47265625 kl_loss: 1633.925048828125\n",
      "epoch: 8 iter: 129 reconn_loss: 9928.962890625 kl_loss: 1637.5584716796875\n",
      "epoch: 8 iter: 130 reconn_loss: 9840.66796875 kl_loss: 1663.68701171875\n",
      "epoch: 8 iter: 131 reconn_loss: 9548.46875 kl_loss: 1631.20654296875\n",
      "epoch: 8 iter: 132 reconn_loss: 10009.5166015625 kl_loss: 1603.6751708984375\n",
      "epoch: 8 iter: 133 reconn_loss: 9957.783203125 kl_loss: 1654.9388427734375\n",
      "epoch: 8 iter: 134 reconn_loss: 10095.1845703125 kl_loss: 1635.6834716796875\n",
      "epoch: 8 iter: 135 reconn_loss: 10086.6748046875 kl_loss: 1616.337646484375\n",
      "epoch: 8 iter: 136 reconn_loss: 9696.1416015625 kl_loss: 1619.4056396484375\n",
      "epoch: 8 iter: 137 reconn_loss: 9706.3876953125 kl_loss: 1684.7735595703125\n",
      "epoch: 8 iter: 138 reconn_loss: 9809.8623046875 kl_loss: 1726.7860107421875\n",
      "epoch: 8 iter: 139 reconn_loss: 9976.046875 kl_loss: 1641.265380859375\n",
      "epoch: 8 iter: 140 reconn_loss: 9561.58203125 kl_loss: 1596.0662841796875\n",
      "epoch: 8 iter: 141 reconn_loss: 9590.4873046875 kl_loss: 1603.6275634765625\n",
      "epoch: 8 iter: 142 reconn_loss: 9815.2138671875 kl_loss: 1708.356689453125\n",
      "epoch: 8 iter: 143 reconn_loss: 9906.884765625 kl_loss: 1638.7099609375\n",
      "epoch: 8 iter: 144 reconn_loss: 10014.056640625 kl_loss: 1657.4034423828125\n",
      "epoch: 8 iter: 145 reconn_loss: 9689.2412109375 kl_loss: 1698.5712890625\n",
      "epoch: 8 iter: 146 reconn_loss: 9873.5 kl_loss: 1580.862548828125\n",
      "epoch: 8 iter: 147 reconn_loss: 9925.3134765625 kl_loss: 1597.2744140625\n",
      "epoch: 8 iter: 148 reconn_loss: 9474.462890625 kl_loss: 1614.3060302734375\n",
      "epoch: 8 iter: 149 reconn_loss: 10115.5703125 kl_loss: 1612.6484375\n",
      "epoch: 8 iter: 150 reconn_loss: 9931.103515625 kl_loss: 1607.5240478515625\n",
      "epoch: 8 iter: 151 reconn_loss: 9593.3681640625 kl_loss: 1675.232421875\n",
      "epoch: 8 iter: 152 reconn_loss: 9803.82421875 kl_loss: 1622.6422119140625\n",
      "epoch: 8 iter: 153 reconn_loss: 9744.197265625 kl_loss: 1666.64453125\n",
      "epoch: 8 iter: 154 reconn_loss: 9651.75390625 kl_loss: 1622.013427734375\n",
      "epoch: 8 iter: 155 reconn_loss: 9675.08203125 kl_loss: 1677.375244140625\n",
      "epoch: 8 iter: 156 reconn_loss: 9763.9921875 kl_loss: 1601.699462890625\n",
      "epoch: 8 iter: 157 reconn_loss: 9949.8837890625 kl_loss: 1683.0113525390625\n",
      "epoch: 8 iter: 158 reconn_loss: 9831.7001953125 kl_loss: 1658.2608642578125\n",
      "epoch: 8 iter: 159 reconn_loss: 9499.7880859375 kl_loss: 1610.02294921875\n",
      "epoch: 8 iter: 160 reconn_loss: 10206.2373046875 kl_loss: 1682.8350830078125\n",
      "epoch: 8 iter: 161 reconn_loss: 9892.408203125 kl_loss: 1615.170654296875\n",
      "epoch: 8 iter: 162 reconn_loss: 9389.3564453125 kl_loss: 1638.9761962890625\n",
      "epoch: 8 iter: 163 reconn_loss: 9511.11328125 kl_loss: 1610.0201416015625\n",
      "epoch: 8 iter: 164 reconn_loss: 9740.857421875 kl_loss: 1671.557861328125\n",
      "epoch: 8 iter: 165 reconn_loss: 9857.642578125 kl_loss: 1675.4991455078125\n",
      "epoch: 8 iter: 166 reconn_loss: 9912.6513671875 kl_loss: 1595.364013671875\n",
      "epoch: 8 iter: 167 reconn_loss: 9893.1416015625 kl_loss: 1648.1419677734375\n",
      "epoch: 8 iter: 168 reconn_loss: 9358.193359375 kl_loss: 1610.5926513671875\n",
      "epoch: 8 iter: 169 reconn_loss: 9840.970703125 kl_loss: 1586.94189453125\n",
      "epoch: 8 iter: 170 reconn_loss: 9469.7822265625 kl_loss: 1590.8048095703125\n",
      "epoch: 8 iter: 171 reconn_loss: 9819.54296875 kl_loss: 1626.970458984375\n",
      "epoch: 8 iter: 172 reconn_loss: 10141.2919921875 kl_loss: 1665.2935791015625\n",
      "epoch: 8 iter: 173 reconn_loss: 10043.908203125 kl_loss: 1600.4381103515625\n",
      "epoch: 8 iter: 174 reconn_loss: 9834.0234375 kl_loss: 1607.04541015625\n",
      "epoch: 8 iter: 175 reconn_loss: 9922.05078125 kl_loss: 1605.11279296875\n",
      "epoch: 8 iter: 176 reconn_loss: 9616.0859375 kl_loss: 1602.46630859375\n",
      "epoch: 8 iter: 177 reconn_loss: 9716.908203125 kl_loss: 1641.319091796875\n",
      "epoch: 8 iter: 178 reconn_loss: 9669.0791015625 kl_loss: 1659.680908203125\n",
      "epoch: 8 iter: 179 reconn_loss: 10217.080078125 kl_loss: 1626.3187255859375\n",
      "epoch: 8 iter: 180 reconn_loss: 9640.033203125 kl_loss: 1612.28662109375\n",
      "epoch: 8 iter: 181 reconn_loss: 10257.5341796875 kl_loss: 1602.3804931640625\n",
      "epoch: 8 iter: 182 reconn_loss: 9893.814453125 kl_loss: 1641.556884765625\n",
      "epoch: 8 iter: 183 reconn_loss: 9686.8232421875 kl_loss: 1668.7540283203125\n",
      "epoch: 8 iter: 184 reconn_loss: 9863.748046875 kl_loss: 1618.52001953125\n",
      "epoch: 8 iter: 185 reconn_loss: 10118.359375 kl_loss: 1690.118408203125\n",
      "epoch: 8 iter: 186 reconn_loss: 10211.416015625 kl_loss: 1676.495361328125\n",
      "epoch: 8 iter: 187 reconn_loss: 9726.5087890625 kl_loss: 1661.0972900390625\n",
      "epoch: 8 iter: 188 reconn_loss: 9682.095703125 kl_loss: 1696.70849609375\n",
      "epoch: 8 iter: 189 reconn_loss: 9892.84375 kl_loss: 1653.8814697265625\n",
      "epoch: 8 iter: 190 reconn_loss: 9605.3154296875 kl_loss: 1649.0902099609375\n",
      "epoch: 8 iter: 191 reconn_loss: 9818.98046875 kl_loss: 1653.37353515625\n",
      "epoch: 8 iter: 192 reconn_loss: 9950.81640625 kl_loss: 1610.5\n",
      "epoch: 8 iter: 193 reconn_loss: 10141.408203125 kl_loss: 1664.609130859375\n",
      "epoch: 8 iter: 194 reconn_loss: 9816.015625 kl_loss: 1601.6099853515625\n",
      "epoch: 8 iter: 195 reconn_loss: 9700.4111328125 kl_loss: 1712.44775390625\n",
      "epoch: 8 iter: 196 reconn_loss: 9828.6708984375 kl_loss: 1723.2059326171875\n",
      "epoch: 8 iter: 197 reconn_loss: 9730.7509765625 kl_loss: 1697.5118408203125\n",
      "epoch: 8 iter: 198 reconn_loss: 9811.7373046875 kl_loss: 1686.84765625\n",
      "epoch: 8 iter: 199 reconn_loss: 9743.919921875 kl_loss: 1577.3548583984375\n",
      "epoch: 8 iter: 200 reconn_loss: 9944.46484375 kl_loss: 1665.3927001953125\n",
      "epoch: 8 iter: 201 reconn_loss: 9927.291015625 kl_loss: 1657.7581787109375\n",
      "epoch: 8 iter: 202 reconn_loss: 10149.666015625 kl_loss: 1724.139892578125\n",
      "epoch: 8 iter: 203 reconn_loss: 9934.7734375 kl_loss: 1711.876953125\n",
      "epoch: 8 iter: 204 reconn_loss: 9896.275390625 kl_loss: 1693.89892578125\n",
      "epoch: 8 iter: 205 reconn_loss: 9555.4326171875 kl_loss: 1590.9429931640625\n",
      "epoch: 8 iter: 206 reconn_loss: 9556.3203125 kl_loss: 1623.9697265625\n",
      "epoch: 8 iter: 207 reconn_loss: 10392.8662109375 kl_loss: 1695.6220703125\n",
      "epoch: 8 iter: 208 reconn_loss: 9915.4208984375 kl_loss: 1751.1253662109375\n",
      "epoch: 8 iter: 209 reconn_loss: 9644.0576171875 kl_loss: 1614.7239990234375\n",
      "epoch: 8 iter: 210 reconn_loss: 10143.939453125 kl_loss: 1616.6500244140625\n",
      "epoch: 8 iter: 211 reconn_loss: 9990.4833984375 kl_loss: 1659.0706787109375\n",
      "epoch: 8 iter: 212 reconn_loss: 9624.984375 kl_loss: 1574.17578125\n",
      "epoch: 8 iter: 213 reconn_loss: 9693.755859375 kl_loss: 1599.798828125\n",
      "epoch: 8 iter: 214 reconn_loss: 10003.734375 kl_loss: 1632.177978515625\n",
      "epoch: 8 iter: 215 reconn_loss: 9523.34765625 kl_loss: 1589.2728271484375\n",
      "epoch: 8 iter: 216 reconn_loss: 9590.296875 kl_loss: 1608.1513671875\n",
      "epoch: 8 iter: 217 reconn_loss: 9516.5107421875 kl_loss: 1625.260986328125\n",
      "epoch: 8 iter: 218 reconn_loss: 9905.05078125 kl_loss: 1664.7928466796875\n",
      "epoch: 8 iter: 219 reconn_loss: 9662.6435546875 kl_loss: 1682.1689453125\n",
      "epoch: 8 iter: 220 reconn_loss: 9857.4853515625 kl_loss: 1680.6307373046875\n",
      "epoch: 8 iter: 221 reconn_loss: 9868.69921875 kl_loss: 1665.2880859375\n",
      "epoch: 8 iter: 222 reconn_loss: 6584.1826171875 kl_loss: 1073.4366455078125\n",
      "0.weight tensor(55.6058) tensor(-51.2165)\n",
      "0.bias tensor(35.7757) tensor(-27.6204)\n",
      "2.weight tensor(19.1612) tensor(-21.9659)\n",
      "2.bias tensor(10.7161) tensor(-11.4721)\n",
      "4.weight tensor(9.8505) tensor(-11.8361)\n",
      "4.bias tensor(4.6332) tensor(-4.8974)\n",
      "epoch: 9 iter: 0 reconn_loss: 10104.375 kl_loss: 1647.093505859375\n",
      "epoch: 9 iter: 1 reconn_loss: 9742.435546875 kl_loss: 1681.34326171875\n",
      "epoch: 9 iter: 2 reconn_loss: 9797.037109375 kl_loss: 1710.417724609375\n",
      "epoch: 9 iter: 3 reconn_loss: 9694.708984375 kl_loss: 1598.462646484375\n",
      "epoch: 9 iter: 4 reconn_loss: 10017.06640625 kl_loss: 1702.884033203125\n",
      "epoch: 9 iter: 5 reconn_loss: 10106.291015625 kl_loss: 1672.48095703125\n",
      "epoch: 9 iter: 6 reconn_loss: 9814.046875 kl_loss: 1650.2786865234375\n",
      "epoch: 9 iter: 7 reconn_loss: 9855.5302734375 kl_loss: 1662.9921875\n",
      "epoch: 9 iter: 8 reconn_loss: 9761.9619140625 kl_loss: 1570.3245849609375\n",
      "epoch: 9 iter: 9 reconn_loss: 9607.052734375 kl_loss: 1657.619140625\n",
      "epoch: 9 iter: 10 reconn_loss: 9748.0673828125 kl_loss: 1686.522216796875\n",
      "epoch: 9 iter: 11 reconn_loss: 9557.46484375 kl_loss: 1690.513916015625\n",
      "epoch: 9 iter: 12 reconn_loss: 9733.68359375 kl_loss: 1675.3629150390625\n",
      "epoch: 9 iter: 13 reconn_loss: 9700.787109375 kl_loss: 1638.20556640625\n",
      "epoch: 9 iter: 14 reconn_loss: 9367.73046875 kl_loss: 1645.329833984375\n",
      "epoch: 9 iter: 15 reconn_loss: 9897.6884765625 kl_loss: 1741.11181640625\n",
      "epoch: 9 iter: 16 reconn_loss: 9708.52734375 kl_loss: 1630.0706787109375\n",
      "epoch: 9 iter: 17 reconn_loss: 9995.50390625 kl_loss: 1690.4722900390625\n",
      "epoch: 9 iter: 18 reconn_loss: 9970.103515625 kl_loss: 1692.2816162109375\n",
      "epoch: 9 iter: 19 reconn_loss: 9667.7451171875 kl_loss: 1742.520263671875\n",
      "epoch: 9 iter: 20 reconn_loss: 9724.98046875 kl_loss: 1691.048828125\n",
      "epoch: 9 iter: 21 reconn_loss: 9922.763671875 kl_loss: 1642.60546875\n",
      "epoch: 9 iter: 22 reconn_loss: 10080.044921875 kl_loss: 1662.68701171875\n",
      "epoch: 9 iter: 23 reconn_loss: 9863.0322265625 kl_loss: 1676.5164794921875\n",
      "epoch: 9 iter: 24 reconn_loss: 9956.1728515625 kl_loss: 1733.747802734375\n",
      "epoch: 9 iter: 25 reconn_loss: 9514.9814453125 kl_loss: 1688.5606689453125\n",
      "epoch: 9 iter: 26 reconn_loss: 9752.1630859375 kl_loss: 1712.1766357421875\n",
      "epoch: 9 iter: 27 reconn_loss: 9690.7763671875 kl_loss: 1647.125732421875\n",
      "epoch: 9 iter: 28 reconn_loss: 9714.0322265625 kl_loss: 1628.1771240234375\n",
      "epoch: 9 iter: 29 reconn_loss: 9783.0537109375 kl_loss: 1706.2479248046875\n",
      "epoch: 9 iter: 30 reconn_loss: 9680.0224609375 kl_loss: 1682.80078125\n",
      "epoch: 9 iter: 31 reconn_loss: 9769.4736328125 kl_loss: 1644.6358642578125\n",
      "epoch: 9 iter: 32 reconn_loss: 9812.515625 kl_loss: 1664.7371826171875\n",
      "epoch: 9 iter: 33 reconn_loss: 9498.0263671875 kl_loss: 1671.0777587890625\n",
      "epoch: 9 iter: 34 reconn_loss: 9841.3515625 kl_loss: 1603.4273681640625\n",
      "epoch: 9 iter: 35 reconn_loss: 10028.0361328125 kl_loss: 1635.6409912109375\n",
      "epoch: 9 iter: 36 reconn_loss: 10139.8046875 kl_loss: 1657.19775390625\n",
      "epoch: 9 iter: 37 reconn_loss: 9932.9462890625 kl_loss: 1522.007080078125\n",
      "epoch: 9 iter: 38 reconn_loss: 9958.9462890625 kl_loss: 1659.942138671875\n",
      "epoch: 9 iter: 39 reconn_loss: 9515.7333984375 kl_loss: 1644.6907958984375\n",
      "epoch: 9 iter: 40 reconn_loss: 9849.0791015625 kl_loss: 1613.4488525390625\n",
      "epoch: 9 iter: 41 reconn_loss: 10017.818359375 kl_loss: 1655.87255859375\n",
      "epoch: 9 iter: 42 reconn_loss: 10092.171875 kl_loss: 1722.8997802734375\n",
      "epoch: 9 iter: 43 reconn_loss: 9828.38671875 kl_loss: 1640.7987060546875\n",
      "epoch: 9 iter: 44 reconn_loss: 10003.212890625 kl_loss: 1682.9837646484375\n",
      "epoch: 9 iter: 45 reconn_loss: 9646.078125 kl_loss: 1693.88134765625\n",
      "epoch: 9 iter: 46 reconn_loss: 9843.775390625 kl_loss: 1720.9373779296875\n",
      "epoch: 9 iter: 47 reconn_loss: 9807.322265625 kl_loss: 1665.5804443359375\n",
      "epoch: 9 iter: 48 reconn_loss: 9798.068359375 kl_loss: 1614.69091796875\n",
      "epoch: 9 iter: 49 reconn_loss: 10045.416015625 kl_loss: 1712.6873779296875\n",
      "epoch: 9 iter: 50 reconn_loss: 9987.232421875 kl_loss: 1651.56298828125\n",
      "epoch: 9 iter: 51 reconn_loss: 9555.2099609375 kl_loss: 1675.2581787109375\n",
      "epoch: 9 iter: 52 reconn_loss: 9922.390625 kl_loss: 1641.7666015625\n",
      "epoch: 9 iter: 53 reconn_loss: 10006.8671875 kl_loss: 1631.9501953125\n",
      "epoch: 9 iter: 54 reconn_loss: 9634.611328125 kl_loss: 1683.15673828125\n",
      "epoch: 9 iter: 55 reconn_loss: 9852.669921875 kl_loss: 1664.7874755859375\n",
      "epoch: 9 iter: 56 reconn_loss: 9959.443359375 kl_loss: 1689.3465576171875\n",
      "epoch: 9 iter: 57 reconn_loss: 9506.263671875 kl_loss: 1688.59521484375\n",
      "epoch: 9 iter: 58 reconn_loss: 9798.9326171875 kl_loss: 1627.01318359375\n",
      "epoch: 9 iter: 59 reconn_loss: 10139.3330078125 kl_loss: 1731.405517578125\n",
      "epoch: 9 iter: 60 reconn_loss: 9701.0 kl_loss: 1682.4124755859375\n",
      "epoch: 9 iter: 61 reconn_loss: 10042.556640625 kl_loss: 1707.264892578125\n",
      "epoch: 9 iter: 62 reconn_loss: 9516.978515625 kl_loss: 1671.533203125\n",
      "epoch: 9 iter: 63 reconn_loss: 9624.58984375 kl_loss: 1748.3365478515625\n",
      "epoch: 9 iter: 64 reconn_loss: 9784.9697265625 kl_loss: 1688.9652099609375\n",
      "epoch: 9 iter: 65 reconn_loss: 9771.9833984375 kl_loss: 1757.603271484375\n",
      "epoch: 9 iter: 66 reconn_loss: 9523.5234375 kl_loss: 1671.524658203125\n",
      "epoch: 9 iter: 67 reconn_loss: 9850.328125 kl_loss: 1628.5418701171875\n",
      "epoch: 9 iter: 68 reconn_loss: 9696.0673828125 kl_loss: 1682.3885498046875\n",
      "epoch: 9 iter: 69 reconn_loss: 9305.4599609375 kl_loss: 1653.717041015625\n",
      "epoch: 9 iter: 70 reconn_loss: 9911.1953125 kl_loss: 1665.8626708984375\n",
      "epoch: 9 iter: 71 reconn_loss: 9600.84375 kl_loss: 1583.875732421875\n",
      "epoch: 9 iter: 72 reconn_loss: 9448.2021484375 kl_loss: 1668.829345703125\n",
      "epoch: 9 iter: 73 reconn_loss: 9672.439453125 kl_loss: 1683.68798828125\n",
      "epoch: 9 iter: 74 reconn_loss: 9807.2412109375 kl_loss: 1630.573486328125\n",
      "epoch: 9 iter: 75 reconn_loss: 9838.29296875 kl_loss: 1664.4814453125\n",
      "epoch: 9 iter: 76 reconn_loss: 9889.6904296875 kl_loss: 1663.580322265625\n",
      "epoch: 9 iter: 77 reconn_loss: 9523.0244140625 kl_loss: 1625.9339599609375\n",
      "epoch: 9 iter: 78 reconn_loss: 9791.45703125 kl_loss: 1688.733642578125\n",
      "epoch: 9 iter: 79 reconn_loss: 9681.546875 kl_loss: 1638.9326171875\n",
      "epoch: 9 iter: 80 reconn_loss: 9672.0498046875 kl_loss: 1754.5413818359375\n",
      "epoch: 9 iter: 81 reconn_loss: 9782.0185546875 kl_loss: 1667.100341796875\n",
      "epoch: 9 iter: 82 reconn_loss: 9791.4560546875 kl_loss: 1651.55078125\n",
      "epoch: 9 iter: 83 reconn_loss: 9811.638671875 kl_loss: 1661.6409912109375\n",
      "epoch: 9 iter: 84 reconn_loss: 9744.6865234375 kl_loss: 1640.07373046875\n",
      "epoch: 9 iter: 85 reconn_loss: 9844.904296875 kl_loss: 1663.8553466796875\n",
      "epoch: 9 iter: 86 reconn_loss: 9915.1484375 kl_loss: 1665.7337646484375\n",
      "epoch: 9 iter: 87 reconn_loss: 9592.4619140625 kl_loss: 1604.488037109375\n",
      "epoch: 9 iter: 88 reconn_loss: 9531.708984375 kl_loss: 1630.772705078125\n",
      "epoch: 9 iter: 89 reconn_loss: 9461.146484375 kl_loss: 1723.353271484375\n",
      "epoch: 9 iter: 90 reconn_loss: 9639.1416015625 kl_loss: 1623.358642578125\n",
      "epoch: 9 iter: 91 reconn_loss: 9778.779296875 kl_loss: 1668.773193359375\n",
      "epoch: 9 iter: 92 reconn_loss: 9501.505859375 kl_loss: 1677.0369873046875\n",
      "epoch: 9 iter: 93 reconn_loss: 9379.783203125 kl_loss: 1657.0106201171875\n",
      "epoch: 9 iter: 94 reconn_loss: 9916.8955078125 kl_loss: 1645.6593017578125\n",
      "epoch: 9 iter: 95 reconn_loss: 9315.8349609375 kl_loss: 1680.5614013671875\n",
      "epoch: 9 iter: 96 reconn_loss: 9650.578125 kl_loss: 1660.5009765625\n",
      "epoch: 9 iter: 97 reconn_loss: 9983.1416015625 kl_loss: 1665.7177734375\n",
      "epoch: 9 iter: 98 reconn_loss: 9690.12890625 kl_loss: 1698.99755859375\n",
      "epoch: 9 iter: 99 reconn_loss: 9801.107421875 kl_loss: 1666.5845947265625\n",
      "epoch: 9 iter: 100 reconn_loss: 9724.0439453125 kl_loss: 1703.06298828125\n",
      "epoch: 9 iter: 101 reconn_loss: 9668.587890625 kl_loss: 1663.150390625\n",
      "epoch: 9 iter: 102 reconn_loss: 9748.9345703125 kl_loss: 1673.1141357421875\n",
      "epoch: 9 iter: 103 reconn_loss: 10099.2998046875 kl_loss: 1626.358642578125\n",
      "epoch: 9 iter: 104 reconn_loss: 9543.513671875 kl_loss: 1599.4222412109375\n",
      "epoch: 9 iter: 105 reconn_loss: 9471.5810546875 kl_loss: 1735.003173828125\n",
      "epoch: 9 iter: 106 reconn_loss: 9657.5517578125 kl_loss: 1657.96142578125\n",
      "epoch: 9 iter: 107 reconn_loss: 9884.720703125 kl_loss: 1746.929931640625\n",
      "epoch: 9 iter: 108 reconn_loss: 9302.041015625 kl_loss: 1686.447509765625\n",
      "epoch: 9 iter: 109 reconn_loss: 9456.380859375 kl_loss: 1739.804931640625\n",
      "epoch: 9 iter: 110 reconn_loss: 9047.93359375 kl_loss: 1670.9530029296875\n",
      "epoch: 9 iter: 111 reconn_loss: 9294.5517578125 kl_loss: 1684.553955078125\n",
      "epoch: 9 iter: 112 reconn_loss: 9683.3759765625 kl_loss: 1716.0115966796875\n",
      "epoch: 9 iter: 113 reconn_loss: 10021.5068359375 kl_loss: 1683.5775146484375\n",
      "epoch: 9 iter: 114 reconn_loss: 9987.5185546875 kl_loss: 1686.460205078125\n",
      "epoch: 9 iter: 115 reconn_loss: 9585.568359375 kl_loss: 1731.50927734375\n",
      "epoch: 9 iter: 116 reconn_loss: 9990.20703125 kl_loss: 1673.249755859375\n",
      "epoch: 9 iter: 117 reconn_loss: 9589.1845703125 kl_loss: 1690.0531005859375\n",
      "epoch: 9 iter: 118 reconn_loss: 9733.21875 kl_loss: 1668.807373046875\n",
      "epoch: 9 iter: 119 reconn_loss: 9668.7822265625 kl_loss: 1640.419189453125\n",
      "epoch: 9 iter: 120 reconn_loss: 9510.0048828125 kl_loss: 1649.3763427734375\n",
      "epoch: 9 iter: 121 reconn_loss: 9426.134765625 kl_loss: 1645.447265625\n",
      "epoch: 9 iter: 122 reconn_loss: 9584.513671875 kl_loss: 1691.4593505859375\n",
      "epoch: 9 iter: 123 reconn_loss: 9736.24609375 kl_loss: 1626.463623046875\n",
      "epoch: 9 iter: 124 reconn_loss: 9479.138671875 kl_loss: 1687.69482421875\n",
      "epoch: 9 iter: 125 reconn_loss: 9592.4306640625 kl_loss: 1707.44970703125\n",
      "epoch: 9 iter: 126 reconn_loss: 9294.1875 kl_loss: 1690.0977783203125\n",
      "epoch: 9 iter: 127 reconn_loss: 9739.6083984375 kl_loss: 1691.6937255859375\n",
      "epoch: 9 iter: 128 reconn_loss: 9920.7177734375 kl_loss: 1643.6978759765625\n",
      "epoch: 9 iter: 129 reconn_loss: 9841.1259765625 kl_loss: 1668.8172607421875\n",
      "epoch: 9 iter: 130 reconn_loss: 9745.19921875 kl_loss: 1665.6787109375\n",
      "epoch: 9 iter: 131 reconn_loss: 9493.884765625 kl_loss: 1689.129150390625\n",
      "epoch: 9 iter: 132 reconn_loss: 9690.0810546875 kl_loss: 1735.15234375\n",
      "epoch: 9 iter: 133 reconn_loss: 9688.216796875 kl_loss: 1720.14013671875\n",
      "epoch: 9 iter: 134 reconn_loss: 9602.0537109375 kl_loss: 1753.25634765625\n",
      "epoch: 9 iter: 135 reconn_loss: 9750.2412109375 kl_loss: 1617.9815673828125\n",
      "epoch: 9 iter: 136 reconn_loss: 9579.83984375 kl_loss: 1735.447021484375\n",
      "epoch: 9 iter: 137 reconn_loss: 10026.080078125 kl_loss: 1688.2529296875\n",
      "epoch: 9 iter: 138 reconn_loss: 10119.6298828125 kl_loss: 1671.992431640625\n",
      "epoch: 9 iter: 139 reconn_loss: 9567.9130859375 kl_loss: 1678.227294921875\n",
      "epoch: 9 iter: 140 reconn_loss: 9868.0546875 kl_loss: 1706.354736328125\n",
      "epoch: 9 iter: 141 reconn_loss: 9908.0439453125 kl_loss: 1683.353759765625\n",
      "epoch: 9 iter: 142 reconn_loss: 9676.9755859375 kl_loss: 1675.056396484375\n",
      "epoch: 9 iter: 143 reconn_loss: 9457.8310546875 kl_loss: 1724.6895751953125\n",
      "epoch: 9 iter: 144 reconn_loss: 9632.623046875 kl_loss: 1660.8671875\n",
      "epoch: 9 iter: 145 reconn_loss: 9794.634765625 kl_loss: 1690.0201416015625\n",
      "epoch: 9 iter: 146 reconn_loss: 9797.4794921875 kl_loss: 1637.3037109375\n",
      "epoch: 9 iter: 147 reconn_loss: 9872.0703125 kl_loss: 1739.76904296875\n",
      "epoch: 9 iter: 148 reconn_loss: 9718.19921875 kl_loss: 1721.67236328125\n",
      "epoch: 9 iter: 149 reconn_loss: 9644.5859375 kl_loss: 1733.25048828125\n",
      "epoch: 9 iter: 150 reconn_loss: 9773.2314453125 kl_loss: 1782.857177734375\n",
      "epoch: 9 iter: 151 reconn_loss: 9732.5224609375 kl_loss: 1665.726806640625\n",
      "epoch: 9 iter: 152 reconn_loss: 9635.873046875 kl_loss: 1698.8553466796875\n",
      "epoch: 9 iter: 153 reconn_loss: 9549.701171875 kl_loss: 1656.516357421875\n",
      "epoch: 9 iter: 154 reconn_loss: 9504.873046875 kl_loss: 1697.3853759765625\n",
      "epoch: 9 iter: 155 reconn_loss: 9715.4326171875 kl_loss: 1700.9659423828125\n",
      "epoch: 9 iter: 156 reconn_loss: 9767.330078125 kl_loss: 1685.051025390625\n",
      "epoch: 9 iter: 157 reconn_loss: 9978.052734375 kl_loss: 1728.727294921875\n",
      "epoch: 9 iter: 158 reconn_loss: 9456.341796875 kl_loss: 1635.0089111328125\n",
      "epoch: 9 iter: 159 reconn_loss: 9507.2021484375 kl_loss: 1606.5518798828125\n",
      "epoch: 9 iter: 160 reconn_loss: 10090.1171875 kl_loss: 1664.0994873046875\n",
      "epoch: 9 iter: 161 reconn_loss: 9994.3271484375 kl_loss: 1671.271484375\n",
      "epoch: 9 iter: 162 reconn_loss: 9580.51171875 kl_loss: 1689.765625\n",
      "epoch: 9 iter: 163 reconn_loss: 9525.814453125 kl_loss: 1605.9207763671875\n",
      "epoch: 9 iter: 164 reconn_loss: 9680.7392578125 kl_loss: 1685.908203125\n",
      "epoch: 9 iter: 165 reconn_loss: 9628.4482421875 kl_loss: 1755.850341796875\n",
      "epoch: 9 iter: 166 reconn_loss: 9678.3701171875 kl_loss: 1651.7578125\n",
      "epoch: 9 iter: 167 reconn_loss: 9799.9345703125 kl_loss: 1684.589599609375\n",
      "epoch: 9 iter: 168 reconn_loss: 9765.87890625 kl_loss: 1686.53466796875\n",
      "epoch: 9 iter: 169 reconn_loss: 9921.326171875 kl_loss: 1694.7254638671875\n",
      "epoch: 9 iter: 170 reconn_loss: 9510.8837890625 kl_loss: 1637.2960205078125\n",
      "epoch: 9 iter: 171 reconn_loss: 9277.92578125 kl_loss: 1630.247802734375\n",
      "epoch: 9 iter: 172 reconn_loss: 9715.044921875 kl_loss: 1656.7158203125\n",
      "epoch: 9 iter: 173 reconn_loss: 9835.4931640625 kl_loss: 1666.8375244140625\n",
      "epoch: 9 iter: 174 reconn_loss: 9337.87109375 kl_loss: 1691.2193603515625\n",
      "epoch: 9 iter: 175 reconn_loss: 9251.54296875 kl_loss: 1690.3717041015625\n",
      "epoch: 9 iter: 176 reconn_loss: 9425.7080078125 kl_loss: 1672.408935546875\n",
      "epoch: 9 iter: 177 reconn_loss: 9574.52734375 kl_loss: 1725.8577880859375\n",
      "epoch: 9 iter: 178 reconn_loss: 9622.5029296875 kl_loss: 1686.8028564453125\n",
      "epoch: 9 iter: 179 reconn_loss: 9646.6484375 kl_loss: 1701.4351806640625\n",
      "epoch: 9 iter: 180 reconn_loss: 9540.0263671875 kl_loss: 1708.825927734375\n",
      "epoch: 9 iter: 181 reconn_loss: 9657.685546875 kl_loss: 1716.0753173828125\n",
      "epoch: 9 iter: 182 reconn_loss: 9744.859375 kl_loss: 1725.9923095703125\n",
      "epoch: 9 iter: 183 reconn_loss: 9715.87109375 kl_loss: 1701.317138671875\n",
      "epoch: 9 iter: 184 reconn_loss: 9265.9765625 kl_loss: 1691.2066650390625\n",
      "epoch: 9 iter: 185 reconn_loss: 9884.4501953125 kl_loss: 1622.629638671875\n",
      "epoch: 9 iter: 186 reconn_loss: 9733.8359375 kl_loss: 1642.3797607421875\n",
      "epoch: 9 iter: 187 reconn_loss: 9563.9365234375 kl_loss: 1719.6177978515625\n",
      "epoch: 9 iter: 188 reconn_loss: 9552.65234375 kl_loss: 1681.3480224609375\n",
      "epoch: 9 iter: 189 reconn_loss: 9345.40234375 kl_loss: 1626.983642578125\n",
      "epoch: 9 iter: 190 reconn_loss: 9722.146484375 kl_loss: 1700.9901123046875\n",
      "epoch: 9 iter: 191 reconn_loss: 9198.23828125 kl_loss: 1757.9898681640625\n",
      "epoch: 9 iter: 192 reconn_loss: 9522.2294921875 kl_loss: 1703.0738525390625\n",
      "epoch: 9 iter: 193 reconn_loss: 9701.984375 kl_loss: 1644.965087890625\n",
      "epoch: 9 iter: 194 reconn_loss: 9505.2197265625 kl_loss: 1607.1578369140625\n",
      "epoch: 9 iter: 195 reconn_loss: 9269.9853515625 kl_loss: 1707.642822265625\n",
      "epoch: 9 iter: 196 reconn_loss: 9529.93359375 kl_loss: 1682.29150390625\n",
      "epoch: 9 iter: 197 reconn_loss: 9555.576171875 kl_loss: 1723.8916015625\n",
      "epoch: 9 iter: 198 reconn_loss: 9506.7138671875 kl_loss: 1658.961669921875\n",
      "epoch: 9 iter: 199 reconn_loss: 9373.880859375 kl_loss: 1676.6392822265625\n",
      "epoch: 9 iter: 200 reconn_loss: 9720.78515625 kl_loss: 1678.806640625\n",
      "epoch: 9 iter: 201 reconn_loss: 9821.23828125 kl_loss: 1664.5675048828125\n",
      "epoch: 9 iter: 202 reconn_loss: 9844.0625 kl_loss: 1709.3642578125\n",
      "epoch: 9 iter: 203 reconn_loss: 9787.1328125 kl_loss: 1690.21533203125\n",
      "epoch: 9 iter: 204 reconn_loss: 9532.494140625 kl_loss: 1659.999267578125\n",
      "epoch: 9 iter: 205 reconn_loss: 9414.5244140625 kl_loss: 1713.0487060546875\n",
      "epoch: 9 iter: 206 reconn_loss: 9698.6025390625 kl_loss: 1689.1510009765625\n",
      "epoch: 9 iter: 207 reconn_loss: 9415.16015625 kl_loss: 1733.49267578125\n",
      "epoch: 9 iter: 208 reconn_loss: 9760.7412109375 kl_loss: 1734.3524169921875\n",
      "epoch: 9 iter: 209 reconn_loss: 9695.9013671875 kl_loss: 1784.5391845703125\n",
      "epoch: 9 iter: 210 reconn_loss: 9534.48828125 kl_loss: 1727.16650390625\n",
      "epoch: 9 iter: 211 reconn_loss: 9211.248046875 kl_loss: 1670.241455078125\n",
      "epoch: 9 iter: 212 reconn_loss: 9318.693359375 kl_loss: 1630.1444091796875\n",
      "epoch: 9 iter: 213 reconn_loss: 9927.53515625 kl_loss: 1769.919921875\n",
      "epoch: 9 iter: 214 reconn_loss: 9607.4580078125 kl_loss: 1711.3916015625\n",
      "epoch: 9 iter: 215 reconn_loss: 9627.3076171875 kl_loss: 1678.0260009765625\n",
      "epoch: 9 iter: 216 reconn_loss: 9490.880859375 kl_loss: 1674.603515625\n",
      "epoch: 9 iter: 217 reconn_loss: 9798.8359375 kl_loss: 1654.458251953125\n",
      "epoch: 9 iter: 218 reconn_loss: 9644.806640625 kl_loss: 1686.33642578125\n",
      "epoch: 9 iter: 219 reconn_loss: 9795.8701171875 kl_loss: 1655.125244140625\n",
      "epoch: 9 iter: 220 reconn_loss: 9697.1435546875 kl_loss: 1649.27880859375\n",
      "epoch: 9 iter: 221 reconn_loss: 9746.7109375 kl_loss: 1602.188720703125\n",
      "epoch: 9 iter: 222 reconn_loss: 6463.36279296875 kl_loss: 1126.4061279296875\n",
      "0.weight tensor(47.6509) tensor(-44.5498)\n",
      "0.bias tensor(39.6022) tensor(-24.8987)\n",
      "2.weight tensor(16.6513) tensor(-18.8611)\n",
      "2.bias tensor(9.3515) tensor(-13.1834)\n",
      "4.weight tensor(11.4964) tensor(-9.3439)\n",
      "4.bias tensor(5.9488) tensor(-5.3066)\n",
      "epoch: 10 iter: 0 reconn_loss: 9254.4296875 kl_loss: 1721.253662109375\n",
      "epoch: 10 iter: 1 reconn_loss: 9441.736328125 kl_loss: 1718.00732421875\n",
      "epoch: 10 iter: 2 reconn_loss: 9488.373046875 kl_loss: 1684.2216796875\n",
      "epoch: 10 iter: 3 reconn_loss: 9533.0498046875 kl_loss: 1666.471435546875\n",
      "epoch: 10 iter: 4 reconn_loss: 10143.1103515625 kl_loss: 1682.6019287109375\n",
      "epoch: 10 iter: 5 reconn_loss: 9392.5849609375 kl_loss: 1686.5587158203125\n",
      "epoch: 10 iter: 6 reconn_loss: 9815.525390625 kl_loss: 1707.197998046875\n",
      "epoch: 10 iter: 7 reconn_loss: 9708.3330078125 kl_loss: 1718.2802734375\n",
      "epoch: 10 iter: 8 reconn_loss: 9740.388671875 kl_loss: 1648.165771484375\n",
      "epoch: 10 iter: 9 reconn_loss: 9660.0712890625 kl_loss: 1680.1702880859375\n",
      "epoch: 10 iter: 10 reconn_loss: 9437.9443359375 kl_loss: 1683.6446533203125\n",
      "epoch: 10 iter: 11 reconn_loss: 9744.0 kl_loss: 1729.0281982421875\n",
      "epoch: 10 iter: 12 reconn_loss: 9559.3359375 kl_loss: 1674.579833984375\n",
      "epoch: 10 iter: 13 reconn_loss: 9638.1669921875 kl_loss: 1756.1888427734375\n",
      "epoch: 10 iter: 14 reconn_loss: 9973.8583984375 kl_loss: 1645.964111328125\n",
      "epoch: 10 iter: 15 reconn_loss: 9593.1552734375 kl_loss: 1693.19140625\n",
      "epoch: 10 iter: 16 reconn_loss: 9983.3349609375 kl_loss: 1699.302734375\n",
      "epoch: 10 iter: 17 reconn_loss: 9567.2490234375 kl_loss: 1728.807861328125\n",
      "epoch: 10 iter: 18 reconn_loss: 9387.9521484375 kl_loss: 1690.9500732421875\n",
      "epoch: 10 iter: 19 reconn_loss: 9557.9521484375 kl_loss: 1717.2340087890625\n",
      "epoch: 10 iter: 20 reconn_loss: 9761.0029296875 kl_loss: 1738.801025390625\n",
      "epoch: 10 iter: 21 reconn_loss: 9671.60546875 kl_loss: 1607.267822265625\n",
      "epoch: 10 iter: 22 reconn_loss: 9425.384765625 kl_loss: 1693.4132080078125\n",
      "epoch: 10 iter: 23 reconn_loss: 9496.5810546875 kl_loss: 1717.782470703125\n",
      "epoch: 10 iter: 24 reconn_loss: 9442.9248046875 kl_loss: 1727.8150634765625\n",
      "epoch: 10 iter: 25 reconn_loss: 9490.5546875 kl_loss: 1718.20849609375\n",
      "epoch: 10 iter: 26 reconn_loss: 9355.8984375 kl_loss: 1762.0078125\n",
      "epoch: 10 iter: 27 reconn_loss: 9832.33203125 kl_loss: 1715.3536376953125\n",
      "epoch: 10 iter: 28 reconn_loss: 9600.5283203125 kl_loss: 1696.5531005859375\n",
      "epoch: 10 iter: 29 reconn_loss: 9795.841796875 kl_loss: 1644.142333984375\n",
      "epoch: 10 iter: 30 reconn_loss: 9528.9091796875 kl_loss: 1725.3487548828125\n",
      "epoch: 10 iter: 31 reconn_loss: 9370.21875 kl_loss: 1677.085205078125\n",
      "epoch: 10 iter: 32 reconn_loss: 9326.583984375 kl_loss: 1759.4635009765625\n",
      "epoch: 10 iter: 33 reconn_loss: 9447.716796875 kl_loss: 1713.21533203125\n",
      "epoch: 10 iter: 34 reconn_loss: 9574.501953125 kl_loss: 1737.03369140625\n",
      "epoch: 10 iter: 35 reconn_loss: 9723.4755859375 kl_loss: 1732.617431640625\n",
      "epoch: 10 iter: 36 reconn_loss: 9855.666015625 kl_loss: 1789.3475341796875\n",
      "epoch: 10 iter: 37 reconn_loss: 9589.4306640625 kl_loss: 1728.4217529296875\n",
      "epoch: 10 iter: 38 reconn_loss: 9590.806640625 kl_loss: 1707.473388671875\n",
      "epoch: 10 iter: 39 reconn_loss: 9376.84375 kl_loss: 1679.043701171875\n",
      "epoch: 10 iter: 40 reconn_loss: 9286.359375 kl_loss: 1708.367919921875\n",
      "epoch: 10 iter: 41 reconn_loss: 9464.3349609375 kl_loss: 1667.3873291015625\n",
      "epoch: 10 iter: 42 reconn_loss: 9411.6025390625 kl_loss: 1660.598388671875\n",
      "epoch: 10 iter: 43 reconn_loss: 9227.126953125 kl_loss: 1705.7982177734375\n",
      "epoch: 10 iter: 44 reconn_loss: 9574.6552734375 kl_loss: 1714.378173828125\n",
      "epoch: 10 iter: 45 reconn_loss: 9108.9873046875 kl_loss: 1637.307373046875\n",
      "epoch: 10 iter: 46 reconn_loss: 9814.162109375 kl_loss: 1655.5078125\n",
      "epoch: 10 iter: 47 reconn_loss: 9363.671875 kl_loss: 1634.818359375\n",
      "epoch: 10 iter: 48 reconn_loss: 9446.125 kl_loss: 1658.254150390625\n",
      "epoch: 10 iter: 49 reconn_loss: 9703.90234375 kl_loss: 1663.650634765625\n",
      "epoch: 10 iter: 50 reconn_loss: 9856.8427734375 kl_loss: 1690.831787109375\n",
      "epoch: 10 iter: 51 reconn_loss: 9501.248046875 kl_loss: 1716.587158203125\n",
      "epoch: 10 iter: 52 reconn_loss: 9735.0849609375 kl_loss: 1651.3389892578125\n",
      "epoch: 10 iter: 53 reconn_loss: 9077.228515625 kl_loss: 1622.230224609375\n",
      "epoch: 10 iter: 54 reconn_loss: 9402.9921875 kl_loss: 1712.8878173828125\n",
      "epoch: 10 iter: 55 reconn_loss: 9514.51953125 kl_loss: 1740.210693359375\n",
      "epoch: 10 iter: 56 reconn_loss: 9550.7138671875 kl_loss: 1736.2554931640625\n",
      "epoch: 10 iter: 57 reconn_loss: 9386.748046875 kl_loss: 1662.0096435546875\n",
      "epoch: 10 iter: 58 reconn_loss: 9645.9150390625 kl_loss: 1776.3304443359375\n",
      "epoch: 10 iter: 59 reconn_loss: 9469.1103515625 kl_loss: 1662.543212890625\n",
      "epoch: 10 iter: 60 reconn_loss: 9345.3876953125 kl_loss: 1702.9315185546875\n",
      "epoch: 10 iter: 61 reconn_loss: 9939.62109375 kl_loss: 1726.6116943359375\n",
      "epoch: 10 iter: 62 reconn_loss: 9586.537109375 kl_loss: 1764.3487548828125\n",
      "epoch: 10 iter: 63 reconn_loss: 9575.841796875 kl_loss: 1705.9139404296875\n",
      "epoch: 10 iter: 64 reconn_loss: 9712.8505859375 kl_loss: 1709.84228515625\n",
      "epoch: 10 iter: 65 reconn_loss: 9516.6796875 kl_loss: 1716.9886474609375\n",
      "epoch: 10 iter: 66 reconn_loss: 9762.0302734375 kl_loss: 1680.7813720703125\n",
      "epoch: 10 iter: 67 reconn_loss: 9373.0419921875 kl_loss: 1702.6600341796875\n",
      "epoch: 10 iter: 68 reconn_loss: 9606.5859375 kl_loss: 1771.686767578125\n",
      "epoch: 10 iter: 69 reconn_loss: 9554.6806640625 kl_loss: 1733.917236328125\n",
      "epoch: 10 iter: 70 reconn_loss: 10046.6044921875 kl_loss: 1703.7109375\n",
      "epoch: 10 iter: 71 reconn_loss: 9592.2861328125 kl_loss: 1697.78466796875\n",
      "epoch: 10 iter: 72 reconn_loss: 9443.525390625 kl_loss: 1676.14208984375\n",
      "epoch: 10 iter: 73 reconn_loss: 9604.68359375 kl_loss: 1622.587890625\n",
      "epoch: 10 iter: 74 reconn_loss: 9608.9013671875 kl_loss: 1704.0555419921875\n",
      "epoch: 10 iter: 75 reconn_loss: 9592.6064453125 kl_loss: 1696.424560546875\n",
      "epoch: 10 iter: 76 reconn_loss: 9531.6552734375 kl_loss: 1660.1517333984375\n",
      "epoch: 10 iter: 77 reconn_loss: 9637.470703125 kl_loss: 1682.8134765625\n",
      "epoch: 10 iter: 78 reconn_loss: 9459.8798828125 kl_loss: 1678.2001953125\n",
      "epoch: 10 iter: 79 reconn_loss: 9751.3232421875 kl_loss: 1719.8292236328125\n",
      "epoch: 10 iter: 80 reconn_loss: 9492.8486328125 kl_loss: 1701.0960693359375\n",
      "epoch: 10 iter: 81 reconn_loss: 9462.80859375 kl_loss: 1690.482177734375\n",
      "epoch: 10 iter: 82 reconn_loss: 9476.63671875 kl_loss: 1720.87646484375\n",
      "epoch: 10 iter: 83 reconn_loss: 9151.5009765625 kl_loss: 1646.162353515625\n",
      "epoch: 10 iter: 84 reconn_loss: 9535.6875 kl_loss: 1704.603515625\n",
      "epoch: 10 iter: 85 reconn_loss: 9558.9765625 kl_loss: 1747.237060546875\n",
      "epoch: 10 iter: 86 reconn_loss: 9812.4404296875 kl_loss: 1695.0616455078125\n",
      "epoch: 10 iter: 87 reconn_loss: 9694.658203125 kl_loss: 1661.34130859375\n",
      "epoch: 10 iter: 88 reconn_loss: 9786.82421875 kl_loss: 1696.5894775390625\n",
      "epoch: 10 iter: 89 reconn_loss: 9381.6123046875 kl_loss: 1694.3468017578125\n",
      "epoch: 10 iter: 90 reconn_loss: 9949.99609375 kl_loss: 1713.7515869140625\n",
      "epoch: 10 iter: 91 reconn_loss: 9683.0810546875 kl_loss: 1712.5936279296875\n",
      "epoch: 10 iter: 92 reconn_loss: 9705.978515625 kl_loss: 1706.8519287109375\n",
      "epoch: 10 iter: 93 reconn_loss: 9474.47265625 kl_loss: 1717.4630126953125\n",
      "epoch: 10 iter: 94 reconn_loss: 9591.90234375 kl_loss: 1672.7301025390625\n",
      "epoch: 10 iter: 95 reconn_loss: 9630.193359375 kl_loss: 1715.1494140625\n",
      "epoch: 10 iter: 96 reconn_loss: 9538.419921875 kl_loss: 1745.279541015625\n",
      "epoch: 10 iter: 97 reconn_loss: 9464.50390625 kl_loss: 1698.39697265625\n",
      "epoch: 10 iter: 98 reconn_loss: 9515.505859375 kl_loss: 1680.2861328125\n",
      "epoch: 10 iter: 99 reconn_loss: 9643.4501953125 kl_loss: 1713.1475830078125\n",
      "epoch: 10 iter: 100 reconn_loss: 9670.0859375 kl_loss: 1731.315673828125\n",
      "epoch: 10 iter: 101 reconn_loss: 9422.4345703125 kl_loss: 1723.7437744140625\n",
      "epoch: 10 iter: 102 reconn_loss: 9238.8876953125 kl_loss: 1743.1121826171875\n",
      "epoch: 10 iter: 103 reconn_loss: 9494.8271484375 kl_loss: 1704.001953125\n",
      "epoch: 10 iter: 104 reconn_loss: 9477.2783203125 kl_loss: 1685.497314453125\n",
      "epoch: 10 iter: 105 reconn_loss: 9564.4765625 kl_loss: 1769.76513671875\n",
      "epoch: 10 iter: 106 reconn_loss: 9580.6953125 kl_loss: 1715.1947021484375\n",
      "epoch: 10 iter: 107 reconn_loss: 9484.7978515625 kl_loss: 1714.5733642578125\n",
      "epoch: 10 iter: 108 reconn_loss: 9343.833984375 kl_loss: 1691.887939453125\n",
      "epoch: 10 iter: 109 reconn_loss: 9538.455078125 kl_loss: 1662.6337890625\n",
      "epoch: 10 iter: 110 reconn_loss: 9637.322265625 kl_loss: 1685.32421875\n",
      "epoch: 10 iter: 111 reconn_loss: 9293.267578125 kl_loss: 1739.874267578125\n",
      "epoch: 10 iter: 112 reconn_loss: 9402.3349609375 kl_loss: 1681.147216796875\n",
      "epoch: 10 iter: 113 reconn_loss: 9430.5263671875 kl_loss: 1694.52001953125\n",
      "epoch: 10 iter: 114 reconn_loss: 9378.205078125 kl_loss: 1680.8299560546875\n",
      "epoch: 10 iter: 115 reconn_loss: 9498.0732421875 kl_loss: 1663.47509765625\n",
      "epoch: 10 iter: 116 reconn_loss: 9754.2421875 kl_loss: 1661.112548828125\n",
      "epoch: 10 iter: 117 reconn_loss: 9722.8154296875 kl_loss: 1726.7900390625\n",
      "epoch: 10 iter: 118 reconn_loss: 9302.40234375 kl_loss: 1712.8359375\n",
      "epoch: 10 iter: 119 reconn_loss: 9598.0234375 kl_loss: 1728.3924560546875\n",
      "epoch: 10 iter: 120 reconn_loss: 9551.197265625 kl_loss: 1758.308837890625\n",
      "epoch: 10 iter: 121 reconn_loss: 9673.294921875 kl_loss: 1721.116943359375\n",
      "epoch: 10 iter: 122 reconn_loss: 9327.7841796875 kl_loss: 1748.57275390625\n",
      "epoch: 10 iter: 123 reconn_loss: 9793.78125 kl_loss: 1700.9949951171875\n",
      "epoch: 10 iter: 124 reconn_loss: 9862.7021484375 kl_loss: 1732.9483642578125\n",
      "epoch: 10 iter: 125 reconn_loss: 9582.51171875 kl_loss: 1745.091552734375\n",
      "epoch: 10 iter: 126 reconn_loss: 9771.658203125 kl_loss: 1696.5765380859375\n",
      "epoch: 10 iter: 127 reconn_loss: 9562.8876953125 kl_loss: 1745.2735595703125\n",
      "epoch: 10 iter: 128 reconn_loss: 9269.6943359375 kl_loss: 1741.71484375\n",
      "epoch: 10 iter: 129 reconn_loss: 9431.158203125 kl_loss: 1762.0440673828125\n",
      "epoch: 10 iter: 130 reconn_loss: 9264.0498046875 kl_loss: 1776.2381591796875\n",
      "epoch: 10 iter: 131 reconn_loss: 9856.701171875 kl_loss: 1759.1348876953125\n",
      "epoch: 10 iter: 132 reconn_loss: 9703.0654296875 kl_loss: 1733.7835693359375\n",
      "epoch: 10 iter: 133 reconn_loss: 9460.0322265625 kl_loss: 1714.75146484375\n",
      "epoch: 10 iter: 134 reconn_loss: 9266.431640625 kl_loss: 1721.6220703125\n",
      "epoch: 10 iter: 135 reconn_loss: 9442.21875 kl_loss: 1616.4730224609375\n",
      "epoch: 10 iter: 136 reconn_loss: 9748.28125 kl_loss: 1680.8236083984375\n",
      "epoch: 10 iter: 137 reconn_loss: 9320.83984375 kl_loss: 1648.6861572265625\n",
      "epoch: 10 iter: 138 reconn_loss: 9586.8916015625 kl_loss: 1722.7203369140625\n",
      "epoch: 10 iter: 139 reconn_loss: 9348.658203125 kl_loss: 1717.21142578125\n",
      "epoch: 10 iter: 140 reconn_loss: 9737.208984375 kl_loss: 1753.320068359375\n",
      "epoch: 10 iter: 141 reconn_loss: 9596.416015625 kl_loss: 1698.0321044921875\n",
      "epoch: 10 iter: 142 reconn_loss: 9945.23046875 kl_loss: 1691.07763671875\n",
      "epoch: 10 iter: 143 reconn_loss: 9514.8876953125 kl_loss: 1686.5716552734375\n",
      "epoch: 10 iter: 144 reconn_loss: 9450.052734375 kl_loss: 1712.3731689453125\n",
      "epoch: 10 iter: 145 reconn_loss: 9427.5419921875 kl_loss: 1640.414794921875\n",
      "epoch: 10 iter: 146 reconn_loss: 9641.3115234375 kl_loss: 1687.6239013671875\n",
      "epoch: 10 iter: 147 reconn_loss: 9635.52734375 kl_loss: 1689.25927734375\n",
      "epoch: 10 iter: 148 reconn_loss: 9392.5546875 kl_loss: 1762.291015625\n",
      "epoch: 10 iter: 149 reconn_loss: 9599.162109375 kl_loss: 1718.703125\n",
      "epoch: 10 iter: 150 reconn_loss: 9341.072265625 kl_loss: 1733.7813720703125\n",
      "epoch: 10 iter: 151 reconn_loss: 9231.4150390625 kl_loss: 1723.845703125\n",
      "epoch: 10 iter: 152 reconn_loss: 9572.986328125 kl_loss: 1777.4976806640625\n",
      "epoch: 10 iter: 153 reconn_loss: 9265.044921875 kl_loss: 1690.551025390625\n",
      "epoch: 10 iter: 154 reconn_loss: 9828.33203125 kl_loss: 1737.4669189453125\n",
      "epoch: 10 iter: 155 reconn_loss: 9412.36328125 kl_loss: 1723.661376953125\n",
      "epoch: 10 iter: 156 reconn_loss: 9264.5068359375 kl_loss: 1742.5364990234375\n",
      "epoch: 10 iter: 157 reconn_loss: 9496.9130859375 kl_loss: 1716.9400634765625\n",
      "epoch: 10 iter: 158 reconn_loss: 9642.822265625 kl_loss: 1760.1497802734375\n",
      "epoch: 10 iter: 159 reconn_loss: 9745.01953125 kl_loss: 1736.024658203125\n",
      "epoch: 10 iter: 160 reconn_loss: 9359.7705078125 kl_loss: 1717.3828125\n",
      "epoch: 10 iter: 161 reconn_loss: 9499.505859375 kl_loss: 1723.650146484375\n",
      "epoch: 10 iter: 162 reconn_loss: 9624.34375 kl_loss: 1750.84716796875\n",
      "epoch: 10 iter: 163 reconn_loss: 9426.408203125 kl_loss: 1691.426513671875\n",
      "epoch: 10 iter: 164 reconn_loss: 9298.8115234375 kl_loss: 1752.0889892578125\n",
      "epoch: 10 iter: 165 reconn_loss: 9217.830078125 kl_loss: 1695.503173828125\n",
      "epoch: 10 iter: 166 reconn_loss: 9686.4541015625 kl_loss: 1690.360107421875\n",
      "epoch: 10 iter: 167 reconn_loss: 9200.47265625 kl_loss: 1692.626708984375\n",
      "epoch: 10 iter: 168 reconn_loss: 9501.251953125 kl_loss: 1660.73193359375\n",
      "epoch: 10 iter: 169 reconn_loss: 9637.83984375 kl_loss: 1711.85546875\n",
      "epoch: 10 iter: 170 reconn_loss: 9555.3193359375 kl_loss: 1689.923095703125\n",
      "epoch: 10 iter: 171 reconn_loss: 9314.32421875 kl_loss: 1721.6826171875\n",
      "epoch: 10 iter: 172 reconn_loss: 9451.8935546875 kl_loss: 1705.0635986328125\n",
      "epoch: 10 iter: 173 reconn_loss: 9285.6982421875 kl_loss: 1716.2855224609375\n",
      "epoch: 10 iter: 174 reconn_loss: 9564.3232421875 kl_loss: 1716.5330810546875\n",
      "epoch: 10 iter: 175 reconn_loss: 9793.9169921875 kl_loss: 1740.9674072265625\n",
      "epoch: 10 iter: 176 reconn_loss: 9631.505859375 kl_loss: 1722.284912109375\n",
      "epoch: 10 iter: 177 reconn_loss: 9672.0205078125 kl_loss: 1783.9864501953125\n",
      "epoch: 10 iter: 178 reconn_loss: 9644.16015625 kl_loss: 1736.64111328125\n",
      "epoch: 10 iter: 179 reconn_loss: 9438.150390625 kl_loss: 1729.771484375\n",
      "epoch: 10 iter: 180 reconn_loss: 9497.4111328125 kl_loss: 1747.2315673828125\n",
      "epoch: 10 iter: 181 reconn_loss: 9759.341796875 kl_loss: 1734.370361328125\n",
      "epoch: 10 iter: 182 reconn_loss: 9656.5947265625 kl_loss: 1712.347412109375\n",
      "epoch: 10 iter: 183 reconn_loss: 9656.970703125 kl_loss: 1725.596923828125\n",
      "epoch: 10 iter: 184 reconn_loss: 9935.48046875 kl_loss: 1770.94921875\n",
      "epoch: 10 iter: 185 reconn_loss: 9768.166015625 kl_loss: 1675.1142578125\n",
      "epoch: 10 iter: 186 reconn_loss: 9432.3046875 kl_loss: 1708.114013671875\n",
      "epoch: 10 iter: 187 reconn_loss: 9472.041015625 kl_loss: 1712.18212890625\n",
      "epoch: 10 iter: 188 reconn_loss: 9364.80078125 kl_loss: 1717.73486328125\n",
      "epoch: 10 iter: 189 reconn_loss: 9711.244140625 kl_loss: 1677.7479248046875\n",
      "epoch: 10 iter: 190 reconn_loss: 9769.97265625 kl_loss: 1688.0709228515625\n",
      "epoch: 10 iter: 191 reconn_loss: 9712.740234375 kl_loss: 1734.37841796875\n",
      "epoch: 10 iter: 192 reconn_loss: 9634.501953125 kl_loss: 1717.72900390625\n",
      "epoch: 10 iter: 193 reconn_loss: 9601.25390625 kl_loss: 1699.7147216796875\n",
      "epoch: 10 iter: 194 reconn_loss: 9548.697265625 kl_loss: 1729.135498046875\n",
      "epoch: 10 iter: 195 reconn_loss: 9456.275390625 kl_loss: 1712.2098388671875\n",
      "epoch: 10 iter: 196 reconn_loss: 9600.119140625 kl_loss: 1810.7451171875\n",
      "epoch: 10 iter: 197 reconn_loss: 9620.2421875 kl_loss: 1748.2537841796875\n",
      "epoch: 10 iter: 198 reconn_loss: 9498.880859375 kl_loss: 1741.8729248046875\n",
      "epoch: 10 iter: 199 reconn_loss: 9483.970703125 kl_loss: 1806.00830078125\n",
      "epoch: 10 iter: 200 reconn_loss: 9447.97265625 kl_loss: 1690.968505859375\n",
      "epoch: 10 iter: 201 reconn_loss: 9436.7705078125 kl_loss: 1719.2943115234375\n",
      "epoch: 10 iter: 202 reconn_loss: 9549.0673828125 kl_loss: 1816.2431640625\n",
      "epoch: 10 iter: 203 reconn_loss: 9358.5927734375 kl_loss: 1747.679931640625\n",
      "epoch: 10 iter: 204 reconn_loss: 9853.037109375 kl_loss: 1744.14599609375\n",
      "epoch: 10 iter: 205 reconn_loss: 9927.421875 kl_loss: 1698.317138671875\n",
      "epoch: 10 iter: 206 reconn_loss: 9588.390625 kl_loss: 1730.3458251953125\n",
      "epoch: 10 iter: 207 reconn_loss: 9509.8203125 kl_loss: 1708.39453125\n",
      "epoch: 10 iter: 208 reconn_loss: 9558.21484375 kl_loss: 1734.75146484375\n",
      "epoch: 10 iter: 209 reconn_loss: 9229.82421875 kl_loss: 1703.94140625\n",
      "epoch: 10 iter: 210 reconn_loss: 9449.48046875 kl_loss: 1774.8704833984375\n",
      "epoch: 10 iter: 211 reconn_loss: 9632.2138671875 kl_loss: 1750.870849609375\n",
      "epoch: 10 iter: 212 reconn_loss: 9337.0556640625 kl_loss: 1765.4945068359375\n",
      "epoch: 10 iter: 213 reconn_loss: 9525.0478515625 kl_loss: 1765.6165771484375\n",
      "epoch: 10 iter: 214 reconn_loss: 9578.5927734375 kl_loss: 1746.0447998046875\n",
      "epoch: 10 iter: 215 reconn_loss: 9641.794921875 kl_loss: 1746.0953369140625\n",
      "epoch: 10 iter: 216 reconn_loss: 8899.3935546875 kl_loss: 1679.3521728515625\n",
      "epoch: 10 iter: 217 reconn_loss: 9604.3046875 kl_loss: 1711.20654296875\n",
      "epoch: 10 iter: 218 reconn_loss: 9417.80859375 kl_loss: 1747.6484375\n",
      "epoch: 10 iter: 219 reconn_loss: 9486.189453125 kl_loss: 1681.5760498046875\n",
      "epoch: 10 iter: 220 reconn_loss: 9429.796875 kl_loss: 1701.5927734375\n",
      "epoch: 10 iter: 221 reconn_loss: 9310.87109375 kl_loss: 1795.2193603515625\n",
      "epoch: 10 iter: 222 reconn_loss: 6241.23779296875 kl_loss: 1154.6776123046875\n",
      "0.weight tensor(56.4304) tensor(-54.8669)\n",
      "0.bias tensor(29.6897) tensor(-26.7324)\n",
      "2.weight tensor(16.5935) tensor(-15.4641)\n",
      "2.bias tensor(9.9171) tensor(-10.0399)\n",
      "4.weight tensor(8.7503) tensor(-10.9072)\n",
      "4.bias tensor(4.5282) tensor(-6.8052)\n",
      "epoch: 11 iter: 0 reconn_loss: 9408.0712890625 kl_loss: 1763.776123046875\n",
      "epoch: 11 iter: 1 reconn_loss: 9445.5546875 kl_loss: 1745.5654296875\n",
      "epoch: 11 iter: 2 reconn_loss: 9591.513671875 kl_loss: 1701.93603515625\n",
      "epoch: 11 iter: 3 reconn_loss: 9553.736328125 kl_loss: 1772.864013671875\n",
      "epoch: 11 iter: 4 reconn_loss: 9414.73828125 kl_loss: 1777.432373046875\n",
      "epoch: 11 iter: 5 reconn_loss: 9786.646484375 kl_loss: 1741.1185302734375\n",
      "epoch: 11 iter: 6 reconn_loss: 9515.0546875 kl_loss: 1687.6922607421875\n",
      "epoch: 11 iter: 7 reconn_loss: 9486.484375 kl_loss: 1760.5548095703125\n",
      "epoch: 11 iter: 8 reconn_loss: 9097.7666015625 kl_loss: 1708.708251953125\n",
      "epoch: 11 iter: 9 reconn_loss: 9667.6494140625 kl_loss: 1693.293212890625\n",
      "epoch: 11 iter: 10 reconn_loss: 9634.7890625 kl_loss: 1696.758544921875\n",
      "epoch: 11 iter: 11 reconn_loss: 9172.384765625 kl_loss: 1739.535888671875\n",
      "epoch: 11 iter: 12 reconn_loss: 9171.8984375 kl_loss: 1780.339599609375\n",
      "epoch: 11 iter: 13 reconn_loss: 9493.0263671875 kl_loss: 1738.10205078125\n",
      "epoch: 11 iter: 14 reconn_loss: 9507.59765625 kl_loss: 1718.8504638671875\n",
      "epoch: 11 iter: 15 reconn_loss: 9564.6572265625 kl_loss: 1726.046875\n",
      "epoch: 11 iter: 16 reconn_loss: 9475.8076171875 kl_loss: 1754.693359375\n",
      "epoch: 11 iter: 17 reconn_loss: 9583.494140625 kl_loss: 1706.4541015625\n",
      "epoch: 11 iter: 18 reconn_loss: 9777.5400390625 kl_loss: 1743.1728515625\n",
      "epoch: 11 iter: 19 reconn_loss: 9506.15234375 kl_loss: 1680.147216796875\n",
      "epoch: 11 iter: 20 reconn_loss: 9372.7802734375 kl_loss: 1749.9471435546875\n",
      "epoch: 11 iter: 21 reconn_loss: 9702.310546875 kl_loss: 1663.89599609375\n",
      "epoch: 11 iter: 22 reconn_loss: 9457.689453125 kl_loss: 1686.58447265625\n",
      "epoch: 11 iter: 23 reconn_loss: 9589.96484375 kl_loss: 1740.304931640625\n",
      "epoch: 11 iter: 24 reconn_loss: 9185.865234375 kl_loss: 1764.1337890625\n",
      "epoch: 11 iter: 25 reconn_loss: 9453.658203125 kl_loss: 1667.5130615234375\n",
      "epoch: 11 iter: 26 reconn_loss: 9413.974609375 kl_loss: 1671.226806640625\n",
      "epoch: 11 iter: 27 reconn_loss: 9497.7529296875 kl_loss: 1706.980712890625\n",
      "epoch: 11 iter: 28 reconn_loss: 9076.427734375 kl_loss: 1697.416259765625\n",
      "epoch: 11 iter: 29 reconn_loss: 9458.5341796875 kl_loss: 1713.865234375\n",
      "epoch: 11 iter: 30 reconn_loss: 9414.6728515625 kl_loss: 1744.291015625\n",
      "epoch: 11 iter: 31 reconn_loss: 9769.6806640625 kl_loss: 1740.1295166015625\n",
      "epoch: 11 iter: 32 reconn_loss: 9337.484375 kl_loss: 1725.3486328125\n",
      "epoch: 11 iter: 33 reconn_loss: 9615.0830078125 kl_loss: 1798.8118896484375\n",
      "epoch: 11 iter: 34 reconn_loss: 9231.732421875 kl_loss: 1713.25830078125\n",
      "epoch: 11 iter: 35 reconn_loss: 9163.09375 kl_loss: 1679.252197265625\n",
      "epoch: 11 iter: 36 reconn_loss: 9391.7119140625 kl_loss: 1712.5301513671875\n",
      "epoch: 11 iter: 37 reconn_loss: 9680.8818359375 kl_loss: 1746.320556640625\n",
      "epoch: 11 iter: 38 reconn_loss: 9712.171875 kl_loss: 1721.354736328125\n",
      "epoch: 11 iter: 39 reconn_loss: 9494.37890625 kl_loss: 1681.708984375\n",
      "epoch: 11 iter: 40 reconn_loss: 9326.556640625 kl_loss: 1716.3031005859375\n",
      "epoch: 11 iter: 41 reconn_loss: 9609.337890625 kl_loss: 1765.02734375\n",
      "epoch: 11 iter: 42 reconn_loss: 9498.1826171875 kl_loss: 1701.1331787109375\n",
      "epoch: 11 iter: 43 reconn_loss: 9668.234375 kl_loss: 1760.6649169921875\n",
      "epoch: 11 iter: 44 reconn_loss: 9290.4833984375 kl_loss: 1712.994384765625\n",
      "epoch: 11 iter: 45 reconn_loss: 9222.5751953125 kl_loss: 1705.1654052734375\n",
      "epoch: 11 iter: 46 reconn_loss: 9528.3203125 kl_loss: 1721.414794921875\n",
      "epoch: 11 iter: 47 reconn_loss: 9399.2080078125 kl_loss: 1771.036376953125\n",
      "epoch: 11 iter: 48 reconn_loss: 9047.2333984375 kl_loss: 1743.0482177734375\n",
      "epoch: 11 iter: 49 reconn_loss: 9322.697265625 kl_loss: 1796.86669921875\n",
      "epoch: 11 iter: 50 reconn_loss: 9182.3779296875 kl_loss: 1744.734619140625\n",
      "epoch: 11 iter: 51 reconn_loss: 9817.3388671875 kl_loss: 1804.038330078125\n",
      "epoch: 11 iter: 52 reconn_loss: 9221.12109375 kl_loss: 1713.6767578125\n",
      "epoch: 11 iter: 53 reconn_loss: 9863.9951171875 kl_loss: 1715.2916259765625\n",
      "epoch: 11 iter: 54 reconn_loss: 9334.1650390625 kl_loss: 1756.199462890625\n",
      "epoch: 11 iter: 55 reconn_loss: 9493.7236328125 kl_loss: 1731.064697265625\n",
      "epoch: 11 iter: 56 reconn_loss: 9267.26171875 kl_loss: 1746.10888671875\n",
      "epoch: 11 iter: 57 reconn_loss: 9362.8271484375 kl_loss: 1718.037109375\n",
      "epoch: 11 iter: 58 reconn_loss: 9610.2958984375 kl_loss: 1762.947509765625\n",
      "epoch: 11 iter: 59 reconn_loss: 9329.9248046875 kl_loss: 1681.579345703125\n",
      "epoch: 11 iter: 60 reconn_loss: 8979.3515625 kl_loss: 1762.3516845703125\n",
      "epoch: 11 iter: 61 reconn_loss: 9619.7431640625 kl_loss: 1732.8924560546875\n",
      "epoch: 11 iter: 62 reconn_loss: 9523.1396484375 kl_loss: 1739.9537353515625\n",
      "epoch: 11 iter: 63 reconn_loss: 9526.0703125 kl_loss: 1798.70703125\n",
      "epoch: 11 iter: 64 reconn_loss: 9568.81640625 kl_loss: 1721.516357421875\n",
      "epoch: 11 iter: 65 reconn_loss: 9399.5458984375 kl_loss: 1735.816162109375\n",
      "epoch: 11 iter: 66 reconn_loss: 9521.67578125 kl_loss: 1712.482421875\n",
      "epoch: 11 iter: 67 reconn_loss: 9693.818359375 kl_loss: 1761.3489990234375\n",
      "epoch: 11 iter: 68 reconn_loss: 9244.1875 kl_loss: 1725.4091796875\n",
      "epoch: 11 iter: 69 reconn_loss: 9381.66015625 kl_loss: 1753.2913818359375\n",
      "epoch: 11 iter: 70 reconn_loss: 9592.423828125 kl_loss: 1725.6177978515625\n",
      "epoch: 11 iter: 71 reconn_loss: 9693.46484375 kl_loss: 1746.5145263671875\n",
      "epoch: 11 iter: 72 reconn_loss: 9725.2890625 kl_loss: 1681.4110107421875\n",
      "epoch: 11 iter: 73 reconn_loss: 9786.6416015625 kl_loss: 1787.1461181640625\n",
      "epoch: 11 iter: 74 reconn_loss: 9243.5185546875 kl_loss: 1717.5186767578125\n",
      "epoch: 11 iter: 75 reconn_loss: 9478.248046875 kl_loss: 1700.4053955078125\n",
      "epoch: 11 iter: 76 reconn_loss: 9291.5458984375 kl_loss: 1746.574462890625\n",
      "epoch: 11 iter: 77 reconn_loss: 9539.720703125 kl_loss: 1712.0736083984375\n",
      "epoch: 11 iter: 78 reconn_loss: 9474.2265625 kl_loss: 1769.8497314453125\n",
      "epoch: 11 iter: 79 reconn_loss: 9603.7294921875 kl_loss: 1747.08056640625\n",
      "epoch: 11 iter: 80 reconn_loss: 9514.2587890625 kl_loss: 1660.7021484375\n",
      "epoch: 11 iter: 81 reconn_loss: 9009.361328125 kl_loss: 1756.91796875\n",
      "epoch: 11 iter: 82 reconn_loss: 9586.4560546875 kl_loss: 1731.5296630859375\n",
      "epoch: 11 iter: 83 reconn_loss: 9321.474609375 kl_loss: 1729.61083984375\n",
      "epoch: 11 iter: 84 reconn_loss: 9801.94921875 kl_loss: 1753.5810546875\n",
      "epoch: 11 iter: 85 reconn_loss: 9351.68359375 kl_loss: 1727.1729736328125\n",
      "epoch: 11 iter: 86 reconn_loss: 9519.6455078125 kl_loss: 1782.331787109375\n",
      "epoch: 11 iter: 87 reconn_loss: 9375.3544921875 kl_loss: 1701.850830078125\n",
      "epoch: 11 iter: 88 reconn_loss: 9601.6044921875 kl_loss: 1742.9715576171875\n",
      "epoch: 11 iter: 89 reconn_loss: 9318.0400390625 kl_loss: 1724.7950439453125\n",
      "epoch: 11 iter: 90 reconn_loss: 9396.134765625 kl_loss: 1728.3612060546875\n",
      "epoch: 11 iter: 91 reconn_loss: 9432.439453125 kl_loss: 1763.9049072265625\n",
      "epoch: 11 iter: 92 reconn_loss: 9353.16015625 kl_loss: 1768.1751708984375\n",
      "epoch: 11 iter: 93 reconn_loss: 9334.9755859375 kl_loss: 1728.8060302734375\n",
      "epoch: 11 iter: 94 reconn_loss: 9278.71484375 kl_loss: 1730.33984375\n",
      "epoch: 11 iter: 95 reconn_loss: 9526.4794921875 kl_loss: 1757.1807861328125\n",
      "epoch: 11 iter: 96 reconn_loss: 9617.5029296875 kl_loss: 1718.3424072265625\n",
      "epoch: 11 iter: 97 reconn_loss: 9738.662109375 kl_loss: 1770.285400390625\n",
      "epoch: 11 iter: 98 reconn_loss: 9931.1416015625 kl_loss: 1771.543701171875\n",
      "epoch: 11 iter: 99 reconn_loss: 9440.380859375 kl_loss: 1721.8411865234375\n",
      "epoch: 11 iter: 100 reconn_loss: 9449.2880859375 kl_loss: 1806.5289306640625\n",
      "epoch: 11 iter: 101 reconn_loss: 9299.529296875 kl_loss: 1750.752685546875\n",
      "epoch: 11 iter: 102 reconn_loss: 9494.62109375 kl_loss: 1759.72802734375\n",
      "epoch: 11 iter: 103 reconn_loss: 9285.822265625 kl_loss: 1738.490966796875\n",
      "epoch: 11 iter: 104 reconn_loss: 9675.46484375 kl_loss: 1758.823974609375\n",
      "epoch: 11 iter: 105 reconn_loss: 9498.2880859375 kl_loss: 1771.7164306640625\n",
      "epoch: 11 iter: 106 reconn_loss: 9298.576171875 kl_loss: 1740.162841796875\n",
      "epoch: 11 iter: 107 reconn_loss: 9628.6591796875 kl_loss: 1731.593505859375\n",
      "epoch: 11 iter: 108 reconn_loss: 9527.359375 kl_loss: 1740.3880615234375\n",
      "epoch: 11 iter: 109 reconn_loss: 9743.2685546875 kl_loss: 1764.504638671875\n",
      "epoch: 11 iter: 110 reconn_loss: 9135.0703125 kl_loss: 1765.6558837890625\n",
      "epoch: 11 iter: 111 reconn_loss: 9249.8369140625 kl_loss: 1794.0474853515625\n",
      "epoch: 11 iter: 112 reconn_loss: 9479.34375 kl_loss: 1777.70556640625\n",
      "epoch: 11 iter: 113 reconn_loss: 9036.9892578125 kl_loss: 1735.7010498046875\n",
      "epoch: 11 iter: 114 reconn_loss: 9302.826171875 kl_loss: 1775.4906005859375\n",
      "epoch: 11 iter: 115 reconn_loss: 9652.125 kl_loss: 1738.79443359375\n",
      "epoch: 11 iter: 116 reconn_loss: 9455.9873046875 kl_loss: 1710.5670166015625\n",
      "epoch: 11 iter: 117 reconn_loss: 9701.814453125 kl_loss: 1784.7496337890625\n",
      "epoch: 11 iter: 118 reconn_loss: 9191.5498046875 kl_loss: 1720.1072998046875\n",
      "epoch: 11 iter: 119 reconn_loss: 9113.9990234375 kl_loss: 1751.2069091796875\n",
      "epoch: 11 iter: 120 reconn_loss: 9266.1318359375 kl_loss: 1726.3199462890625\n",
      "epoch: 11 iter: 121 reconn_loss: 9249.9013671875 kl_loss: 1767.4248046875\n",
      "epoch: 11 iter: 122 reconn_loss: 9722.8671875 kl_loss: 1767.3009033203125\n",
      "epoch: 11 iter: 123 reconn_loss: 9490.1640625 kl_loss: 1749.54150390625\n",
      "epoch: 11 iter: 124 reconn_loss: 9618.55078125 kl_loss: 1800.709228515625\n",
      "epoch: 11 iter: 125 reconn_loss: 9446.5693359375 kl_loss: 1795.9266357421875\n",
      "epoch: 11 iter: 126 reconn_loss: 9387.0908203125 kl_loss: 1784.8409423828125\n",
      "epoch: 11 iter: 127 reconn_loss: 9287.5830078125 kl_loss: 1762.826904296875\n",
      "epoch: 11 iter: 128 reconn_loss: 9230.298828125 kl_loss: 1765.55126953125\n",
      "epoch: 11 iter: 129 reconn_loss: 9454.84765625 kl_loss: 1733.70361328125\n",
      "epoch: 11 iter: 130 reconn_loss: 9447.45703125 kl_loss: 1713.5830078125\n",
      "epoch: 11 iter: 131 reconn_loss: 9652.05078125 kl_loss: 1767.8077392578125\n",
      "epoch: 11 iter: 132 reconn_loss: 9502.375 kl_loss: 1703.9981689453125\n",
      "epoch: 11 iter: 133 reconn_loss: 9290.4501953125 kl_loss: 1725.649169921875\n",
      "epoch: 11 iter: 134 reconn_loss: 9649.8916015625 kl_loss: 1709.6202392578125\n",
      "epoch: 11 iter: 135 reconn_loss: 9267.5400390625 kl_loss: 1712.6383056640625\n",
      "epoch: 11 iter: 136 reconn_loss: 9318.47265625 kl_loss: 1713.3701171875\n",
      "epoch: 11 iter: 137 reconn_loss: 9523.7802734375 kl_loss: 1733.78125\n",
      "epoch: 11 iter: 138 reconn_loss: 9274.8193359375 kl_loss: 1768.64111328125\n",
      "epoch: 11 iter: 139 reconn_loss: 9276.1572265625 kl_loss: 1735.459716796875\n",
      "epoch: 11 iter: 140 reconn_loss: 9623.431640625 kl_loss: 1741.96337890625\n",
      "epoch: 11 iter: 141 reconn_loss: 9357.701171875 kl_loss: 1704.0081787109375\n",
      "epoch: 11 iter: 142 reconn_loss: 9365.4140625 kl_loss: 1715.9400634765625\n",
      "epoch: 11 iter: 143 reconn_loss: 9037.9375 kl_loss: 1747.1708984375\n",
      "epoch: 11 iter: 144 reconn_loss: 9286.90234375 kl_loss: 1759.111328125\n",
      "epoch: 11 iter: 145 reconn_loss: 9253.263671875 kl_loss: 1809.82373046875\n",
      "epoch: 11 iter: 146 reconn_loss: 9585.6005859375 kl_loss: 1765.3448486328125\n",
      "epoch: 11 iter: 147 reconn_loss: 9543.7626953125 kl_loss: 1757.296142578125\n",
      "epoch: 11 iter: 148 reconn_loss: 9611.564453125 kl_loss: 1828.5538330078125\n",
      "epoch: 11 iter: 149 reconn_loss: 9413.3818359375 kl_loss: 1710.1551513671875\n",
      "epoch: 11 iter: 150 reconn_loss: 9068.7568359375 kl_loss: 1784.8580322265625\n",
      "epoch: 11 iter: 151 reconn_loss: 9190.5478515625 kl_loss: 1743.8114013671875\n",
      "epoch: 11 iter: 152 reconn_loss: 9534.1044921875 kl_loss: 1762.4215087890625\n",
      "epoch: 11 iter: 153 reconn_loss: 9311.7734375 kl_loss: 1708.6043701171875\n",
      "epoch: 11 iter: 154 reconn_loss: 9251.3017578125 kl_loss: 1756.1563720703125\n",
      "epoch: 11 iter: 155 reconn_loss: 9576.08984375 kl_loss: 1798.0439453125\n",
      "epoch: 11 iter: 156 reconn_loss: 9541.7314453125 kl_loss: 1752.748046875\n",
      "epoch: 11 iter: 157 reconn_loss: 9702.2421875 kl_loss: 1801.3489990234375\n",
      "epoch: 11 iter: 158 reconn_loss: 9256.23828125 kl_loss: 1745.2130126953125\n",
      "epoch: 11 iter: 159 reconn_loss: 9317.5703125 kl_loss: 1743.8133544921875\n",
      "epoch: 11 iter: 160 reconn_loss: 9295.3232421875 kl_loss: 1717.015625\n",
      "epoch: 11 iter: 161 reconn_loss: 9380.1826171875 kl_loss: 1787.817626953125\n",
      "epoch: 11 iter: 162 reconn_loss: 9352.9052734375 kl_loss: 1754.750244140625\n",
      "epoch: 11 iter: 163 reconn_loss: 9366.61328125 kl_loss: 1739.6890869140625\n",
      "epoch: 11 iter: 164 reconn_loss: 9522.509765625 kl_loss: 1710.709228515625\n",
      "epoch: 11 iter: 165 reconn_loss: 9394.236328125 kl_loss: 1780.5838623046875\n",
      "epoch: 11 iter: 166 reconn_loss: 9227.5068359375 kl_loss: 1733.7735595703125\n",
      "epoch: 11 iter: 167 reconn_loss: 9233.9423828125 kl_loss: 1766.212890625\n",
      "epoch: 11 iter: 168 reconn_loss: 9388.62890625 kl_loss: 1741.6876220703125\n",
      "epoch: 11 iter: 169 reconn_loss: 9585.8974609375 kl_loss: 1750.7293701171875\n",
      "epoch: 11 iter: 170 reconn_loss: 9807.7099609375 kl_loss: 1739.3277587890625\n",
      "epoch: 11 iter: 171 reconn_loss: 9600.10546875 kl_loss: 1814.2435302734375\n",
      "epoch: 11 iter: 172 reconn_loss: 9529.3701171875 kl_loss: 1744.20166015625\n",
      "epoch: 11 iter: 173 reconn_loss: 9406.947265625 kl_loss: 1698.85546875\n",
      "epoch: 11 iter: 174 reconn_loss: 9426.046875 kl_loss: 1699.842529296875\n",
      "epoch: 11 iter: 175 reconn_loss: 9724.875 kl_loss: 1792.267333984375\n",
      "epoch: 11 iter: 176 reconn_loss: 9411.181640625 kl_loss: 1695.673828125\n",
      "epoch: 11 iter: 177 reconn_loss: 9533.6416015625 kl_loss: 1707.274169921875\n",
      "epoch: 11 iter: 178 reconn_loss: 9685.28125 kl_loss: 1732.4854736328125\n",
      "epoch: 11 iter: 179 reconn_loss: 9691.3857421875 kl_loss: 1736.587646484375\n",
      "epoch: 11 iter: 180 reconn_loss: 9409.806640625 kl_loss: 1694.1727294921875\n",
      "epoch: 11 iter: 181 reconn_loss: 9414.458984375 kl_loss: 1738.2381591796875\n",
      "epoch: 11 iter: 182 reconn_loss: 9364.685546875 kl_loss: 1734.716796875\n",
      "epoch: 11 iter: 183 reconn_loss: 9313.533203125 kl_loss: 1732.8446044921875\n",
      "epoch: 11 iter: 184 reconn_loss: 9201.2724609375 kl_loss: 1679.5404052734375\n",
      "epoch: 11 iter: 185 reconn_loss: 9845.369140625 kl_loss: 1829.72021484375\n",
      "epoch: 11 iter: 186 reconn_loss: 9596.0322265625 kl_loss: 1715.508544921875\n",
      "epoch: 11 iter: 187 reconn_loss: 9339.2421875 kl_loss: 1688.8756103515625\n",
      "epoch: 11 iter: 188 reconn_loss: 9501.53515625 kl_loss: 1758.524169921875\n",
      "epoch: 11 iter: 189 reconn_loss: 9464.669921875 kl_loss: 1804.078125\n",
      "epoch: 11 iter: 190 reconn_loss: 9231.38671875 kl_loss: 1781.6785888671875\n",
      "epoch: 11 iter: 191 reconn_loss: 9503.228515625 kl_loss: 1739.6737060546875\n",
      "epoch: 11 iter: 192 reconn_loss: 9269.1142578125 kl_loss: 1694.222900390625\n",
      "epoch: 11 iter: 193 reconn_loss: 9326.0654296875 kl_loss: 1803.2945556640625\n",
      "epoch: 11 iter: 194 reconn_loss: 9468.6962890625 kl_loss: 1755.1605224609375\n",
      "epoch: 11 iter: 195 reconn_loss: 9639.2392578125 kl_loss: 1854.263671875\n",
      "epoch: 11 iter: 196 reconn_loss: 9474.0634765625 kl_loss: 1745.9901123046875\n",
      "epoch: 11 iter: 197 reconn_loss: 9354.845703125 kl_loss: 1783.836669921875\n",
      "epoch: 11 iter: 198 reconn_loss: 9462.521484375 kl_loss: 1754.2254638671875\n",
      "epoch: 11 iter: 199 reconn_loss: 9311.283203125 kl_loss: 1745.8153076171875\n",
      "epoch: 11 iter: 200 reconn_loss: 9370.8974609375 kl_loss: 1766.8065185546875\n",
      "epoch: 11 iter: 201 reconn_loss: 9674.921875 kl_loss: 1747.5908203125\n",
      "epoch: 11 iter: 202 reconn_loss: 9464.611328125 kl_loss: 1732.014892578125\n",
      "epoch: 11 iter: 203 reconn_loss: 9784.26953125 kl_loss: 1754.17138671875\n",
      "epoch: 11 iter: 204 reconn_loss: 9429.0439453125 kl_loss: 1768.2744140625\n",
      "epoch: 11 iter: 205 reconn_loss: 9321.8720703125 kl_loss: 1717.598388671875\n",
      "epoch: 11 iter: 206 reconn_loss: 9100.9248046875 kl_loss: 1733.1063232421875\n",
      "epoch: 11 iter: 207 reconn_loss: 9780.80859375 kl_loss: 1789.09912109375\n",
      "epoch: 11 iter: 208 reconn_loss: 9324.642578125 kl_loss: 1753.597900390625\n",
      "epoch: 11 iter: 209 reconn_loss: 9419.369140625 kl_loss: 1742.92431640625\n",
      "epoch: 11 iter: 210 reconn_loss: 9399.25390625 kl_loss: 1728.1422119140625\n",
      "epoch: 11 iter: 211 reconn_loss: 9265.021484375 kl_loss: 1718.9747314453125\n",
      "epoch: 11 iter: 212 reconn_loss: 9677.5859375 kl_loss: 1769.712158203125\n",
      "epoch: 11 iter: 213 reconn_loss: 9501.0 kl_loss: 1745.52001953125\n",
      "epoch: 11 iter: 214 reconn_loss: 9520.880859375 kl_loss: 1762.2333984375\n",
      "epoch: 11 iter: 215 reconn_loss: 9273.205078125 kl_loss: 1718.21630859375\n",
      "epoch: 11 iter: 216 reconn_loss: 9372.59765625 kl_loss: 1695.6546630859375\n",
      "epoch: 11 iter: 217 reconn_loss: 9496.94140625 kl_loss: 1726.7470703125\n",
      "epoch: 11 iter: 218 reconn_loss: 9327.515625 kl_loss: 1805.8892822265625\n",
      "epoch: 11 iter: 219 reconn_loss: 9663.1484375 kl_loss: 1712.691162109375\n",
      "epoch: 11 iter: 220 reconn_loss: 9365.201171875 kl_loss: 1751.5118408203125\n",
      "epoch: 11 iter: 221 reconn_loss: 9696.1337890625 kl_loss: 1755.3507080078125\n",
      "epoch: 11 iter: 222 reconn_loss: 6175.865234375 kl_loss: 1175.036865234375\n",
      "0.weight tensor(67.0903) tensor(-66.1762)\n",
      "0.bias tensor(34.9535) tensor(-34.1625)\n",
      "2.weight tensor(10.3114) tensor(-30.5024)\n",
      "2.bias tensor(9.3603) tensor(-12.5053)\n",
      "4.weight tensor(7.6548) tensor(-10.1651)\n",
      "4.bias tensor(4.1142) tensor(-4.8659)\n",
      "epoch: 12 iter: 0 reconn_loss: 9247.541015625 kl_loss: 1794.9755859375\n",
      "epoch: 12 iter: 1 reconn_loss: 9600.955078125 kl_loss: 1760.2379150390625\n",
      "epoch: 12 iter: 2 reconn_loss: 9794.0146484375 kl_loss: 1725.101806640625\n",
      "epoch: 12 iter: 3 reconn_loss: 9229.3974609375 kl_loss: 1818.0950927734375\n",
      "epoch: 12 iter: 4 reconn_loss: 9455.0185546875 kl_loss: 1770.717529296875\n",
      "epoch: 12 iter: 5 reconn_loss: 9571.0703125 kl_loss: 1842.3896484375\n",
      "epoch: 12 iter: 6 reconn_loss: 9736.5068359375 kl_loss: 1789.697509765625\n",
      "epoch: 12 iter: 7 reconn_loss: 9310.1875 kl_loss: 1788.25537109375\n",
      "epoch: 12 iter: 8 reconn_loss: 9370.4375 kl_loss: 1771.2774658203125\n",
      "epoch: 12 iter: 9 reconn_loss: 9556.734375 kl_loss: 1789.19580078125\n",
      "epoch: 12 iter: 10 reconn_loss: 9427.322265625 kl_loss: 1791.9761962890625\n",
      "epoch: 12 iter: 11 reconn_loss: 8986.8310546875 kl_loss: 1778.3707275390625\n",
      "epoch: 12 iter: 12 reconn_loss: 9624.66015625 kl_loss: 1748.604736328125\n",
      "epoch: 12 iter: 13 reconn_loss: 9304.78125 kl_loss: 1742.2828369140625\n",
      "epoch: 12 iter: 14 reconn_loss: 9379.228515625 kl_loss: 1713.4561767578125\n",
      "epoch: 12 iter: 15 reconn_loss: 9830.390625 kl_loss: 1745.53271484375\n",
      "epoch: 12 iter: 16 reconn_loss: 9482.544921875 kl_loss: 1743.7506103515625\n",
      "epoch: 12 iter: 17 reconn_loss: 9520.826171875 kl_loss: 1725.9091796875\n",
      "epoch: 12 iter: 18 reconn_loss: 9513.8701171875 kl_loss: 1689.221435546875\n",
      "epoch: 12 iter: 19 reconn_loss: 9525.9560546875 kl_loss: 1802.8006591796875\n",
      "epoch: 12 iter: 20 reconn_loss: 9115.1962890625 kl_loss: 1755.396240234375\n",
      "epoch: 12 iter: 21 reconn_loss: 9056.482421875 kl_loss: 1722.6595458984375\n",
      "epoch: 12 iter: 22 reconn_loss: 9295.2236328125 kl_loss: 1731.9298095703125\n",
      "epoch: 12 iter: 23 reconn_loss: 9318.6640625 kl_loss: 1767.76171875\n",
      "epoch: 12 iter: 24 reconn_loss: 9574.763671875 kl_loss: 1764.2950439453125\n",
      "epoch: 12 iter: 25 reconn_loss: 9220.2822265625 kl_loss: 1796.8946533203125\n",
      "epoch: 12 iter: 26 reconn_loss: 9378.8671875 kl_loss: 1781.989990234375\n",
      "epoch: 12 iter: 27 reconn_loss: 9146.72265625 kl_loss: 1739.0625\n",
      "epoch: 12 iter: 28 reconn_loss: 9452.6298828125 kl_loss: 1760.0675048828125\n",
      "epoch: 12 iter: 29 reconn_loss: 9341.1318359375 kl_loss: 1781.0838623046875\n",
      "epoch: 12 iter: 30 reconn_loss: 9368.3310546875 kl_loss: 1766.219482421875\n",
      "epoch: 12 iter: 31 reconn_loss: 9410.8251953125 kl_loss: 1808.789306640625\n",
      "epoch: 12 iter: 32 reconn_loss: 9316.9384765625 kl_loss: 1765.6768798828125\n",
      "epoch: 12 iter: 33 reconn_loss: 9448.0 kl_loss: 1795.3055419921875\n",
      "epoch: 12 iter: 34 reconn_loss: 9652.884765625 kl_loss: 1806.811767578125\n",
      "epoch: 12 iter: 35 reconn_loss: 9265.5283203125 kl_loss: 1774.68115234375\n",
      "epoch: 12 iter: 36 reconn_loss: 9334.5732421875 kl_loss: 1773.7059326171875\n",
      "epoch: 12 iter: 37 reconn_loss: 9300.86328125 kl_loss: 1800.3809814453125\n",
      "epoch: 12 iter: 38 reconn_loss: 9363.55078125 kl_loss: 1771.201904296875\n",
      "epoch: 12 iter: 39 reconn_loss: 9417.138671875 kl_loss: 1770.47509765625\n",
      "epoch: 12 iter: 40 reconn_loss: 9358.490234375 kl_loss: 1786.109375\n",
      "epoch: 12 iter: 41 reconn_loss: 9741.8251953125 kl_loss: 1766.9854736328125\n",
      "epoch: 12 iter: 42 reconn_loss: 9246.240234375 kl_loss: 1795.7996826171875\n",
      "epoch: 12 iter: 43 reconn_loss: 9630.88671875 kl_loss: 1744.7135009765625\n",
      "epoch: 12 iter: 44 reconn_loss: 9337.9794921875 kl_loss: 1766.1287841796875\n",
      "epoch: 12 iter: 45 reconn_loss: 9319.00390625 kl_loss: 1741.62353515625\n",
      "epoch: 12 iter: 46 reconn_loss: 9446.8330078125 kl_loss: 1755.822998046875\n",
      "epoch: 12 iter: 47 reconn_loss: 9504.1455078125 kl_loss: 1673.7196044921875\n",
      "epoch: 12 iter: 48 reconn_loss: 9607.873046875 kl_loss: 1787.3675537109375\n",
      "epoch: 12 iter: 49 reconn_loss: 9623.9931640625 kl_loss: 1720.6968994140625\n",
      "epoch: 12 iter: 50 reconn_loss: 9463.625 kl_loss: 1759.5277099609375\n",
      "epoch: 12 iter: 51 reconn_loss: 9299.15234375 kl_loss: 1750.661865234375\n",
      "epoch: 12 iter: 52 reconn_loss: 9369.0986328125 kl_loss: 1741.173828125\n",
      "epoch: 12 iter: 53 reconn_loss: 9527.892578125 kl_loss: 1804.38427734375\n",
      "epoch: 12 iter: 54 reconn_loss: 9686.7724609375 kl_loss: 1781.669189453125\n",
      "epoch: 12 iter: 55 reconn_loss: 9696.9052734375 kl_loss: 1786.705078125\n",
      "epoch: 12 iter: 56 reconn_loss: 9560.76171875 kl_loss: 1750.0123291015625\n",
      "epoch: 12 iter: 57 reconn_loss: 9165.865234375 kl_loss: 1762.926025390625\n",
      "epoch: 12 iter: 58 reconn_loss: 9270.384765625 kl_loss: 1748.4019775390625\n",
      "epoch: 12 iter: 59 reconn_loss: 9396.408203125 kl_loss: 1706.0169677734375\n",
      "epoch: 12 iter: 60 reconn_loss: 9384.0380859375 kl_loss: 1832.8350830078125\n",
      "epoch: 12 iter: 61 reconn_loss: 9044.326171875 kl_loss: 1737.796875\n",
      "epoch: 12 iter: 62 reconn_loss: 9604.8154296875 kl_loss: 1777.972412109375\n",
      "epoch: 12 iter: 63 reconn_loss: 9327.98828125 kl_loss: 1769.3682861328125\n",
      "epoch: 12 iter: 64 reconn_loss: 9467.712890625 kl_loss: 1774.842041015625\n",
      "epoch: 12 iter: 65 reconn_loss: 9464.5224609375 kl_loss: 1776.1900634765625\n",
      "epoch: 12 iter: 66 reconn_loss: 9330.9892578125 kl_loss: 1750.9619140625\n",
      "epoch: 12 iter: 67 reconn_loss: 9314.4501953125 kl_loss: 1785.5091552734375\n",
      "epoch: 12 iter: 68 reconn_loss: 9462.525390625 kl_loss: 1786.5086669921875\n",
      "epoch: 12 iter: 69 reconn_loss: 9052.7109375 kl_loss: 1753.4263916015625\n",
      "epoch: 12 iter: 70 reconn_loss: 9040.67578125 kl_loss: 1752.8709716796875\n",
      "epoch: 12 iter: 71 reconn_loss: 9359.76953125 kl_loss: 1755.32080078125\n",
      "epoch: 12 iter: 72 reconn_loss: 9099.599609375 kl_loss: 1732.577880859375\n",
      "epoch: 12 iter: 73 reconn_loss: 9283.9306640625 kl_loss: 1780.4754638671875\n",
      "epoch: 12 iter: 74 reconn_loss: 9416.8212890625 kl_loss: 1760.75390625\n",
      "epoch: 12 iter: 75 reconn_loss: 9232.3232421875 kl_loss: 1766.8062744140625\n",
      "epoch: 12 iter: 76 reconn_loss: 9176.998046875 kl_loss: 1789.49658203125\n",
      "epoch: 12 iter: 77 reconn_loss: 9667.44140625 kl_loss: 1808.79150390625\n",
      "epoch: 12 iter: 78 reconn_loss: 9368.4599609375 kl_loss: 1773.896240234375\n",
      "epoch: 12 iter: 79 reconn_loss: 9441.3125 kl_loss: 1747.319091796875\n",
      "epoch: 12 iter: 80 reconn_loss: 9637.1474609375 kl_loss: 1828.8289794921875\n",
      "epoch: 12 iter: 81 reconn_loss: 9510.1552734375 kl_loss: 1739.8052978515625\n",
      "epoch: 12 iter: 82 reconn_loss: 9391.880859375 kl_loss: 1744.7100830078125\n",
      "epoch: 12 iter: 83 reconn_loss: 9557.400390625 kl_loss: 1717.371337890625\n",
      "epoch: 12 iter: 84 reconn_loss: 9322.91015625 kl_loss: 1726.81396484375\n",
      "epoch: 12 iter: 85 reconn_loss: 9222.0595703125 kl_loss: 1746.598876953125\n",
      "epoch: 12 iter: 86 reconn_loss: 9320.607421875 kl_loss: 1774.0169677734375\n",
      "epoch: 12 iter: 87 reconn_loss: 9015.9677734375 kl_loss: 1740.077392578125\n",
      "epoch: 12 iter: 88 reconn_loss: 9101.5810546875 kl_loss: 1746.624267578125\n",
      "epoch: 12 iter: 89 reconn_loss: 9341.26953125 kl_loss: 1736.7572021484375\n",
      "epoch: 12 iter: 90 reconn_loss: 9346.658203125 kl_loss: 1713.687744140625\n",
      "epoch: 12 iter: 91 reconn_loss: 9267.0634765625 kl_loss: 1733.372802734375\n",
      "epoch: 12 iter: 92 reconn_loss: 9418.8134765625 kl_loss: 1716.858642578125\n",
      "epoch: 12 iter: 93 reconn_loss: 9334.755859375 kl_loss: 1711.949462890625\n",
      "epoch: 12 iter: 94 reconn_loss: 9661.88671875 kl_loss: 1726.9324951171875\n",
      "epoch: 12 iter: 95 reconn_loss: 9389.150390625 kl_loss: 1769.17041015625\n",
      "epoch: 12 iter: 96 reconn_loss: 9280.4833984375 kl_loss: 1742.322998046875\n",
      "epoch: 12 iter: 97 reconn_loss: 9278.255859375 kl_loss: 1766.6962890625\n",
      "epoch: 12 iter: 98 reconn_loss: 9338.837890625 kl_loss: 1788.9986572265625\n",
      "epoch: 12 iter: 99 reconn_loss: 9286.67578125 kl_loss: 1765.815673828125\n",
      "epoch: 12 iter: 100 reconn_loss: 9100.4873046875 kl_loss: 1756.408935546875\n",
      "epoch: 12 iter: 101 reconn_loss: 9316.5048828125 kl_loss: 1825.048095703125\n",
      "epoch: 12 iter: 102 reconn_loss: 9552.1640625 kl_loss: 1760.516845703125\n",
      "epoch: 12 iter: 103 reconn_loss: 9349.7080078125 kl_loss: 1713.1641845703125\n",
      "epoch: 12 iter: 104 reconn_loss: 9094.5947265625 kl_loss: 1759.9449462890625\n",
      "epoch: 12 iter: 105 reconn_loss: 9602.9423828125 kl_loss: 1782.9105224609375\n",
      "epoch: 12 iter: 106 reconn_loss: 9764.8369140625 kl_loss: 1721.0902099609375\n",
      "epoch: 12 iter: 107 reconn_loss: 9583.5849609375 kl_loss: 1757.1165771484375\n",
      "epoch: 12 iter: 108 reconn_loss: 9335.66015625 kl_loss: 1796.21533203125\n",
      "epoch: 12 iter: 109 reconn_loss: 9099.48828125 kl_loss: 1775.16650390625\n",
      "epoch: 12 iter: 110 reconn_loss: 9300.16796875 kl_loss: 1739.1846923828125\n",
      "epoch: 12 iter: 111 reconn_loss: 9213.7275390625 kl_loss: 1763.751220703125\n",
      "epoch: 12 iter: 112 reconn_loss: 9002.09765625 kl_loss: 1761.0626220703125\n",
      "epoch: 12 iter: 113 reconn_loss: 9491.4921875 kl_loss: 1766.1220703125\n",
      "epoch: 12 iter: 114 reconn_loss: 9129.587890625 kl_loss: 1781.4849853515625\n",
      "epoch: 12 iter: 115 reconn_loss: 9305.1416015625 kl_loss: 1775.584716796875\n",
      "epoch: 12 iter: 116 reconn_loss: 9418.158203125 kl_loss: 1772.23974609375\n",
      "epoch: 12 iter: 117 reconn_loss: 9461.8115234375 kl_loss: 1748.640869140625\n",
      "epoch: 12 iter: 118 reconn_loss: 9036.8359375 kl_loss: 1752.812255859375\n",
      "epoch: 12 iter: 119 reconn_loss: 9450.9150390625 kl_loss: 1768.3912353515625\n",
      "epoch: 12 iter: 120 reconn_loss: 9468.7431640625 kl_loss: 1802.8802490234375\n",
      "epoch: 12 iter: 121 reconn_loss: 9512.154296875 kl_loss: 1752.287841796875\n",
      "epoch: 12 iter: 122 reconn_loss: 9302.298828125 kl_loss: 1804.2607421875\n",
      "epoch: 12 iter: 123 reconn_loss: 9791.5087890625 kl_loss: 1794.725830078125\n",
      "epoch: 12 iter: 124 reconn_loss: 9663.306640625 kl_loss: 1827.04931640625\n",
      "epoch: 12 iter: 125 reconn_loss: 9358.8955078125 kl_loss: 1752.271240234375\n",
      "epoch: 12 iter: 126 reconn_loss: 9443.4833984375 kl_loss: 1724.570068359375\n",
      "epoch: 12 iter: 127 reconn_loss: 9535.34375 kl_loss: 1709.130859375\n",
      "epoch: 12 iter: 128 reconn_loss: 9407.982421875 kl_loss: 1707.4508056640625\n",
      "epoch: 12 iter: 129 reconn_loss: 9245.4404296875 kl_loss: 1744.0576171875\n",
      "epoch: 12 iter: 130 reconn_loss: 9356.7099609375 kl_loss: 1746.4505615234375\n",
      "epoch: 12 iter: 131 reconn_loss: 9452.5126953125 kl_loss: 1758.0616455078125\n",
      "epoch: 12 iter: 132 reconn_loss: 9432.7041015625 kl_loss: 1764.448486328125\n",
      "epoch: 12 iter: 133 reconn_loss: 9092.841796875 kl_loss: 1765.1221923828125\n",
      "epoch: 12 iter: 134 reconn_loss: 9148.439453125 kl_loss: 1790.689208984375\n",
      "epoch: 12 iter: 135 reconn_loss: 9493.6845703125 kl_loss: 1739.2730712890625\n",
      "epoch: 12 iter: 136 reconn_loss: 8964.392578125 kl_loss: 1757.48486328125\n",
      "epoch: 12 iter: 137 reconn_loss: 9508.009765625 kl_loss: 1767.88623046875\n",
      "epoch: 12 iter: 138 reconn_loss: 9403.73828125 kl_loss: 1753.8055419921875\n",
      "epoch: 12 iter: 139 reconn_loss: 9455.2568359375 kl_loss: 1741.4990234375\n",
      "epoch: 12 iter: 140 reconn_loss: 8971.580078125 kl_loss: 1728.77197265625\n",
      "epoch: 12 iter: 141 reconn_loss: 9406.896484375 kl_loss: 1732.8076171875\n",
      "epoch: 12 iter: 142 reconn_loss: 9423.767578125 kl_loss: 1765.26171875\n",
      "epoch: 12 iter: 143 reconn_loss: 9443.2529296875 kl_loss: 1770.7320556640625\n",
      "epoch: 12 iter: 144 reconn_loss: 9106.6083984375 kl_loss: 1780.734619140625\n",
      "epoch: 12 iter: 145 reconn_loss: 9517.3271484375 kl_loss: 1785.109619140625\n",
      "epoch: 12 iter: 146 reconn_loss: 9492.53515625 kl_loss: 1789.32080078125\n",
      "epoch: 12 iter: 147 reconn_loss: 9418.66796875 kl_loss: 1749.096923828125\n",
      "epoch: 12 iter: 148 reconn_loss: 9656.41015625 kl_loss: 1771.1575927734375\n",
      "epoch: 12 iter: 149 reconn_loss: 9649.0908203125 kl_loss: 1770.377197265625\n",
      "epoch: 12 iter: 150 reconn_loss: 9267.26171875 kl_loss: 1745.610107421875\n",
      "epoch: 12 iter: 151 reconn_loss: 9148.1826171875 kl_loss: 1676.8153076171875\n",
      "epoch: 12 iter: 152 reconn_loss: 9645.064453125 kl_loss: 1752.945556640625\n",
      "epoch: 12 iter: 153 reconn_loss: 9156.2041015625 kl_loss: 1763.619873046875\n",
      "epoch: 12 iter: 154 reconn_loss: 9492.626953125 kl_loss: 1764.6712646484375\n",
      "epoch: 12 iter: 155 reconn_loss: 9122.3193359375 kl_loss: 1786.3214111328125\n",
      "epoch: 12 iter: 156 reconn_loss: 9339.265625 kl_loss: 1777.5489501953125\n",
      "epoch: 12 iter: 157 reconn_loss: 9455.736328125 kl_loss: 1798.4801025390625\n",
      "epoch: 12 iter: 158 reconn_loss: 9425.3515625 kl_loss: 1792.58837890625\n",
      "epoch: 12 iter: 159 reconn_loss: 9644.1943359375 kl_loss: 1762.675048828125\n",
      "epoch: 12 iter: 160 reconn_loss: 9646.73828125 kl_loss: 1730.44384765625\n",
      "epoch: 12 iter: 161 reconn_loss: 9107.150390625 kl_loss: 1738.9818115234375\n",
      "epoch: 12 iter: 162 reconn_loss: 9681.7021484375 kl_loss: 1726.9139404296875\n",
      "epoch: 12 iter: 163 reconn_loss: 9586.60546875 kl_loss: 1718.01318359375\n",
      "epoch: 12 iter: 164 reconn_loss: 9368.87109375 kl_loss: 1748.345703125\n",
      "epoch: 12 iter: 165 reconn_loss: 9677.4052734375 kl_loss: 1762.1883544921875\n",
      "epoch: 12 iter: 166 reconn_loss: 9180.0380859375 kl_loss: 1808.9163818359375\n",
      "epoch: 12 iter: 167 reconn_loss: 9344.087890625 kl_loss: 1813.23828125\n",
      "epoch: 12 iter: 168 reconn_loss: 9446.244140625 kl_loss: 1793.4849853515625\n",
      "epoch: 12 iter: 169 reconn_loss: 9509.853515625 kl_loss: 1794.22216796875\n",
      "epoch: 12 iter: 170 reconn_loss: 9428.0712890625 kl_loss: 1772.22509765625\n",
      "epoch: 12 iter: 171 reconn_loss: 9259.732421875 kl_loss: 1771.9124755859375\n",
      "epoch: 12 iter: 172 reconn_loss: 9307.1162109375 kl_loss: 1787.9805908203125\n",
      "epoch: 12 iter: 173 reconn_loss: 9234.8427734375 kl_loss: 1748.92041015625\n",
      "epoch: 12 iter: 174 reconn_loss: 9244.56640625 kl_loss: 1810.373779296875\n",
      "epoch: 12 iter: 175 reconn_loss: 9380.87890625 kl_loss: 1827.406005859375\n",
      "epoch: 12 iter: 176 reconn_loss: 9261.298828125 kl_loss: 1764.98388671875\n",
      "epoch: 12 iter: 177 reconn_loss: 9292.1796875 kl_loss: 1785.3431396484375\n",
      "epoch: 12 iter: 178 reconn_loss: 9380.892578125 kl_loss: 1782.24072265625\n",
      "epoch: 12 iter: 179 reconn_loss: 9207.71875 kl_loss: 1796.6534423828125\n",
      "epoch: 12 iter: 180 reconn_loss: 9420.283203125 kl_loss: 1812.0802001953125\n",
      "epoch: 12 iter: 181 reconn_loss: 9076.0888671875 kl_loss: 1755.7376708984375\n",
      "epoch: 12 iter: 182 reconn_loss: 9380.2783203125 kl_loss: 1727.6004638671875\n",
      "epoch: 12 iter: 183 reconn_loss: 8897.7734375 kl_loss: 1690.602294921875\n",
      "epoch: 12 iter: 184 reconn_loss: 9556.0419921875 kl_loss: 1774.991943359375\n",
      "epoch: 12 iter: 185 reconn_loss: 9099.2001953125 kl_loss: 1803.306396484375\n",
      "epoch: 12 iter: 186 reconn_loss: 9656.1318359375 kl_loss: 1724.962158203125\n",
      "epoch: 12 iter: 187 reconn_loss: 9315.4072265625 kl_loss: 1767.003662109375\n",
      "epoch: 12 iter: 188 reconn_loss: 9628.765625 kl_loss: 1763.8768310546875\n",
      "epoch: 12 iter: 189 reconn_loss: 9256.400390625 kl_loss: 1754.9801025390625\n",
      "epoch: 12 iter: 190 reconn_loss: 8934.26171875 kl_loss: 1743.27587890625\n",
      "epoch: 12 iter: 191 reconn_loss: 9283.369140625 kl_loss: 1838.904296875\n",
      "epoch: 12 iter: 192 reconn_loss: 9609.7060546875 kl_loss: 1770.1060791015625\n",
      "epoch: 12 iter: 193 reconn_loss: 9268.521484375 kl_loss: 1780.2421875\n",
      "epoch: 12 iter: 194 reconn_loss: 9207.5419921875 kl_loss: 1756.30078125\n",
      "epoch: 12 iter: 195 reconn_loss: 9519.2529296875 kl_loss: 1726.2548828125\n",
      "epoch: 12 iter: 196 reconn_loss: 9096.462890625 kl_loss: 1780.031982421875\n",
      "epoch: 12 iter: 197 reconn_loss: 9114.0478515625 kl_loss: 1740.993896484375\n",
      "epoch: 12 iter: 198 reconn_loss: 9218.689453125 kl_loss: 1736.89404296875\n",
      "epoch: 12 iter: 199 reconn_loss: 9327.3330078125 kl_loss: 1746.78271484375\n",
      "epoch: 12 iter: 200 reconn_loss: 9500.9912109375 kl_loss: 1860.568115234375\n",
      "epoch: 12 iter: 201 reconn_loss: 9121.947265625 kl_loss: 1799.7642822265625\n",
      "epoch: 12 iter: 202 reconn_loss: 9042.1396484375 kl_loss: 1797.2835693359375\n",
      "epoch: 12 iter: 203 reconn_loss: 9231.392578125 kl_loss: 1788.6240234375\n",
      "epoch: 12 iter: 204 reconn_loss: 9493.275390625 kl_loss: 1796.5283203125\n",
      "epoch: 12 iter: 205 reconn_loss: 9445.833984375 kl_loss: 1792.766845703125\n",
      "epoch: 12 iter: 206 reconn_loss: 9272.083984375 kl_loss: 1828.36181640625\n",
      "epoch: 12 iter: 207 reconn_loss: 9325.142578125 kl_loss: 1842.7264404296875\n",
      "epoch: 12 iter: 208 reconn_loss: 8936.078125 kl_loss: 1775.3126220703125\n",
      "epoch: 12 iter: 209 reconn_loss: 9448.3056640625 kl_loss: 1799.3475341796875\n",
      "epoch: 12 iter: 210 reconn_loss: 9063.333984375 kl_loss: 1803.166015625\n",
      "epoch: 12 iter: 211 reconn_loss: 9144.884765625 kl_loss: 1803.9072265625\n",
      "epoch: 12 iter: 212 reconn_loss: 9073.04296875 kl_loss: 1764.1910400390625\n",
      "epoch: 12 iter: 213 reconn_loss: 9016.7529296875 kl_loss: 1753.0316162109375\n",
      "epoch: 12 iter: 214 reconn_loss: 9561.8427734375 kl_loss: 1799.0394287109375\n",
      "epoch: 12 iter: 215 reconn_loss: 9411.7568359375 kl_loss: 1768.4791259765625\n",
      "epoch: 12 iter: 216 reconn_loss: 9519.80859375 kl_loss: 1752.5400390625\n",
      "epoch: 12 iter: 217 reconn_loss: 9365.423828125 kl_loss: 1777.150634765625\n",
      "epoch: 12 iter: 218 reconn_loss: 8975.49609375 kl_loss: 1737.720947265625\n",
      "epoch: 12 iter: 219 reconn_loss: 9448.1376953125 kl_loss: 1750.6673583984375\n",
      "epoch: 12 iter: 220 reconn_loss: 9162.244140625 kl_loss: 1689.04150390625\n",
      "epoch: 12 iter: 221 reconn_loss: 9281.095703125 kl_loss: 1720.404296875\n",
      "epoch: 12 iter: 222 reconn_loss: 6075.36865234375 kl_loss: 1180.8330078125\n",
      "0.weight tensor(66.0670) tensor(-74.0873)\n",
      "0.bias tensor(28.4367) tensor(-51.0478)\n",
      "2.weight tensor(14.1353) tensor(-17.1683)\n",
      "2.bias tensor(8.7558) tensor(-13.9279)\n",
      "4.weight tensor(8.9746) tensor(-12.9982)\n",
      "4.bias tensor(3.1625) tensor(-6.7953)\n",
      "epoch: 13 iter: 0 reconn_loss: 9473.591796875 kl_loss: 1747.93408203125\n",
      "epoch: 13 iter: 1 reconn_loss: 9440.375 kl_loss: 1782.53955078125\n",
      "epoch: 13 iter: 2 reconn_loss: 9482.1015625 kl_loss: 1736.8084716796875\n",
      "epoch: 13 iter: 3 reconn_loss: 9481.75390625 kl_loss: 1800.7142333984375\n",
      "epoch: 13 iter: 4 reconn_loss: 9732.0283203125 kl_loss: 1816.169677734375\n",
      "epoch: 13 iter: 5 reconn_loss: 9433.451171875 kl_loss: 1802.0654296875\n",
      "epoch: 13 iter: 6 reconn_loss: 9516.0810546875 kl_loss: 1809.3134765625\n",
      "epoch: 13 iter: 7 reconn_loss: 9453.078125 kl_loss: 1825.7138671875\n",
      "epoch: 13 iter: 8 reconn_loss: 9436.7939453125 kl_loss: 1797.379638671875\n",
      "epoch: 13 iter: 9 reconn_loss: 9442.3173828125 kl_loss: 1800.9012451171875\n",
      "epoch: 13 iter: 10 reconn_loss: 9256.740234375 kl_loss: 1766.314208984375\n",
      "epoch: 13 iter: 11 reconn_loss: 9265.466796875 kl_loss: 1776.90234375\n",
      "epoch: 13 iter: 12 reconn_loss: 9168.166015625 kl_loss: 1750.74267578125\n",
      "epoch: 13 iter: 13 reconn_loss: 9169.806640625 kl_loss: 1763.758544921875\n",
      "epoch: 13 iter: 14 reconn_loss: 9560.11328125 kl_loss: 1766.1661376953125\n",
      "epoch: 13 iter: 15 reconn_loss: 9260.6669921875 kl_loss: 1784.6888427734375\n",
      "epoch: 13 iter: 16 reconn_loss: 9074.7607421875 kl_loss: 1808.909423828125\n",
      "epoch: 13 iter: 17 reconn_loss: 9287.4658203125 kl_loss: 1751.590087890625\n",
      "epoch: 13 iter: 18 reconn_loss: 9278.29296875 kl_loss: 1789.6116943359375\n",
      "epoch: 13 iter: 19 reconn_loss: 9582.541015625 kl_loss: 1763.29248046875\n",
      "epoch: 13 iter: 20 reconn_loss: 9493.5703125 kl_loss: 1756.1378173828125\n",
      "epoch: 13 iter: 21 reconn_loss: 9362.142578125 kl_loss: 1790.9326171875\n",
      "epoch: 13 iter: 22 reconn_loss: 9209.111328125 kl_loss: 1777.927490234375\n",
      "epoch: 13 iter: 23 reconn_loss: 9688.8583984375 kl_loss: 1749.7655029296875\n",
      "epoch: 13 iter: 24 reconn_loss: 9087.021484375 kl_loss: 1806.260498046875\n",
      "epoch: 13 iter: 25 reconn_loss: 9157.9375 kl_loss: 1804.5489501953125\n",
      "epoch: 13 iter: 26 reconn_loss: 9293.7958984375 kl_loss: 1735.201416015625\n",
      "epoch: 13 iter: 27 reconn_loss: 9745.9619140625 kl_loss: 1834.6124267578125\n",
      "epoch: 13 iter: 28 reconn_loss: 9193.7578125 kl_loss: 1772.5787353515625\n",
      "epoch: 13 iter: 29 reconn_loss: 9168.91796875 kl_loss: 1767.824462890625\n",
      "epoch: 13 iter: 30 reconn_loss: 9040.337890625 kl_loss: 1741.423583984375\n",
      "epoch: 13 iter: 31 reconn_loss: 9149.3583984375 kl_loss: 1784.032958984375\n",
      "epoch: 13 iter: 32 reconn_loss: 9082.7451171875 kl_loss: 1754.51318359375\n",
      "epoch: 13 iter: 33 reconn_loss: 9038.1103515625 kl_loss: 1757.082275390625\n",
      "epoch: 13 iter: 34 reconn_loss: 9119.2763671875 kl_loss: 1790.188232421875\n",
      "epoch: 13 iter: 35 reconn_loss: 9288.66015625 kl_loss: 1786.73828125\n",
      "epoch: 13 iter: 36 reconn_loss: 9062.5009765625 kl_loss: 1777.0089111328125\n",
      "epoch: 13 iter: 37 reconn_loss: 9675.728515625 kl_loss: 1755.84619140625\n",
      "epoch: 13 iter: 38 reconn_loss: 9337.755859375 kl_loss: 1785.54443359375\n",
      "epoch: 13 iter: 39 reconn_loss: 9313.24609375 kl_loss: 1785.47509765625\n",
      "epoch: 13 iter: 40 reconn_loss: 9188.728515625 kl_loss: 1741.364501953125\n",
      "epoch: 13 iter: 41 reconn_loss: 9372.8427734375 kl_loss: 1763.926513671875\n",
      "epoch: 13 iter: 42 reconn_loss: 9279.4521484375 kl_loss: 1751.614013671875\n",
      "epoch: 13 iter: 43 reconn_loss: 9450.1220703125 kl_loss: 1756.6292724609375\n",
      "epoch: 13 iter: 44 reconn_loss: 9656.251953125 kl_loss: 1864.646240234375\n",
      "epoch: 13 iter: 45 reconn_loss: 9449.9775390625 kl_loss: 1791.697998046875\n",
      "epoch: 13 iter: 46 reconn_loss: 9215.396484375 kl_loss: 1769.749267578125\n",
      "epoch: 13 iter: 47 reconn_loss: 9317.11328125 kl_loss: 1803.00537109375\n",
      "epoch: 13 iter: 48 reconn_loss: 9384.947265625 kl_loss: 1783.8074951171875\n",
      "epoch: 13 iter: 49 reconn_loss: 9435.984375 kl_loss: 1803.680908203125\n",
      "epoch: 13 iter: 50 reconn_loss: 9064.4765625 kl_loss: 1750.82763671875\n",
      "epoch: 13 iter: 51 reconn_loss: 8936.0751953125 kl_loss: 1776.0780029296875\n",
      "epoch: 13 iter: 52 reconn_loss: 9468.4404296875 kl_loss: 1791.334716796875\n",
      "epoch: 13 iter: 53 reconn_loss: 9260.4326171875 kl_loss: 1780.6470947265625\n",
      "epoch: 13 iter: 54 reconn_loss: 9151.3203125 kl_loss: 1760.52490234375\n",
      "epoch: 13 iter: 55 reconn_loss: 9126.3330078125 kl_loss: 1793.513671875\n",
      "epoch: 13 iter: 56 reconn_loss: 9531.6884765625 kl_loss: 1790.0897216796875\n",
      "epoch: 13 iter: 57 reconn_loss: 9161.3955078125 kl_loss: 1799.5748291015625\n",
      "epoch: 13 iter: 58 reconn_loss: 9391.3994140625 kl_loss: 1787.95947265625\n",
      "epoch: 13 iter: 59 reconn_loss: 9387.029296875 kl_loss: 1759.46826171875\n",
      "epoch: 13 iter: 60 reconn_loss: 9351.26953125 kl_loss: 1733.9161376953125\n",
      "epoch: 13 iter: 61 reconn_loss: 8952.9775390625 kl_loss: 1723.2220458984375\n",
      "epoch: 13 iter: 62 reconn_loss: 9347.970703125 kl_loss: 1810.5784912109375\n",
      "epoch: 13 iter: 63 reconn_loss: 9569.201171875 kl_loss: 1781.4669189453125\n",
      "epoch: 13 iter: 64 reconn_loss: 9537.8466796875 kl_loss: 1777.4075927734375\n",
      "epoch: 13 iter: 65 reconn_loss: 9506.634765625 kl_loss: 1844.7337646484375\n",
      "epoch: 13 iter: 66 reconn_loss: 9309.7822265625 kl_loss: 1752.489013671875\n",
      "epoch: 13 iter: 67 reconn_loss: 9540.9189453125 kl_loss: 1783.7613525390625\n",
      "epoch: 13 iter: 68 reconn_loss: 9545.537109375 kl_loss: 1781.8631591796875\n",
      "epoch: 13 iter: 69 reconn_loss: 9270.3974609375 kl_loss: 1775.4066162109375\n",
      "epoch: 13 iter: 70 reconn_loss: 9283.73046875 kl_loss: 1779.2303466796875\n",
      "epoch: 13 iter: 71 reconn_loss: 9063.94921875 kl_loss: 1791.12939453125\n",
      "epoch: 13 iter: 72 reconn_loss: 9161.599609375 kl_loss: 1811.95751953125\n",
      "epoch: 13 iter: 73 reconn_loss: 9101.5087890625 kl_loss: 1730.393798828125\n",
      "epoch: 13 iter: 74 reconn_loss: 9152.51953125 kl_loss: 1813.031494140625\n",
      "epoch: 13 iter: 75 reconn_loss: 9310.810546875 kl_loss: 1753.755859375\n",
      "epoch: 13 iter: 76 reconn_loss: 9250.1005859375 kl_loss: 1727.980224609375\n",
      "epoch: 13 iter: 77 reconn_loss: 9706.5634765625 kl_loss: 1771.1773681640625\n",
      "epoch: 13 iter: 78 reconn_loss: 9493.5654296875 kl_loss: 1768.8270263671875\n",
      "epoch: 13 iter: 79 reconn_loss: 9391.5009765625 kl_loss: 1835.464599609375\n",
      "epoch: 13 iter: 80 reconn_loss: 9312.4462890625 kl_loss: 1751.8980712890625\n",
      "epoch: 13 iter: 81 reconn_loss: 9262.6962890625 kl_loss: 1792.3719482421875\n",
      "epoch: 13 iter: 82 reconn_loss: 9448.7939453125 kl_loss: 1757.7298583984375\n",
      "epoch: 13 iter: 83 reconn_loss: 8932.4580078125 kl_loss: 1764.0460205078125\n",
      "epoch: 13 iter: 84 reconn_loss: 9215.8984375 kl_loss: 1731.1641845703125\n",
      "epoch: 13 iter: 85 reconn_loss: 9406.0625 kl_loss: 1783.445556640625\n",
      "epoch: 13 iter: 86 reconn_loss: 9343.3583984375 kl_loss: 1848.53955078125\n",
      "epoch: 13 iter: 87 reconn_loss: 9305.1064453125 kl_loss: 1766.9127197265625\n",
      "epoch: 13 iter: 88 reconn_loss: 9077.80078125 kl_loss: 1763.5716552734375\n",
      "epoch: 13 iter: 89 reconn_loss: 9255.00390625 kl_loss: 1784.8741455078125\n",
      "epoch: 13 iter: 90 reconn_loss: 9444.0419921875 kl_loss: 1740.1204833984375\n",
      "epoch: 13 iter: 91 reconn_loss: 9154.8427734375 kl_loss: 1809.573486328125\n",
      "epoch: 13 iter: 92 reconn_loss: 9323.435546875 kl_loss: 1804.316650390625\n",
      "epoch: 13 iter: 93 reconn_loss: 9520.5361328125 kl_loss: 1805.4580078125\n",
      "epoch: 13 iter: 94 reconn_loss: 9556.4140625 kl_loss: 1801.11279296875\n",
      "epoch: 13 iter: 95 reconn_loss: 8950.4619140625 kl_loss: 1787.5799560546875\n",
      "epoch: 13 iter: 96 reconn_loss: 9192.6552734375 kl_loss: 1776.149169921875\n",
      "epoch: 13 iter: 97 reconn_loss: 9524.16796875 kl_loss: 1829.043212890625\n",
      "epoch: 13 iter: 98 reconn_loss: 9261.57421875 kl_loss: 1761.6568603515625\n",
      "epoch: 13 iter: 99 reconn_loss: 9244.22265625 kl_loss: 1854.281005859375\n",
      "epoch: 13 iter: 100 reconn_loss: 9283.5322265625 kl_loss: 1796.072998046875\n",
      "epoch: 13 iter: 101 reconn_loss: 9234.30859375 kl_loss: 1811.4859619140625\n",
      "epoch: 13 iter: 102 reconn_loss: 9142.6689453125 kl_loss: 1825.8358154296875\n",
      "epoch: 13 iter: 103 reconn_loss: 9387.521484375 kl_loss: 1765.186279296875\n",
      "epoch: 13 iter: 104 reconn_loss: 9345.78515625 kl_loss: 1782.00732421875\n",
      "epoch: 13 iter: 105 reconn_loss: 9129.8720703125 kl_loss: 1766.2977294921875\n",
      "epoch: 13 iter: 106 reconn_loss: 9351.166015625 kl_loss: 1796.7998046875\n",
      "epoch: 13 iter: 107 reconn_loss: 8866.0693359375 kl_loss: 1796.457275390625\n",
      "epoch: 13 iter: 108 reconn_loss: 9429.9501953125 kl_loss: 1789.974853515625\n",
      "epoch: 13 iter: 109 reconn_loss: 9032.6748046875 kl_loss: 1749.806640625\n",
      "epoch: 13 iter: 110 reconn_loss: 9285.7060546875 kl_loss: 1836.046630859375\n",
      "epoch: 13 iter: 111 reconn_loss: 9159.0849609375 kl_loss: 1750.570068359375\n",
      "epoch: 13 iter: 112 reconn_loss: 9296.083984375 kl_loss: 1762.0478515625\n",
      "epoch: 13 iter: 113 reconn_loss: 9399.2744140625 kl_loss: 1819.653076171875\n",
      "epoch: 13 iter: 114 reconn_loss: 9342.44921875 kl_loss: 1845.497314453125\n",
      "epoch: 13 iter: 115 reconn_loss: 9489.375 kl_loss: 1854.4920654296875\n",
      "epoch: 13 iter: 116 reconn_loss: 8938.2373046875 kl_loss: 1797.724853515625\n",
      "epoch: 13 iter: 117 reconn_loss: 9482.5703125 kl_loss: 1791.705810546875\n",
      "epoch: 13 iter: 118 reconn_loss: 9103.1162109375 kl_loss: 1848.318115234375\n",
      "epoch: 13 iter: 119 reconn_loss: 9538.0048828125 kl_loss: 1849.5345458984375\n",
      "epoch: 13 iter: 120 reconn_loss: 9544.59765625 kl_loss: 1823.178466796875\n",
      "epoch: 13 iter: 121 reconn_loss: 9316.427734375 kl_loss: 1822.5498046875\n",
      "epoch: 13 iter: 122 reconn_loss: 9353.208984375 kl_loss: 1766.642578125\n",
      "epoch: 13 iter: 123 reconn_loss: 9625.89453125 kl_loss: 1822.52392578125\n",
      "epoch: 13 iter: 124 reconn_loss: 9110.8046875 kl_loss: 1768.415283203125\n",
      "epoch: 13 iter: 125 reconn_loss: 9312.287109375 kl_loss: 1788.068603515625\n",
      "epoch: 13 iter: 126 reconn_loss: 9273.390625 kl_loss: 1779.5731201171875\n",
      "epoch: 13 iter: 127 reconn_loss: 9610.365234375 kl_loss: 1805.1663818359375\n",
      "epoch: 13 iter: 128 reconn_loss: 9676.521484375 kl_loss: 1785.0753173828125\n",
      "epoch: 13 iter: 129 reconn_loss: 9447.904296875 kl_loss: 1815.052001953125\n",
      "epoch: 13 iter: 130 reconn_loss: 9493.6513671875 kl_loss: 1774.0758056640625\n",
      "epoch: 13 iter: 131 reconn_loss: 9062.39453125 kl_loss: 1782.867431640625\n",
      "epoch: 13 iter: 132 reconn_loss: 9188.490234375 kl_loss: 1815.57568359375\n",
      "epoch: 13 iter: 133 reconn_loss: 9464.9189453125 kl_loss: 1822.422119140625\n",
      "epoch: 13 iter: 134 reconn_loss: 9156.181640625 kl_loss: 1788.8935546875\n",
      "epoch: 13 iter: 135 reconn_loss: 9320.333984375 kl_loss: 1739.920166015625\n",
      "epoch: 13 iter: 136 reconn_loss: 9258.828125 kl_loss: 1759.174072265625\n",
      "epoch: 13 iter: 137 reconn_loss: 9269.091796875 kl_loss: 1767.99072265625\n",
      "epoch: 13 iter: 138 reconn_loss: 9333.7421875 kl_loss: 1825.5626220703125\n",
      "epoch: 13 iter: 139 reconn_loss: 9735.58203125 kl_loss: 1793.2911376953125\n",
      "epoch: 13 iter: 140 reconn_loss: 9318.50390625 kl_loss: 1752.750732421875\n",
      "epoch: 13 iter: 141 reconn_loss: 9347.53515625 kl_loss: 1809.0997314453125\n",
      "epoch: 13 iter: 142 reconn_loss: 9325.326171875 kl_loss: 1806.28271484375\n",
      "epoch: 13 iter: 143 reconn_loss: 9083.3310546875 kl_loss: 1771.116943359375\n",
      "epoch: 13 iter: 144 reconn_loss: 9516.46484375 kl_loss: 1806.04833984375\n",
      "epoch: 13 iter: 145 reconn_loss: 9289.046875 kl_loss: 1774.9833984375\n",
      "epoch: 13 iter: 146 reconn_loss: 9467.912109375 kl_loss: 1799.1103515625\n",
      "epoch: 13 iter: 147 reconn_loss: 9124.927734375 kl_loss: 1759.784912109375\n",
      "epoch: 13 iter: 148 reconn_loss: 9212.388671875 kl_loss: 1723.417236328125\n",
      "epoch: 13 iter: 149 reconn_loss: 9435.6103515625 kl_loss: 1781.0682373046875\n",
      "epoch: 13 iter: 150 reconn_loss: 9482.525390625 kl_loss: 1799.2353515625\n",
      "epoch: 13 iter: 151 reconn_loss: 9009.4619140625 kl_loss: 1804.9658203125\n",
      "epoch: 13 iter: 152 reconn_loss: 9159.134765625 kl_loss: 1828.009033203125\n",
      "epoch: 13 iter: 153 reconn_loss: 9532.970703125 kl_loss: 1854.8665771484375\n",
      "epoch: 13 iter: 154 reconn_loss: 9337.4716796875 kl_loss: 1849.533203125\n",
      "epoch: 13 iter: 155 reconn_loss: 9142.9755859375 kl_loss: 1773.0743408203125\n",
      "epoch: 13 iter: 156 reconn_loss: 8735.091796875 kl_loss: 1847.671630859375\n",
      "epoch: 13 iter: 157 reconn_loss: 9502.943359375 kl_loss: 1838.30712890625\n",
      "epoch: 13 iter: 158 reconn_loss: 9513.4833984375 kl_loss: 1787.9490966796875\n",
      "epoch: 13 iter: 159 reconn_loss: 9339.7685546875 kl_loss: 1786.18017578125\n",
      "epoch: 13 iter: 160 reconn_loss: 9194.990234375 kl_loss: 1760.4974365234375\n",
      "epoch: 13 iter: 161 reconn_loss: 9195.6416015625 kl_loss: 1800.609130859375\n",
      "epoch: 13 iter: 162 reconn_loss: 8975.8798828125 kl_loss: 1793.624755859375\n",
      "epoch: 13 iter: 163 reconn_loss: 9059.025390625 kl_loss: 1697.576171875\n",
      "epoch: 13 iter: 164 reconn_loss: 9271.66796875 kl_loss: 1771.17724609375\n",
      "epoch: 13 iter: 165 reconn_loss: 9064.48828125 kl_loss: 1781.475341796875\n",
      "epoch: 13 iter: 166 reconn_loss: 8907.1240234375 kl_loss: 1834.66455078125\n",
      "epoch: 13 iter: 167 reconn_loss: 9087.36328125 kl_loss: 1799.882568359375\n",
      "epoch: 13 iter: 168 reconn_loss: 9691.8564453125 kl_loss: 1789.6800537109375\n",
      "epoch: 13 iter: 169 reconn_loss: 9076.712890625 kl_loss: 1741.5235595703125\n",
      "epoch: 13 iter: 170 reconn_loss: 9077.044921875 kl_loss: 1781.803955078125\n",
      "epoch: 13 iter: 171 reconn_loss: 9370.16015625 kl_loss: 1796.556396484375\n",
      "epoch: 13 iter: 172 reconn_loss: 9247.0771484375 kl_loss: 1786.391845703125\n",
      "epoch: 13 iter: 173 reconn_loss: 9467.69140625 kl_loss: 1760.8865966796875\n",
      "epoch: 13 iter: 174 reconn_loss: 9498.4287109375 kl_loss: 1836.336181640625\n",
      "epoch: 13 iter: 175 reconn_loss: 9276.9140625 kl_loss: 1777.8756103515625\n",
      "epoch: 13 iter: 176 reconn_loss: 8902.6962890625 kl_loss: 1818.3751220703125\n",
      "epoch: 13 iter: 177 reconn_loss: 9390.966796875 kl_loss: 1747.0648193359375\n",
      "epoch: 13 iter: 178 reconn_loss: 9056.97265625 kl_loss: 1809.7625732421875\n",
      "epoch: 13 iter: 179 reconn_loss: 9217.908203125 kl_loss: 1797.1976318359375\n",
      "epoch: 13 iter: 180 reconn_loss: 9131.2275390625 kl_loss: 1764.150146484375\n",
      "epoch: 13 iter: 181 reconn_loss: 9167.18359375 kl_loss: 1800.2535400390625\n",
      "epoch: 13 iter: 182 reconn_loss: 8845.6611328125 kl_loss: 1781.2440185546875\n",
      "epoch: 13 iter: 183 reconn_loss: 9228.3818359375 kl_loss: 1821.52734375\n",
      "epoch: 13 iter: 184 reconn_loss: 8962.9326171875 kl_loss: 1804.7880859375\n",
      "epoch: 13 iter: 185 reconn_loss: 9216.8896484375 kl_loss: 1765.8843994140625\n",
      "epoch: 13 iter: 186 reconn_loss: 9230.486328125 kl_loss: 1821.4189453125\n",
      "epoch: 13 iter: 187 reconn_loss: 9228.5400390625 kl_loss: 1780.0877685546875\n",
      "epoch: 13 iter: 188 reconn_loss: 9197.685546875 kl_loss: 1830.11962890625\n",
      "epoch: 13 iter: 189 reconn_loss: 9385.1337890625 kl_loss: 1787.537109375\n",
      "epoch: 13 iter: 190 reconn_loss: 9184.1689453125 kl_loss: 1807.567626953125\n",
      "epoch: 13 iter: 191 reconn_loss: 9352.86328125 kl_loss: 1784.6807861328125\n",
      "epoch: 13 iter: 192 reconn_loss: 9372.7802734375 kl_loss: 1765.00634765625\n",
      "epoch: 13 iter: 193 reconn_loss: 9388.1513671875 kl_loss: 1816.5137939453125\n",
      "epoch: 13 iter: 194 reconn_loss: 9148.3759765625 kl_loss: 1839.737548828125\n",
      "epoch: 13 iter: 195 reconn_loss: 9529.38671875 kl_loss: 1763.917724609375\n",
      "epoch: 13 iter: 196 reconn_loss: 9317.77734375 kl_loss: 1798.9488525390625\n",
      "epoch: 13 iter: 197 reconn_loss: 9379.91796875 kl_loss: 1813.25732421875\n",
      "epoch: 13 iter: 198 reconn_loss: 9398.232421875 kl_loss: 1765.8016357421875\n",
      "epoch: 13 iter: 199 reconn_loss: 9210.267578125 kl_loss: 1758.466064453125\n",
      "epoch: 13 iter: 200 reconn_loss: 9291.8974609375 kl_loss: 1778.7740478515625\n",
      "epoch: 13 iter: 201 reconn_loss: 9390.380859375 kl_loss: 1716.902587890625\n",
      "epoch: 13 iter: 202 reconn_loss: 9214.6767578125 kl_loss: 1791.6397705078125\n",
      "epoch: 13 iter: 203 reconn_loss: 9102.09375 kl_loss: 1791.4200439453125\n",
      "epoch: 13 iter: 204 reconn_loss: 9327.220703125 kl_loss: 1773.84765625\n",
      "epoch: 13 iter: 205 reconn_loss: 9213.8095703125 kl_loss: 1758.7274169921875\n",
      "epoch: 13 iter: 206 reconn_loss: 8975.353515625 kl_loss: 1817.190673828125\n",
      "epoch: 13 iter: 207 reconn_loss: 9214.5517578125 kl_loss: 1777.59326171875\n",
      "epoch: 13 iter: 208 reconn_loss: 9392.896484375 kl_loss: 1754.472900390625\n",
      "epoch: 13 iter: 209 reconn_loss: 9265.03515625 kl_loss: 1796.706787109375\n",
      "epoch: 13 iter: 210 reconn_loss: 9073.0498046875 kl_loss: 1772.571044921875\n",
      "epoch: 13 iter: 211 reconn_loss: 9185.5302734375 kl_loss: 1802.765869140625\n",
      "epoch: 13 iter: 212 reconn_loss: 8994.1015625 kl_loss: 1732.851806640625\n",
      "epoch: 13 iter: 213 reconn_loss: 9221.1611328125 kl_loss: 1803.7484130859375\n",
      "epoch: 13 iter: 214 reconn_loss: 9106.2470703125 kl_loss: 1762.5098876953125\n",
      "epoch: 13 iter: 215 reconn_loss: 9353.70703125 kl_loss: 1808.0693359375\n",
      "epoch: 13 iter: 216 reconn_loss: 9177.9970703125 kl_loss: 1769.257080078125\n",
      "epoch: 13 iter: 217 reconn_loss: 9335.5556640625 kl_loss: 1802.2974853515625\n",
      "epoch: 13 iter: 218 reconn_loss: 9380.7119140625 kl_loss: 1816.9869384765625\n",
      "epoch: 13 iter: 219 reconn_loss: 9154.0458984375 kl_loss: 1823.0771484375\n",
      "epoch: 13 iter: 220 reconn_loss: 9281.3173828125 kl_loss: 1826.525146484375\n",
      "epoch: 13 iter: 221 reconn_loss: 9302.5068359375 kl_loss: 1838.707763671875\n",
      "epoch: 13 iter: 222 reconn_loss: 6203.71630859375 kl_loss: 1159.5882568359375\n",
      "0.weight tensor(72.1330) tensor(-66.7664)\n",
      "0.bias tensor(28.4285) tensor(-41.5775)\n",
      "2.weight tensor(28.2523) tensor(-20.9922)\n",
      "2.bias tensor(17.6748) tensor(-17.9605)\n",
      "4.weight tensor(10.5847) tensor(-11.8203)\n",
      "4.bias tensor(5.3324) tensor(-7.1003)\n",
      "epoch: 14 iter: 0 reconn_loss: 9260.380859375 kl_loss: 1738.823974609375\n",
      "epoch: 14 iter: 1 reconn_loss: 9386.48828125 kl_loss: 1788.1448974609375\n",
      "epoch: 14 iter: 2 reconn_loss: 9468.0 kl_loss: 1790.5863037109375\n",
      "epoch: 14 iter: 3 reconn_loss: 9302.884765625 kl_loss: 1809.6544189453125\n",
      "epoch: 14 iter: 4 reconn_loss: 9382.998046875 kl_loss: 1783.5888671875\n",
      "epoch: 14 iter: 5 reconn_loss: 9251.259765625 kl_loss: 1802.78955078125\n",
      "epoch: 14 iter: 6 reconn_loss: 9439.3466796875 kl_loss: 1797.945068359375\n",
      "epoch: 14 iter: 7 reconn_loss: 9155.767578125 kl_loss: 1769.3878173828125\n",
      "epoch: 14 iter: 8 reconn_loss: 9241.708984375 kl_loss: 1772.7501220703125\n",
      "epoch: 14 iter: 9 reconn_loss: 9380.599609375 kl_loss: 1804.714599609375\n",
      "epoch: 14 iter: 10 reconn_loss: 9341.552734375 kl_loss: 1798.7332763671875\n",
      "epoch: 14 iter: 11 reconn_loss: 9533.5498046875 kl_loss: 1850.3175048828125\n",
      "epoch: 14 iter: 12 reconn_loss: 8930.7490234375 kl_loss: 1744.6524658203125\n",
      "epoch: 14 iter: 13 reconn_loss: 9078.0615234375 kl_loss: 1804.647705078125\n",
      "epoch: 14 iter: 14 reconn_loss: 8911.4580078125 kl_loss: 1795.4781494140625\n",
      "epoch: 14 iter: 15 reconn_loss: 9238.9833984375 kl_loss: 1821.9552001953125\n",
      "epoch: 14 iter: 16 reconn_loss: 9276.794921875 kl_loss: 1779.5147705078125\n",
      "epoch: 14 iter: 17 reconn_loss: 9316.951171875 kl_loss: 1814.1114501953125\n",
      "epoch: 14 iter: 18 reconn_loss: 9360.6630859375 kl_loss: 1779.632568359375\n",
      "epoch: 14 iter: 19 reconn_loss: 9326.013671875 kl_loss: 1803.0478515625\n",
      "epoch: 14 iter: 20 reconn_loss: 8968.486328125 kl_loss: 1791.4339599609375\n",
      "epoch: 14 iter: 21 reconn_loss: 9333.5078125 kl_loss: 1742.3201904296875\n",
      "epoch: 14 iter: 22 reconn_loss: 9060.92578125 kl_loss: 1793.2017822265625\n",
      "epoch: 14 iter: 23 reconn_loss: 9182.4306640625 kl_loss: 1779.362548828125\n",
      "epoch: 14 iter: 24 reconn_loss: 9193.7119140625 kl_loss: 1799.887451171875\n",
      "epoch: 14 iter: 25 reconn_loss: 9602.9013671875 kl_loss: 1837.4615478515625\n",
      "epoch: 14 iter: 26 reconn_loss: 9256.0703125 kl_loss: 1858.5457763671875\n",
      "epoch: 14 iter: 27 reconn_loss: 9246.798828125 kl_loss: 1867.65283203125\n",
      "epoch: 14 iter: 28 reconn_loss: 8996.220703125 kl_loss: 1880.776611328125\n",
      "epoch: 14 iter: 29 reconn_loss: 9103.6455078125 kl_loss: 1781.48095703125\n",
      "epoch: 14 iter: 30 reconn_loss: 9301.4296875 kl_loss: 1870.8619384765625\n",
      "epoch: 14 iter: 31 reconn_loss: 9219.2490234375 kl_loss: 1832.1417236328125\n",
      "epoch: 14 iter: 32 reconn_loss: 9164.099609375 kl_loss: 1790.287841796875\n",
      "epoch: 14 iter: 33 reconn_loss: 9463.3203125 kl_loss: 1787.6072998046875\n",
      "epoch: 14 iter: 34 reconn_loss: 9379.7041015625 kl_loss: 1780.675048828125\n",
      "epoch: 14 iter: 35 reconn_loss: 9118.5732421875 kl_loss: 1845.242431640625\n",
      "epoch: 14 iter: 36 reconn_loss: 9341.2919921875 kl_loss: 1834.7174072265625\n",
      "epoch: 14 iter: 37 reconn_loss: 9391.3681640625 kl_loss: 1826.568359375\n",
      "epoch: 14 iter: 38 reconn_loss: 9354.1826171875 kl_loss: 1831.413330078125\n",
      "epoch: 14 iter: 39 reconn_loss: 9470.5166015625 kl_loss: 1818.4427490234375\n",
      "epoch: 14 iter: 40 reconn_loss: 9396.515625 kl_loss: 1887.0390625\n",
      "epoch: 14 iter: 41 reconn_loss: 9725.701171875 kl_loss: 1847.6202392578125\n",
      "epoch: 14 iter: 42 reconn_loss: 9390.01171875 kl_loss: 1784.8348388671875\n",
      "epoch: 14 iter: 43 reconn_loss: 9443.154296875 kl_loss: 1799.635986328125\n",
      "epoch: 14 iter: 44 reconn_loss: 8881.0654296875 kl_loss: 1780.6082763671875\n",
      "epoch: 14 iter: 45 reconn_loss: 9563.9287109375 kl_loss: 1817.7257080078125\n",
      "epoch: 14 iter: 46 reconn_loss: 9282.1728515625 kl_loss: 1815.685791015625\n",
      "epoch: 14 iter: 47 reconn_loss: 9218.001953125 kl_loss: 1798.5762939453125\n",
      "epoch: 14 iter: 48 reconn_loss: 9341.5859375 kl_loss: 1810.0460205078125\n",
      "epoch: 14 iter: 49 reconn_loss: 9151.0791015625 kl_loss: 1775.706298828125\n",
      "epoch: 14 iter: 50 reconn_loss: 9235.78125 kl_loss: 1820.90380859375\n",
      "epoch: 14 iter: 51 reconn_loss: 9349.86328125 kl_loss: 1877.16357421875\n",
      "epoch: 14 iter: 52 reconn_loss: 9600.716796875 kl_loss: 1769.1591796875\n",
      "epoch: 14 iter: 53 reconn_loss: 9018.7470703125 kl_loss: 1811.784912109375\n",
      "epoch: 14 iter: 54 reconn_loss: 9266.1650390625 kl_loss: 1792.1983642578125\n",
      "epoch: 14 iter: 55 reconn_loss: 9182.025390625 kl_loss: 1812.689453125\n",
      "epoch: 14 iter: 56 reconn_loss: 9353.3955078125 kl_loss: 1812.040771484375\n",
      "epoch: 14 iter: 57 reconn_loss: 9194.724609375 kl_loss: 1845.2066650390625\n",
      "epoch: 14 iter: 58 reconn_loss: 9617.4951171875 kl_loss: 1801.750732421875\n",
      "epoch: 14 iter: 59 reconn_loss: 9532.759765625 kl_loss: 1874.7677001953125\n",
      "epoch: 14 iter: 60 reconn_loss: 9179.5068359375 kl_loss: 1813.2213134765625\n",
      "epoch: 14 iter: 61 reconn_loss: 9374.8251953125 kl_loss: 1872.6689453125\n",
      "epoch: 14 iter: 62 reconn_loss: 9669.841796875 kl_loss: 1856.0556640625\n",
      "epoch: 14 iter: 63 reconn_loss: 8815.447265625 kl_loss: 1797.50048828125\n",
      "epoch: 14 iter: 64 reconn_loss: 9536.4208984375 kl_loss: 1838.5804443359375\n",
      "epoch: 14 iter: 65 reconn_loss: 9295.59375 kl_loss: 1840.579833984375\n",
      "epoch: 14 iter: 66 reconn_loss: 9291.267578125 kl_loss: 1789.9427490234375\n",
      "epoch: 14 iter: 67 reconn_loss: 9079.0673828125 kl_loss: 1839.1297607421875\n",
      "epoch: 14 iter: 68 reconn_loss: 8929.94921875 kl_loss: 1828.611083984375\n",
      "epoch: 14 iter: 69 reconn_loss: 9213.486328125 kl_loss: 1820.731201171875\n",
      "epoch: 14 iter: 70 reconn_loss: 9237.72265625 kl_loss: 1859.4381103515625\n",
      "epoch: 14 iter: 71 reconn_loss: 9269.818359375 kl_loss: 1807.99755859375\n",
      "epoch: 14 iter: 72 reconn_loss: 9313.177734375 kl_loss: 1800.0062255859375\n",
      "epoch: 14 iter: 73 reconn_loss: 9409.3662109375 kl_loss: 1810.865478515625\n",
      "epoch: 14 iter: 74 reconn_loss: 9443.609375 kl_loss: 1844.378173828125\n",
      "epoch: 14 iter: 75 reconn_loss: 9439.6181640625 kl_loss: 1838.864013671875\n",
      "epoch: 14 iter: 76 reconn_loss: 9239.9873046875 kl_loss: 1803.6649169921875\n",
      "epoch: 14 iter: 77 reconn_loss: 8880.875 kl_loss: 1830.2452392578125\n",
      "epoch: 14 iter: 78 reconn_loss: 9424.3271484375 kl_loss: 1831.8077392578125\n",
      "epoch: 14 iter: 79 reconn_loss: 9218.9267578125 kl_loss: 1820.449951171875\n",
      "epoch: 14 iter: 80 reconn_loss: 9118.6865234375 kl_loss: 1792.8831787109375\n",
      "epoch: 14 iter: 81 reconn_loss: 9317.875 kl_loss: 1853.06298828125\n",
      "epoch: 14 iter: 82 reconn_loss: 8895.2626953125 kl_loss: 1779.9783935546875\n",
      "epoch: 14 iter: 83 reconn_loss: 9166.5673828125 kl_loss: 1812.705810546875\n",
      "epoch: 14 iter: 84 reconn_loss: 9125.0009765625 kl_loss: 1792.5150146484375\n",
      "epoch: 14 iter: 85 reconn_loss: 9445.396484375 kl_loss: 1774.19091796875\n",
      "epoch: 14 iter: 86 reconn_loss: 9417.56640625 kl_loss: 1818.3194580078125\n",
      "epoch: 14 iter: 87 reconn_loss: 9259.0966796875 kl_loss: 1795.7413330078125\n",
      "epoch: 14 iter: 88 reconn_loss: 9131.0888671875 kl_loss: 1818.8306884765625\n",
      "epoch: 14 iter: 89 reconn_loss: 9354.03515625 kl_loss: 1838.222412109375\n",
      "epoch: 14 iter: 90 reconn_loss: 9248.16796875 kl_loss: 1806.9482421875\n",
      "epoch: 14 iter: 91 reconn_loss: 9481.3134765625 kl_loss: 1834.1168212890625\n",
      "epoch: 14 iter: 92 reconn_loss: 9228.0888671875 kl_loss: 1786.198486328125\n",
      "epoch: 14 iter: 93 reconn_loss: 9072.361328125 kl_loss: 1826.6044921875\n",
      "epoch: 14 iter: 94 reconn_loss: 9407.9140625 kl_loss: 1787.123046875\n",
      "epoch: 14 iter: 95 reconn_loss: 9067.6435546875 kl_loss: 1783.4156494140625\n",
      "epoch: 14 iter: 96 reconn_loss: 9137.3369140625 kl_loss: 1807.533935546875\n",
      "epoch: 14 iter: 97 reconn_loss: 9511.6494140625 kl_loss: 1787.5897216796875\n",
      "epoch: 14 iter: 98 reconn_loss: 9196.328125 kl_loss: 1789.002197265625\n",
      "epoch: 14 iter: 99 reconn_loss: 8971.447265625 kl_loss: 1791.2105712890625\n",
      "epoch: 14 iter: 100 reconn_loss: 9157.06640625 kl_loss: 1777.5516357421875\n",
      "epoch: 14 iter: 101 reconn_loss: 9314.009765625 kl_loss: 1765.535400390625\n",
      "epoch: 14 iter: 102 reconn_loss: 9551.0771484375 kl_loss: 1819.889892578125\n",
      "epoch: 14 iter: 103 reconn_loss: 9368.34375 kl_loss: 1828.7537841796875\n",
      "epoch: 14 iter: 104 reconn_loss: 9403.1337890625 kl_loss: 1784.1964111328125\n",
      "epoch: 14 iter: 105 reconn_loss: 9257.5546875 kl_loss: 1775.64892578125\n",
      "epoch: 14 iter: 106 reconn_loss: 9210.791015625 kl_loss: 1843.578857421875\n",
      "epoch: 14 iter: 107 reconn_loss: 9289.388671875 kl_loss: 1814.508544921875\n",
      "epoch: 14 iter: 108 reconn_loss: 9364.1953125 kl_loss: 1832.225830078125\n",
      "epoch: 14 iter: 109 reconn_loss: 9117.7421875 kl_loss: 1819.3375244140625\n",
      "epoch: 14 iter: 110 reconn_loss: 9112.720703125 kl_loss: 1750.863525390625\n",
      "epoch: 14 iter: 111 reconn_loss: 9085.1923828125 kl_loss: 1816.416015625\n",
      "epoch: 14 iter: 112 reconn_loss: 9244.2587890625 kl_loss: 1831.5880126953125\n",
      "epoch: 14 iter: 113 reconn_loss: 9248.6279296875 kl_loss: 1807.8128662109375\n",
      "epoch: 14 iter: 114 reconn_loss: 9197.9140625 kl_loss: 1824.8985595703125\n",
      "epoch: 14 iter: 115 reconn_loss: 9248.4248046875 kl_loss: 1824.4210205078125\n",
      "epoch: 14 iter: 116 reconn_loss: 9067.44140625 kl_loss: 1846.751953125\n",
      "epoch: 14 iter: 117 reconn_loss: 9137.259765625 kl_loss: 1831.878662109375\n",
      "epoch: 14 iter: 118 reconn_loss: 9103.0986328125 kl_loss: 1845.5950927734375\n",
      "epoch: 14 iter: 119 reconn_loss: 9462.1630859375 kl_loss: 1868.868408203125\n",
      "epoch: 14 iter: 120 reconn_loss: 9219.67578125 kl_loss: 1854.6025390625\n",
      "epoch: 14 iter: 121 reconn_loss: 9565.263671875 kl_loss: 1843.6121826171875\n",
      "epoch: 14 iter: 122 reconn_loss: 9094.9912109375 kl_loss: 1775.5848388671875\n",
      "epoch: 14 iter: 123 reconn_loss: 9308.90234375 kl_loss: 1861.62451171875\n",
      "epoch: 14 iter: 124 reconn_loss: 9262.896484375 kl_loss: 1841.181640625\n",
      "epoch: 14 iter: 125 reconn_loss: 9243.83203125 kl_loss: 1800.114990234375\n",
      "epoch: 14 iter: 126 reconn_loss: 8894.8251953125 kl_loss: 1847.60009765625\n",
      "epoch: 14 iter: 127 reconn_loss: 9210.59375 kl_loss: 1861.69677734375\n",
      "epoch: 14 iter: 128 reconn_loss: 9299.9345703125 kl_loss: 1850.664794921875\n",
      "epoch: 14 iter: 129 reconn_loss: 9421.5693359375 kl_loss: 1770.4915771484375\n",
      "epoch: 14 iter: 130 reconn_loss: 9469.3408203125 kl_loss: 1808.0374755859375\n",
      "epoch: 14 iter: 131 reconn_loss: 8955.92578125 kl_loss: 1747.940673828125\n",
      "epoch: 14 iter: 132 reconn_loss: 9312.7509765625 kl_loss: 1772.0125732421875\n",
      "epoch: 14 iter: 133 reconn_loss: 9382.001953125 kl_loss: 1798.1103515625\n",
      "epoch: 14 iter: 134 reconn_loss: 8964.775390625 kl_loss: 1799.3409423828125\n",
      "epoch: 14 iter: 135 reconn_loss: 9101.203125 kl_loss: 1833.31640625\n",
      "epoch: 14 iter: 136 reconn_loss: 9389.8173828125 kl_loss: 1791.039794921875\n",
      "epoch: 14 iter: 137 reconn_loss: 9242.330078125 kl_loss: 1791.432861328125\n",
      "epoch: 14 iter: 138 reconn_loss: 9005.4521484375 kl_loss: 1775.5924072265625\n",
      "epoch: 14 iter: 139 reconn_loss: 9434.5224609375 kl_loss: 1821.40234375\n",
      "epoch: 14 iter: 140 reconn_loss: 9180.154296875 kl_loss: 1769.3665771484375\n",
      "epoch: 14 iter: 141 reconn_loss: 8928.7236328125 kl_loss: 1819.4893798828125\n",
      "epoch: 14 iter: 142 reconn_loss: 9121.23046875 kl_loss: 1782.6331787109375\n",
      "epoch: 14 iter: 143 reconn_loss: 9196.123046875 kl_loss: 1818.59912109375\n",
      "epoch: 14 iter: 144 reconn_loss: 9189.2890625 kl_loss: 1781.3814697265625\n",
      "epoch: 14 iter: 145 reconn_loss: 8976.34765625 kl_loss: 1823.6734619140625\n",
      "epoch: 14 iter: 146 reconn_loss: 8986.98046875 kl_loss: 1767.2945556640625\n",
      "epoch: 14 iter: 147 reconn_loss: 9388.04296875 kl_loss: 1798.102783203125\n",
      "epoch: 14 iter: 148 reconn_loss: 9143.1494140625 kl_loss: 1815.341064453125\n",
      "epoch: 14 iter: 149 reconn_loss: 9002.978515625 kl_loss: 1798.10986328125\n",
      "epoch: 14 iter: 150 reconn_loss: 9174.751953125 kl_loss: 1846.6689453125\n",
      "epoch: 14 iter: 151 reconn_loss: 9047.0673828125 kl_loss: 1871.9437255859375\n",
      "epoch: 14 iter: 152 reconn_loss: 9425.8837890625 kl_loss: 1804.899658203125\n",
      "epoch: 14 iter: 153 reconn_loss: 8991.580078125 kl_loss: 1814.4149169921875\n",
      "epoch: 14 iter: 154 reconn_loss: 9239.400390625 kl_loss: 1856.2071533203125\n",
      "epoch: 14 iter: 155 reconn_loss: 9206.544921875 kl_loss: 1833.91650390625\n",
      "epoch: 14 iter: 156 reconn_loss: 9339.3974609375 kl_loss: 1845.382568359375\n",
      "epoch: 14 iter: 157 reconn_loss: 9446.673828125 kl_loss: 1867.2564697265625\n",
      "epoch: 14 iter: 158 reconn_loss: 8897.849609375 kl_loss: 1854.0882568359375\n",
      "epoch: 14 iter: 159 reconn_loss: 8989.7822265625 kl_loss: 1793.0245361328125\n",
      "epoch: 14 iter: 160 reconn_loss: 9214.556640625 kl_loss: 1808.38037109375\n",
      "epoch: 14 iter: 161 reconn_loss: 9435.00390625 kl_loss: 1799.62890625\n",
      "epoch: 14 iter: 162 reconn_loss: 9288.4296875 kl_loss: 1783.0721435546875\n",
      "epoch: 14 iter: 163 reconn_loss: 8993.0361328125 kl_loss: 1801.3148193359375\n",
      "epoch: 14 iter: 164 reconn_loss: 8946.14453125 kl_loss: 1789.5882568359375\n",
      "epoch: 14 iter: 165 reconn_loss: 9008.48046875 kl_loss: 1825.1162109375\n",
      "epoch: 14 iter: 166 reconn_loss: 9236.0517578125 kl_loss: 1800.3033447265625\n",
      "epoch: 14 iter: 167 reconn_loss: 9294.7109375 kl_loss: 1792.22900390625\n",
      "epoch: 14 iter: 168 reconn_loss: 9427.6015625 kl_loss: 1736.46728515625\n",
      "epoch: 14 iter: 169 reconn_loss: 9290.955078125 kl_loss: 1820.2415771484375\n",
      "epoch: 14 iter: 170 reconn_loss: 9259.66015625 kl_loss: 1775.0869140625\n",
      "epoch: 14 iter: 171 reconn_loss: 9329.85546875 kl_loss: 1777.25537109375\n",
      "epoch: 14 iter: 172 reconn_loss: 9210.82421875 kl_loss: 1783.4349365234375\n",
      "epoch: 14 iter: 173 reconn_loss: 8999.5078125 kl_loss: 1763.072998046875\n",
      "epoch: 14 iter: 174 reconn_loss: 9150.7685546875 kl_loss: 1756.8857421875\n",
      "epoch: 14 iter: 175 reconn_loss: 9149.0966796875 kl_loss: 1762.424072265625\n",
      "epoch: 14 iter: 176 reconn_loss: 9406.296875 kl_loss: 1825.141845703125\n",
      "epoch: 14 iter: 177 reconn_loss: 9481.072265625 kl_loss: 1835.2354736328125\n",
      "epoch: 14 iter: 178 reconn_loss: 9429.0654296875 kl_loss: 1794.689453125\n",
      "epoch: 14 iter: 179 reconn_loss: 9411.291015625 kl_loss: 1819.906494140625\n",
      "epoch: 14 iter: 180 reconn_loss: 9141.763671875 kl_loss: 1862.8648681640625\n",
      "epoch: 14 iter: 181 reconn_loss: 9487.6884765625 kl_loss: 1846.3519287109375\n",
      "epoch: 14 iter: 182 reconn_loss: 9275.8671875 kl_loss: 1816.2135009765625\n",
      "epoch: 14 iter: 183 reconn_loss: 8925.822265625 kl_loss: 1799.43603515625\n",
      "epoch: 14 iter: 184 reconn_loss: 9375.6044921875 kl_loss: 1837.932861328125\n",
      "epoch: 14 iter: 185 reconn_loss: 9443.638671875 kl_loss: 1796.78759765625\n",
      "epoch: 14 iter: 186 reconn_loss: 9064.9248046875 kl_loss: 1829.97802734375\n",
      "epoch: 14 iter: 187 reconn_loss: 9369.2294921875 kl_loss: 1834.662353515625\n",
      "epoch: 14 iter: 188 reconn_loss: 9262.7265625 kl_loss: 1840.1749267578125\n",
      "epoch: 14 iter: 189 reconn_loss: 9265.41796875 kl_loss: 1847.2373046875\n",
      "epoch: 14 iter: 190 reconn_loss: 8969.5966796875 kl_loss: 1797.531494140625\n",
      "epoch: 14 iter: 191 reconn_loss: 8961.271484375 kl_loss: 1815.56396484375\n",
      "epoch: 14 iter: 192 reconn_loss: 8998.9833984375 kl_loss: 1807.56689453125\n",
      "epoch: 14 iter: 193 reconn_loss: 9443.30859375 kl_loss: 1809.073974609375\n",
      "epoch: 14 iter: 194 reconn_loss: 9416.59375 kl_loss: 1836.508544921875\n",
      "epoch: 14 iter: 195 reconn_loss: 8847.9150390625 kl_loss: 1831.576171875\n",
      "epoch: 14 iter: 196 reconn_loss: 8968.3203125 kl_loss: 1829.9571533203125\n",
      "epoch: 14 iter: 197 reconn_loss: 9249.2548828125 kl_loss: 1821.153564453125\n",
      "epoch: 14 iter: 198 reconn_loss: 9175.37109375 kl_loss: 1817.335205078125\n",
      "epoch: 14 iter: 199 reconn_loss: 9323.6865234375 kl_loss: 1771.869140625\n",
      "epoch: 14 iter: 200 reconn_loss: 9386.9052734375 kl_loss: 1827.483642578125\n",
      "epoch: 14 iter: 201 reconn_loss: 9170.111328125 kl_loss: 1891.9715576171875\n",
      "epoch: 14 iter: 202 reconn_loss: 9164.31640625 kl_loss: 1807.6201171875\n",
      "epoch: 14 iter: 203 reconn_loss: 9225.0654296875 kl_loss: 1822.512939453125\n",
      "epoch: 14 iter: 204 reconn_loss: 9283.5791015625 kl_loss: 1786.4295654296875\n",
      "epoch: 14 iter: 205 reconn_loss: 9088.890625 kl_loss: 1781.7705078125\n",
      "epoch: 14 iter: 206 reconn_loss: 8925.66796875 kl_loss: 1822.785888671875\n",
      "epoch: 14 iter: 207 reconn_loss: 9378.72265625 kl_loss: 1816.7672119140625\n",
      "epoch: 14 iter: 208 reconn_loss: 9275.126953125 kl_loss: 1852.2603759765625\n",
      "epoch: 14 iter: 209 reconn_loss: 9424.1181640625 kl_loss: 1838.14697265625\n",
      "epoch: 14 iter: 210 reconn_loss: 9474.080078125 kl_loss: 1841.010986328125\n",
      "epoch: 14 iter: 211 reconn_loss: 8939.4775390625 kl_loss: 1803.25830078125\n",
      "epoch: 14 iter: 212 reconn_loss: 9245.58984375 kl_loss: 1850.7783203125\n",
      "epoch: 14 iter: 213 reconn_loss: 9086.5966796875 kl_loss: 1810.6455078125\n",
      "epoch: 14 iter: 214 reconn_loss: 9506.8212890625 kl_loss: 1798.0389404296875\n",
      "epoch: 14 iter: 215 reconn_loss: 9263.6611328125 kl_loss: 1816.1026611328125\n",
      "epoch: 14 iter: 216 reconn_loss: 9464.4111328125 kl_loss: 1820.2391357421875\n",
      "epoch: 14 iter: 217 reconn_loss: 9200.01171875 kl_loss: 1843.132568359375\n",
      "epoch: 14 iter: 218 reconn_loss: 9187.3984375 kl_loss: 1824.4376220703125\n",
      "epoch: 14 iter: 219 reconn_loss: 9383.0224609375 kl_loss: 1841.541015625\n",
      "epoch: 14 iter: 220 reconn_loss: 9345.0322265625 kl_loss: 1844.462646484375\n",
      "epoch: 14 iter: 221 reconn_loss: 9179.78125 kl_loss: 1900.4112548828125\n",
      "epoch: 14 iter: 222 reconn_loss: 5890.224609375 kl_loss: 1181.725341796875\n",
      "0.weight tensor(68.5571) tensor(-52.0476)\n",
      "0.bias tensor(27.4307) tensor(-21.9166)\n",
      "2.weight tensor(27.0827) tensor(-12.6426)\n",
      "2.bias tensor(14.0873) tensor(-7.5066)\n",
      "4.weight tensor(11.2108) tensor(-10.5731)\n",
      "4.bias tensor(5.9438) tensor(-4.9887)\n",
      "epoch: 15 iter: 0 reconn_loss: 9443.1142578125 kl_loss: 1836.3372802734375\n",
      "epoch: 15 iter: 1 reconn_loss: 9610.80859375 kl_loss: 1834.532958984375\n",
      "epoch: 15 iter: 2 reconn_loss: 9407.873046875 kl_loss: 1812.8587646484375\n",
      "epoch: 15 iter: 3 reconn_loss: 9396.322265625 kl_loss: 1783.149169921875\n",
      "epoch: 15 iter: 4 reconn_loss: 8966.880859375 kl_loss: 1823.38525390625\n",
      "epoch: 15 iter: 5 reconn_loss: 9352.1259765625 kl_loss: 1791.7955322265625\n",
      "epoch: 15 iter: 6 reconn_loss: 9328.5625 kl_loss: 1838.834716796875\n",
      "epoch: 15 iter: 7 reconn_loss: 9052.2998046875 kl_loss: 1808.9061279296875\n",
      "epoch: 15 iter: 8 reconn_loss: 9467.39453125 kl_loss: 1814.7943115234375\n",
      "epoch: 15 iter: 9 reconn_loss: 9299.8076171875 kl_loss: 1828.978271484375\n",
      "epoch: 15 iter: 10 reconn_loss: 9065.98828125 kl_loss: 1858.946533203125\n",
      "epoch: 15 iter: 11 reconn_loss: 9248.818359375 kl_loss: 1805.36181640625\n",
      "epoch: 15 iter: 12 reconn_loss: 9289.51171875 kl_loss: 1816.2449951171875\n",
      "epoch: 15 iter: 13 reconn_loss: 9468.880859375 kl_loss: 1832.495849609375\n",
      "epoch: 15 iter: 14 reconn_loss: 9080.625 kl_loss: 1807.939208984375\n",
      "epoch: 15 iter: 15 reconn_loss: 8976.8720703125 kl_loss: 1789.1207275390625\n",
      "epoch: 15 iter: 16 reconn_loss: 9108.8125 kl_loss: 1814.948974609375\n",
      "epoch: 15 iter: 17 reconn_loss: 9267.421875 kl_loss: 1779.7593994140625\n",
      "epoch: 15 iter: 18 reconn_loss: 9011.1142578125 kl_loss: 1746.114013671875\n",
      "epoch: 15 iter: 19 reconn_loss: 9062.759765625 kl_loss: 1769.613525390625\n",
      "epoch: 15 iter: 20 reconn_loss: 9168.9169921875 kl_loss: 1831.2420654296875\n",
      "epoch: 15 iter: 21 reconn_loss: 9161.724609375 kl_loss: 1799.7027587890625\n",
      "epoch: 15 iter: 22 reconn_loss: 9248.40625 kl_loss: 1801.1878662109375\n",
      "epoch: 15 iter: 23 reconn_loss: 8914.4921875 kl_loss: 1821.1943359375\n",
      "epoch: 15 iter: 24 reconn_loss: 9130.6904296875 kl_loss: 1822.5601806640625\n",
      "epoch: 15 iter: 25 reconn_loss: 8964.5361328125 kl_loss: 1836.0255126953125\n",
      "epoch: 15 iter: 26 reconn_loss: 9239.5849609375 kl_loss: 1899.760986328125\n",
      "epoch: 15 iter: 27 reconn_loss: 9100.5546875 kl_loss: 1823.81005859375\n",
      "epoch: 15 iter: 28 reconn_loss: 9573.6201171875 kl_loss: 1808.6768798828125\n",
      "epoch: 15 iter: 29 reconn_loss: 9311.900390625 kl_loss: 1791.726318359375\n",
      "epoch: 15 iter: 30 reconn_loss: 9135.080078125 kl_loss: 1814.951904296875\n",
      "epoch: 15 iter: 31 reconn_loss: 9323.908203125 kl_loss: 1813.6419677734375\n",
      "epoch: 15 iter: 32 reconn_loss: 9115.859375 kl_loss: 1902.465576171875\n",
      "epoch: 15 iter: 33 reconn_loss: 9362.3232421875 kl_loss: 1815.934814453125\n",
      "epoch: 15 iter: 34 reconn_loss: 9225.017578125 kl_loss: 1833.79296875\n",
      "epoch: 15 iter: 35 reconn_loss: 9331.814453125 kl_loss: 1823.637451171875\n",
      "epoch: 15 iter: 36 reconn_loss: 9027.9765625 kl_loss: 1868.65966796875\n",
      "epoch: 15 iter: 37 reconn_loss: 9115.2392578125 kl_loss: 1804.2254638671875\n",
      "epoch: 15 iter: 38 reconn_loss: 9041.609375 kl_loss: 1818.9827880859375\n",
      "epoch: 15 iter: 39 reconn_loss: 9292.8525390625 kl_loss: 1849.3509521484375\n",
      "epoch: 15 iter: 40 reconn_loss: 9245.6435546875 kl_loss: 1814.745361328125\n",
      "epoch: 15 iter: 41 reconn_loss: 9232.197265625 kl_loss: 1787.92529296875\n",
      "epoch: 15 iter: 42 reconn_loss: 9312.0341796875 kl_loss: 1849.853515625\n",
      "epoch: 15 iter: 43 reconn_loss: 9080.048828125 kl_loss: 1805.416748046875\n",
      "epoch: 15 iter: 44 reconn_loss: 9259.123046875 kl_loss: 1853.15283203125\n",
      "epoch: 15 iter: 45 reconn_loss: 9049.6328125 kl_loss: 1820.8394775390625\n",
      "epoch: 15 iter: 46 reconn_loss: 9335.1435546875 kl_loss: 1800.9903564453125\n",
      "epoch: 15 iter: 47 reconn_loss: 9133.642578125 kl_loss: 1823.2733154296875\n",
      "epoch: 15 iter: 48 reconn_loss: 9049.873046875 kl_loss: 1805.1910400390625\n",
      "epoch: 15 iter: 49 reconn_loss: 9258.3017578125 kl_loss: 1854.1065673828125\n",
      "epoch: 15 iter: 50 reconn_loss: 9121.9326171875 kl_loss: 1848.4071044921875\n",
      "epoch: 15 iter: 51 reconn_loss: 9176.8212890625 kl_loss: 1833.1865234375\n",
      "epoch: 15 iter: 52 reconn_loss: 9376.4169921875 kl_loss: 1820.6473388671875\n",
      "epoch: 15 iter: 53 reconn_loss: 9277.1669921875 kl_loss: 1844.1998291015625\n",
      "epoch: 15 iter: 54 reconn_loss: 9143.287109375 kl_loss: 1826.6611328125\n",
      "epoch: 15 iter: 55 reconn_loss: 9083.99609375 kl_loss: 1811.8077392578125\n",
      "epoch: 15 iter: 56 reconn_loss: 9583.8076171875 kl_loss: 1833.611572265625\n",
      "epoch: 15 iter: 57 reconn_loss: 9480.677734375 kl_loss: 1818.409912109375\n",
      "epoch: 15 iter: 58 reconn_loss: 9121.150390625 kl_loss: 1828.6605224609375\n",
      "epoch: 15 iter: 59 reconn_loss: 9232.8271484375 kl_loss: 1818.559814453125\n",
      "epoch: 15 iter: 60 reconn_loss: 9155.7841796875 kl_loss: 1822.747802734375\n",
      "epoch: 15 iter: 61 reconn_loss: 9103.3916015625 kl_loss: 1811.857666015625\n",
      "epoch: 15 iter: 62 reconn_loss: 9006.2265625 kl_loss: 1876.9119873046875\n",
      "epoch: 15 iter: 63 reconn_loss: 9440.888671875 kl_loss: 1879.1221923828125\n",
      "epoch: 15 iter: 64 reconn_loss: 8637.044921875 kl_loss: 1850.067626953125\n",
      "epoch: 15 iter: 65 reconn_loss: 9257.07421875 kl_loss: 1840.73193359375\n",
      "epoch: 15 iter: 66 reconn_loss: 9119.287109375 kl_loss: 1849.974365234375\n",
      "epoch: 15 iter: 67 reconn_loss: 8945.8046875 kl_loss: 1798.58447265625\n",
      "epoch: 15 iter: 68 reconn_loss: 9306.767578125 kl_loss: 1808.4637451171875\n",
      "epoch: 15 iter: 69 reconn_loss: 9110.99609375 kl_loss: 1822.3134765625\n",
      "epoch: 15 iter: 70 reconn_loss: 9142.6220703125 kl_loss: 1829.1578369140625\n",
      "epoch: 15 iter: 71 reconn_loss: 9346.146484375 kl_loss: 1867.70556640625\n",
      "epoch: 15 iter: 72 reconn_loss: 9268.396484375 kl_loss: 1833.9278564453125\n",
      "epoch: 15 iter: 73 reconn_loss: 9104.57421875 kl_loss: 1817.212158203125\n",
      "epoch: 15 iter: 74 reconn_loss: 8766.609375 kl_loss: 1782.09423828125\n",
      "epoch: 15 iter: 75 reconn_loss: 9354.9814453125 kl_loss: 1844.725830078125\n",
      "epoch: 15 iter: 76 reconn_loss: 9265.3916015625 kl_loss: 1839.25\n",
      "epoch: 15 iter: 77 reconn_loss: 8910.9765625 kl_loss: 1829.8472900390625\n",
      "epoch: 15 iter: 78 reconn_loss: 9058.755859375 kl_loss: 1821.6513671875\n",
      "epoch: 15 iter: 79 reconn_loss: 9594.693359375 kl_loss: 1822.42431640625\n",
      "epoch: 15 iter: 80 reconn_loss: 9163.158203125 kl_loss: 1808.404541015625\n",
      "epoch: 15 iter: 81 reconn_loss: 9086.32421875 kl_loss: 1802.67333984375\n",
      "epoch: 15 iter: 82 reconn_loss: 9274.060546875 kl_loss: 1834.9566650390625\n",
      "epoch: 15 iter: 83 reconn_loss: 9197.2890625 kl_loss: 1819.60302734375\n",
      "epoch: 15 iter: 84 reconn_loss: 9240.6943359375 kl_loss: 1854.3470458984375\n",
      "epoch: 15 iter: 85 reconn_loss: 8932.7509765625 kl_loss: 1809.741455078125\n",
      "epoch: 15 iter: 86 reconn_loss: 9219.89453125 kl_loss: 1852.66796875\n",
      "epoch: 15 iter: 87 reconn_loss: 9467.666015625 kl_loss: 1838.9368896484375\n",
      "epoch: 15 iter: 88 reconn_loss: 9240.615234375 kl_loss: 1839.2601318359375\n",
      "epoch: 15 iter: 89 reconn_loss: 9076.40234375 kl_loss: 1852.5345458984375\n",
      "epoch: 15 iter: 90 reconn_loss: 9584.048828125 kl_loss: 1862.71728515625\n",
      "epoch: 15 iter: 91 reconn_loss: 9349.583984375 kl_loss: 1811.5537109375\n",
      "epoch: 15 iter: 92 reconn_loss: 9040.671875 kl_loss: 1868.1038818359375\n",
      "epoch: 15 iter: 93 reconn_loss: 8969.55078125 kl_loss: 1799.03662109375\n",
      "epoch: 15 iter: 94 reconn_loss: 9360.755859375 kl_loss: 1834.61474609375\n",
      "epoch: 15 iter: 95 reconn_loss: 9345.95703125 kl_loss: 1789.4075927734375\n",
      "epoch: 15 iter: 96 reconn_loss: 8848.794921875 kl_loss: 1822.04638671875\n",
      "epoch: 15 iter: 97 reconn_loss: 8932.7314453125 kl_loss: 1773.742431640625\n",
      "epoch: 15 iter: 98 reconn_loss: 8910.03515625 kl_loss: 1784.5416259765625\n",
      "epoch: 15 iter: 99 reconn_loss: 9169.265625 kl_loss: 1802.0567626953125\n",
      "epoch: 15 iter: 100 reconn_loss: 9066.734375 kl_loss: 1831.0228271484375\n",
      "epoch: 15 iter: 101 reconn_loss: 9278.41015625 kl_loss: 1841.7354736328125\n",
      "epoch: 15 iter: 102 reconn_loss: 9430.23828125 kl_loss: 1806.24462890625\n",
      "epoch: 15 iter: 103 reconn_loss: 9104.9306640625 kl_loss: 1779.6326904296875\n",
      "epoch: 15 iter: 104 reconn_loss: 9410.4619140625 kl_loss: 1869.317626953125\n",
      "epoch: 15 iter: 105 reconn_loss: 9056.0302734375 kl_loss: 1819.203857421875\n",
      "epoch: 15 iter: 106 reconn_loss: 9052.7900390625 kl_loss: 1842.719482421875\n",
      "epoch: 15 iter: 107 reconn_loss: 8819.42578125 kl_loss: 1821.7919921875\n",
      "epoch: 15 iter: 108 reconn_loss: 9222.4619140625 kl_loss: 1801.756103515625\n",
      "epoch: 15 iter: 109 reconn_loss: 9491.33984375 kl_loss: 1816.599853515625\n",
      "epoch: 15 iter: 110 reconn_loss: 9181.865234375 kl_loss: 1840.169921875\n",
      "epoch: 15 iter: 111 reconn_loss: 9342.0556640625 kl_loss: 1806.872314453125\n",
      "epoch: 15 iter: 112 reconn_loss: 9368.5 kl_loss: 1778.571533203125\n",
      "epoch: 15 iter: 113 reconn_loss: 9318.5185546875 kl_loss: 1850.4346923828125\n",
      "epoch: 15 iter: 114 reconn_loss: 8840.16796875 kl_loss: 1826.97509765625\n",
      "epoch: 15 iter: 115 reconn_loss: 9327.8701171875 kl_loss: 1860.4193115234375\n",
      "epoch: 15 iter: 116 reconn_loss: 9024.1669921875 kl_loss: 1827.1328125\n",
      "epoch: 15 iter: 117 reconn_loss: 9201.0751953125 kl_loss: 1799.7225341796875\n",
      "epoch: 15 iter: 118 reconn_loss: 9029.7587890625 kl_loss: 1823.998779296875\n",
      "epoch: 15 iter: 119 reconn_loss: 9331.888671875 kl_loss: 1809.472900390625\n",
      "epoch: 15 iter: 120 reconn_loss: 9072.546875 kl_loss: 1839.5855712890625\n",
      "epoch: 15 iter: 121 reconn_loss: 9213.732421875 kl_loss: 1869.4078369140625\n",
      "epoch: 15 iter: 122 reconn_loss: 9244.6630859375 kl_loss: 1838.676513671875\n",
      "epoch: 15 iter: 123 reconn_loss: 8961.9794921875 kl_loss: 1778.8582763671875\n",
      "epoch: 15 iter: 124 reconn_loss: 8893.8408203125 kl_loss: 1832.4320068359375\n",
      "epoch: 15 iter: 125 reconn_loss: 9427.904296875 kl_loss: 1856.055419921875\n",
      "epoch: 15 iter: 126 reconn_loss: 9263.609375 kl_loss: 1835.301513671875\n",
      "epoch: 15 iter: 127 reconn_loss: 9189.7890625 kl_loss: 1787.67724609375\n",
      "epoch: 15 iter: 128 reconn_loss: 9274.9541015625 kl_loss: 1856.978759765625\n",
      "epoch: 15 iter: 129 reconn_loss: 9513.8447265625 kl_loss: 1855.7276611328125\n",
      "epoch: 15 iter: 130 reconn_loss: 9360.501953125 kl_loss: 1841.29248046875\n",
      "epoch: 15 iter: 131 reconn_loss: 9221.958984375 kl_loss: 1852.916259765625\n",
      "epoch: 15 iter: 132 reconn_loss: 9213.03125 kl_loss: 1802.3521728515625\n",
      "epoch: 15 iter: 133 reconn_loss: 9490.1240234375 kl_loss: 1812.5257568359375\n",
      "epoch: 15 iter: 134 reconn_loss: 9165.48828125 kl_loss: 1853.48193359375\n",
      "epoch: 15 iter: 135 reconn_loss: 9171.5888671875 kl_loss: 1807.785888671875\n",
      "epoch: 15 iter: 136 reconn_loss: 9475.185546875 kl_loss: 1845.609375\n",
      "epoch: 15 iter: 137 reconn_loss: 9062.865234375 kl_loss: 1816.416259765625\n",
      "epoch: 15 iter: 138 reconn_loss: 9388.7900390625 kl_loss: 1836.5740966796875\n",
      "epoch: 15 iter: 139 reconn_loss: 9268.2109375 kl_loss: 1871.021484375\n",
      "epoch: 15 iter: 140 reconn_loss: 9176.033203125 kl_loss: 1811.6395263671875\n",
      "epoch: 15 iter: 141 reconn_loss: 9225.2578125 kl_loss: 1803.9288330078125\n",
      "epoch: 15 iter: 142 reconn_loss: 8975.26953125 kl_loss: 1872.4708251953125\n",
      "epoch: 15 iter: 143 reconn_loss: 9070.5517578125 kl_loss: 1849.9501953125\n",
      "epoch: 15 iter: 144 reconn_loss: 9163.279296875 kl_loss: 1856.9617919921875\n",
      "epoch: 15 iter: 145 reconn_loss: 8919.5498046875 kl_loss: 1772.4312744140625\n",
      "epoch: 15 iter: 146 reconn_loss: 9230.5302734375 kl_loss: 1857.0318603515625\n",
      "epoch: 15 iter: 147 reconn_loss: 8829.7138671875 kl_loss: 1880.462646484375\n",
      "epoch: 15 iter: 148 reconn_loss: 9163.630859375 kl_loss: 1792.9835205078125\n",
      "epoch: 15 iter: 149 reconn_loss: 9297.810546875 kl_loss: 1877.8797607421875\n",
      "epoch: 15 iter: 150 reconn_loss: 9179.017578125 kl_loss: 1872.9930419921875\n",
      "epoch: 15 iter: 151 reconn_loss: 9269.859375 kl_loss: 1841.7120361328125\n",
      "epoch: 15 iter: 152 reconn_loss: 9066.7578125 kl_loss: 1865.5860595703125\n",
      "epoch: 15 iter: 153 reconn_loss: 9371.09765625 kl_loss: 1841.1121826171875\n",
      "epoch: 15 iter: 154 reconn_loss: 9353.822265625 kl_loss: 1820.1383056640625\n",
      "epoch: 15 iter: 155 reconn_loss: 9480.0947265625 kl_loss: 1835.231201171875\n",
      "epoch: 15 iter: 156 reconn_loss: 9228.3359375 kl_loss: 1853.2093505859375\n",
      "epoch: 15 iter: 157 reconn_loss: 8864.857421875 kl_loss: 1843.7103271484375\n",
      "epoch: 15 iter: 158 reconn_loss: 9040.884765625 kl_loss: 1881.463623046875\n",
      "epoch: 15 iter: 159 reconn_loss: 9103.7421875 kl_loss: 1866.91015625\n",
      "epoch: 15 iter: 160 reconn_loss: 9039.958984375 kl_loss: 1833.3798828125\n",
      "epoch: 15 iter: 161 reconn_loss: 9325.740234375 kl_loss: 1877.9107666015625\n",
      "epoch: 15 iter: 162 reconn_loss: 9634.8427734375 kl_loss: 1865.105712890625\n",
      "epoch: 15 iter: 163 reconn_loss: 9179.85546875 kl_loss: 1829.44677734375\n",
      "epoch: 15 iter: 164 reconn_loss: 9276.7353515625 kl_loss: 1858.522216796875\n",
      "epoch: 15 iter: 165 reconn_loss: 8949.5830078125 kl_loss: 1794.74267578125\n",
      "epoch: 15 iter: 166 reconn_loss: 9243.0732421875 kl_loss: 1842.6192626953125\n",
      "epoch: 15 iter: 167 reconn_loss: 9047.265625 kl_loss: 1841.357177734375\n",
      "epoch: 15 iter: 168 reconn_loss: 9360.8671875 kl_loss: 1873.0899658203125\n",
      "epoch: 15 iter: 169 reconn_loss: 9173.078125 kl_loss: 1835.0771484375\n",
      "epoch: 15 iter: 170 reconn_loss: 9289.9755859375 kl_loss: 1860.18896484375\n",
      "epoch: 15 iter: 171 reconn_loss: 9321.66015625 kl_loss: 1789.41552734375\n",
      "epoch: 15 iter: 172 reconn_loss: 8959.7333984375 kl_loss: 1784.267822265625\n",
      "epoch: 15 iter: 173 reconn_loss: 8914.58984375 kl_loss: 1803.781982421875\n",
      "epoch: 15 iter: 174 reconn_loss: 9104.68359375 kl_loss: 1789.0391845703125\n",
      "epoch: 15 iter: 175 reconn_loss: 9445.0087890625 kl_loss: 1862.3446044921875\n",
      "epoch: 15 iter: 176 reconn_loss: 9461.96484375 kl_loss: 1819.8355712890625\n",
      "epoch: 15 iter: 177 reconn_loss: 9102.421875 kl_loss: 1813.675537109375\n",
      "epoch: 15 iter: 178 reconn_loss: 9219.5849609375 kl_loss: 1781.853515625\n",
      "epoch: 15 iter: 179 reconn_loss: 9260.416015625 kl_loss: 1837.4005126953125\n",
      "epoch: 15 iter: 180 reconn_loss: 9181.2470703125 kl_loss: 1860.8321533203125\n",
      "epoch: 15 iter: 181 reconn_loss: 9257.236328125 kl_loss: 1823.6021728515625\n",
      "epoch: 15 iter: 182 reconn_loss: 9034.626953125 kl_loss: 1766.556640625\n",
      "epoch: 15 iter: 183 reconn_loss: 9307.646484375 kl_loss: 1772.388916015625\n",
      "epoch: 15 iter: 184 reconn_loss: 9106.2060546875 kl_loss: 1785.357177734375\n",
      "epoch: 15 iter: 185 reconn_loss: 9240.5908203125 kl_loss: 1821.1788330078125\n",
      "epoch: 15 iter: 186 reconn_loss: 9076.6220703125 kl_loss: 1843.0654296875\n",
      "epoch: 15 iter: 187 reconn_loss: 9491.64453125 kl_loss: 1856.007568359375\n",
      "epoch: 15 iter: 188 reconn_loss: 9480.3798828125 kl_loss: 1856.8582763671875\n",
      "epoch: 15 iter: 189 reconn_loss: 9113.6611328125 kl_loss: 1822.975341796875\n",
      "epoch: 15 iter: 190 reconn_loss: 9347.5654296875 kl_loss: 1864.864501953125\n",
      "epoch: 15 iter: 191 reconn_loss: 9216.900390625 kl_loss: 1847.6895751953125\n",
      "epoch: 15 iter: 192 reconn_loss: 9178.67578125 kl_loss: 1853.9095458984375\n",
      "epoch: 15 iter: 193 reconn_loss: 9215.6611328125 kl_loss: 1834.246337890625\n",
      "epoch: 15 iter: 194 reconn_loss: 8925.8583984375 kl_loss: 1783.689208984375\n",
      "epoch: 15 iter: 195 reconn_loss: 9219.3994140625 kl_loss: 1806.05859375\n",
      "epoch: 15 iter: 196 reconn_loss: 9077.279296875 kl_loss: 1847.3140869140625\n",
      "epoch: 15 iter: 197 reconn_loss: 9283.9423828125 kl_loss: 1828.046875\n",
      "epoch: 15 iter: 198 reconn_loss: 9345.0537109375 kl_loss: 1870.4031982421875\n",
      "epoch: 15 iter: 199 reconn_loss: 9073.9833984375 kl_loss: 1937.4068603515625\n",
      "epoch: 15 iter: 200 reconn_loss: 9015.0517578125 kl_loss: 1807.692138671875\n",
      "epoch: 15 iter: 201 reconn_loss: 9093.4033203125 kl_loss: 1838.3265380859375\n",
      "epoch: 15 iter: 202 reconn_loss: 8905.9296875 kl_loss: 1869.748046875\n",
      "epoch: 15 iter: 203 reconn_loss: 8814.7060546875 kl_loss: 1798.517822265625\n",
      "epoch: 15 iter: 204 reconn_loss: 9223.5390625 kl_loss: 1838.13037109375\n",
      "epoch: 15 iter: 205 reconn_loss: 9134.494140625 kl_loss: 1812.2037353515625\n",
      "epoch: 15 iter: 206 reconn_loss: 8973.12890625 kl_loss: 1843.8306884765625\n",
      "epoch: 15 iter: 207 reconn_loss: 9100.28125 kl_loss: 1830.255615234375\n",
      "epoch: 15 iter: 208 reconn_loss: 9198.857421875 kl_loss: 1832.0299072265625\n",
      "epoch: 15 iter: 209 reconn_loss: 9277.4765625 kl_loss: 1824.41748046875\n",
      "epoch: 15 iter: 210 reconn_loss: 9061.154296875 kl_loss: 1819.5093994140625\n",
      "epoch: 15 iter: 211 reconn_loss: 9286.5224609375 kl_loss: 1825.6329345703125\n",
      "epoch: 15 iter: 212 reconn_loss: 8809.3056640625 kl_loss: 1793.1417236328125\n",
      "epoch: 15 iter: 213 reconn_loss: 9111.927734375 kl_loss: 1811.1512451171875\n",
      "epoch: 15 iter: 214 reconn_loss: 9124.9755859375 kl_loss: 1772.1988525390625\n",
      "epoch: 15 iter: 215 reconn_loss: 9030.8203125 kl_loss: 1837.959716796875\n",
      "epoch: 15 iter: 216 reconn_loss: 9254.794921875 kl_loss: 1823.2652587890625\n",
      "epoch: 15 iter: 217 reconn_loss: 8927.3203125 kl_loss: 1791.7811279296875\n",
      "epoch: 15 iter: 218 reconn_loss: 9292.0166015625 kl_loss: 1803.552734375\n",
      "epoch: 15 iter: 219 reconn_loss: 9257.8818359375 kl_loss: 1847.05419921875\n",
      "epoch: 15 iter: 220 reconn_loss: 9385.9365234375 kl_loss: 1864.401123046875\n",
      "epoch: 15 iter: 221 reconn_loss: 9126.59375 kl_loss: 1825.52490234375\n",
      "epoch: 15 iter: 222 reconn_loss: 5975.5546875 kl_loss: 1180.8731689453125\n",
      "0.weight tensor(71.1560) tensor(-73.4412)\n",
      "0.bias tensor(18.8760) tensor(-39.4848)\n",
      "2.weight tensor(24.4593) tensor(-19.0600)\n",
      "2.bias tensor(11.2716) tensor(-13.4688)\n",
      "4.weight tensor(12.2734) tensor(-9.0967)\n",
      "4.bias tensor(5.7094) tensor(-4.6955)\n",
      "epoch: 16 iter: 0 reconn_loss: 9217.5537109375 kl_loss: 1862.031494140625\n",
      "epoch: 16 iter: 1 reconn_loss: 8891.12890625 kl_loss: 1822.3966064453125\n",
      "epoch: 16 iter: 2 reconn_loss: 9103.4892578125 kl_loss: 1824.6064453125\n",
      "epoch: 16 iter: 3 reconn_loss: 9121.41015625 kl_loss: 1808.31591796875\n",
      "epoch: 16 iter: 4 reconn_loss: 9228.3115234375 kl_loss: 1823.0184326171875\n",
      "epoch: 16 iter: 5 reconn_loss: 9341.568359375 kl_loss: 1799.1741943359375\n",
      "epoch: 16 iter: 6 reconn_loss: 9151.2099609375 kl_loss: 1833.860107421875\n",
      "epoch: 16 iter: 7 reconn_loss: 8831.9873046875 kl_loss: 1839.160400390625\n",
      "epoch: 16 iter: 8 reconn_loss: 9333.328125 kl_loss: 1826.097900390625\n",
      "epoch: 16 iter: 9 reconn_loss: 9148.408203125 kl_loss: 1810.3642578125\n",
      "epoch: 16 iter: 10 reconn_loss: 9016.04296875 kl_loss: 1856.9237060546875\n",
      "epoch: 16 iter: 11 reconn_loss: 9051.3974609375 kl_loss: 1836.77734375\n",
      "epoch: 16 iter: 12 reconn_loss: 9274.8798828125 kl_loss: 1808.413818359375\n",
      "epoch: 16 iter: 13 reconn_loss: 9014.033203125 kl_loss: 1817.7581787109375\n",
      "epoch: 16 iter: 14 reconn_loss: 8972.396484375 kl_loss: 1833.605712890625\n",
      "epoch: 16 iter: 15 reconn_loss: 8714.2333984375 kl_loss: 1789.783935546875\n",
      "epoch: 16 iter: 16 reconn_loss: 9359.208984375 kl_loss: 1845.7860107421875\n",
      "epoch: 16 iter: 17 reconn_loss: 8965.1064453125 kl_loss: 1855.887451171875\n",
      "epoch: 16 iter: 18 reconn_loss: 9278.1513671875 kl_loss: 1871.9188232421875\n",
      "epoch: 16 iter: 19 reconn_loss: 9178.572265625 kl_loss: 1858.2694091796875\n",
      "epoch: 16 iter: 20 reconn_loss: 9025.599609375 kl_loss: 1792.1826171875\n",
      "epoch: 16 iter: 21 reconn_loss: 8756.1103515625 kl_loss: 1821.416259765625\n",
      "epoch: 16 iter: 22 reconn_loss: 9234.9384765625 kl_loss: 1871.145751953125\n",
      "epoch: 16 iter: 23 reconn_loss: 9188.517578125 kl_loss: 1846.877197265625\n",
      "epoch: 16 iter: 24 reconn_loss: 9019.419921875 kl_loss: 1826.52197265625\n",
      "epoch: 16 iter: 25 reconn_loss: 9164.5400390625 kl_loss: 1848.80810546875\n",
      "epoch: 16 iter: 26 reconn_loss: 9674.0234375 kl_loss: 1858.1478271484375\n",
      "epoch: 16 iter: 27 reconn_loss: 9012.2900390625 kl_loss: 1816.4708251953125\n",
      "epoch: 16 iter: 28 reconn_loss: 9308.0634765625 kl_loss: 1853.5594482421875\n",
      "epoch: 16 iter: 29 reconn_loss: 9283.8544921875 kl_loss: 1895.3499755859375\n",
      "epoch: 16 iter: 30 reconn_loss: 8936.216796875 kl_loss: 1824.0186767578125\n",
      "epoch: 16 iter: 31 reconn_loss: 9319.1103515625 kl_loss: 1864.416015625\n",
      "epoch: 16 iter: 32 reconn_loss: 9284.9248046875 kl_loss: 1860.198974609375\n",
      "epoch: 16 iter: 33 reconn_loss: 9073.2119140625 kl_loss: 1830.947509765625\n",
      "epoch: 16 iter: 34 reconn_loss: 9331.1142578125 kl_loss: 1835.3514404296875\n",
      "epoch: 16 iter: 35 reconn_loss: 8938.75390625 kl_loss: 1841.09375\n",
      "epoch: 16 iter: 36 reconn_loss: 9260.720703125 kl_loss: 1843.6734619140625\n",
      "epoch: 16 iter: 37 reconn_loss: 9601.427734375 kl_loss: 1848.8902587890625\n",
      "epoch: 16 iter: 38 reconn_loss: 8984.5458984375 kl_loss: 1835.6634521484375\n",
      "epoch: 16 iter: 39 reconn_loss: 9291.6748046875 kl_loss: 1869.1099853515625\n",
      "epoch: 16 iter: 40 reconn_loss: 8712.2060546875 kl_loss: 1820.399658203125\n",
      "epoch: 16 iter: 41 reconn_loss: 9234.857421875 kl_loss: 1833.1724853515625\n",
      "epoch: 16 iter: 42 reconn_loss: 9080.6064453125 kl_loss: 1819.5950927734375\n",
      "epoch: 16 iter: 43 reconn_loss: 9143.2578125 kl_loss: 1816.3551025390625\n",
      "epoch: 16 iter: 44 reconn_loss: 9229.236328125 kl_loss: 1838.7080078125\n",
      "epoch: 16 iter: 45 reconn_loss: 9305.5078125 kl_loss: 1856.5244140625\n",
      "epoch: 16 iter: 46 reconn_loss: 8945.5859375 kl_loss: 1847.61669921875\n",
      "epoch: 16 iter: 47 reconn_loss: 8914.4921875 kl_loss: 1816.332275390625\n",
      "epoch: 16 iter: 48 reconn_loss: 8832.431640625 kl_loss: 1854.2322998046875\n",
      "epoch: 16 iter: 49 reconn_loss: 9081.494140625 kl_loss: 1873.5390625\n",
      "epoch: 16 iter: 50 reconn_loss: 9258.447265625 kl_loss: 1902.2183837890625\n",
      "epoch: 16 iter: 51 reconn_loss: 9004.251953125 kl_loss: 1835.9774169921875\n",
      "epoch: 16 iter: 52 reconn_loss: 9058.6748046875 kl_loss: 1880.22900390625\n",
      "epoch: 16 iter: 53 reconn_loss: 8905.87890625 kl_loss: 1828.9957275390625\n",
      "epoch: 16 iter: 54 reconn_loss: 8946.7041015625 kl_loss: 1825.0543212890625\n",
      "epoch: 16 iter: 55 reconn_loss: 9475.5283203125 kl_loss: 1904.2003173828125\n",
      "epoch: 16 iter: 56 reconn_loss: 9332.6025390625 kl_loss: 1845.292236328125\n",
      "epoch: 16 iter: 57 reconn_loss: 9410.4521484375 kl_loss: 1838.977783203125\n",
      "epoch: 16 iter: 58 reconn_loss: 9140.35546875 kl_loss: 1828.68408203125\n",
      "epoch: 16 iter: 59 reconn_loss: 9317.29296875 kl_loss: 1853.5087890625\n",
      "epoch: 16 iter: 60 reconn_loss: 8956.966796875 kl_loss: 1878.8359375\n",
      "epoch: 16 iter: 61 reconn_loss: 8920.03125 kl_loss: 1825.2535400390625\n",
      "epoch: 16 iter: 62 reconn_loss: 8898.15234375 kl_loss: 1804.82958984375\n",
      "epoch: 16 iter: 63 reconn_loss: 9362.3759765625 kl_loss: 1841.8509521484375\n",
      "epoch: 16 iter: 64 reconn_loss: 9364.1103515625 kl_loss: 1856.311279296875\n",
      "epoch: 16 iter: 65 reconn_loss: 9052.9267578125 kl_loss: 1829.0345458984375\n",
      "epoch: 16 iter: 66 reconn_loss: 9037.1806640625 kl_loss: 1824.5057373046875\n",
      "epoch: 16 iter: 67 reconn_loss: 8873.2236328125 kl_loss: 1787.4815673828125\n",
      "epoch: 16 iter: 68 reconn_loss: 9205.052734375 kl_loss: 1842.0712890625\n",
      "epoch: 16 iter: 69 reconn_loss: 9052.9345703125 kl_loss: 1835.858642578125\n",
      "epoch: 16 iter: 70 reconn_loss: 9171.6875 kl_loss: 1777.32666015625\n",
      "epoch: 16 iter: 71 reconn_loss: 9322.4072265625 kl_loss: 1817.8310546875\n",
      "epoch: 16 iter: 72 reconn_loss: 9227.3896484375 kl_loss: 1829.2396240234375\n",
      "epoch: 16 iter: 73 reconn_loss: 9085.779296875 kl_loss: 1837.33349609375\n",
      "epoch: 16 iter: 74 reconn_loss: 9004.125 kl_loss: 1861.42919921875\n",
      "epoch: 16 iter: 75 reconn_loss: 8746.5048828125 kl_loss: 1856.23193359375\n",
      "epoch: 16 iter: 76 reconn_loss: 9148.431640625 kl_loss: 1871.191162109375\n",
      "epoch: 16 iter: 77 reconn_loss: 9221.8955078125 kl_loss: 1811.3114013671875\n",
      "epoch: 16 iter: 78 reconn_loss: 9091.986328125 kl_loss: 1838.435302734375\n",
      "epoch: 16 iter: 79 reconn_loss: 9478.2646484375 kl_loss: 1837.9925537109375\n",
      "epoch: 16 iter: 80 reconn_loss: 9199.2880859375 kl_loss: 1845.3736572265625\n",
      "epoch: 16 iter: 81 reconn_loss: 9260.99609375 kl_loss: 1854.93701171875\n",
      "epoch: 16 iter: 82 reconn_loss: 9347.25 kl_loss: 1820.51953125\n",
      "epoch: 16 iter: 83 reconn_loss: 9296.662109375 kl_loss: 1863.659423828125\n",
      "epoch: 16 iter: 84 reconn_loss: 9177.701171875 kl_loss: 1872.45849609375\n",
      "epoch: 16 iter: 85 reconn_loss: 8974.080078125 kl_loss: 1848.0869140625\n",
      "epoch: 16 iter: 86 reconn_loss: 9089.5087890625 kl_loss: 1824.6578369140625\n",
      "epoch: 16 iter: 87 reconn_loss: 9170.3759765625 kl_loss: 1842.1466064453125\n",
      "epoch: 16 iter: 88 reconn_loss: 9360.892578125 kl_loss: 1846.07861328125\n",
      "epoch: 16 iter: 89 reconn_loss: 9408.0625 kl_loss: 1868.64404296875\n",
      "epoch: 16 iter: 90 reconn_loss: 8957.6083984375 kl_loss: 1866.32421875\n",
      "epoch: 16 iter: 91 reconn_loss: 9070.6123046875 kl_loss: 1806.2772216796875\n",
      "epoch: 16 iter: 92 reconn_loss: 9459.8935546875 kl_loss: 1892.916259765625\n",
      "epoch: 16 iter: 93 reconn_loss: 9479.0498046875 kl_loss: 1785.6673583984375\n",
      "epoch: 16 iter: 94 reconn_loss: 9294.01953125 kl_loss: 1809.03466796875\n",
      "epoch: 16 iter: 95 reconn_loss: 9236.25 kl_loss: 1855.8721923828125\n",
      "epoch: 16 iter: 96 reconn_loss: 8924.9130859375 kl_loss: 1837.317626953125\n",
      "epoch: 16 iter: 97 reconn_loss: 9221.146484375 kl_loss: 1816.9649658203125\n",
      "epoch: 16 iter: 98 reconn_loss: 8790.673828125 kl_loss: 1829.1788330078125\n",
      "epoch: 16 iter: 99 reconn_loss: 9353.9609375 kl_loss: 1810.2301025390625\n",
      "epoch: 16 iter: 100 reconn_loss: 9182.8408203125 kl_loss: 1850.7239990234375\n",
      "epoch: 16 iter: 101 reconn_loss: 9426.615234375 kl_loss: 1856.599853515625\n",
      "epoch: 16 iter: 102 reconn_loss: 9452.1484375 kl_loss: 1785.3184814453125\n",
      "epoch: 16 iter: 103 reconn_loss: 8509.052734375 kl_loss: 1815.86376953125\n",
      "epoch: 16 iter: 104 reconn_loss: 9356.0947265625 kl_loss: 1826.033203125\n",
      "epoch: 16 iter: 105 reconn_loss: 9356.3466796875 kl_loss: 1882.9752197265625\n",
      "epoch: 16 iter: 106 reconn_loss: 9216.3154296875 kl_loss: 1818.58984375\n",
      "epoch: 16 iter: 107 reconn_loss: 8899.990234375 kl_loss: 1821.4151611328125\n",
      "epoch: 16 iter: 108 reconn_loss: 8903.830078125 kl_loss: 1787.2354736328125\n",
      "epoch: 16 iter: 109 reconn_loss: 9213.3740234375 kl_loss: 1852.00048828125\n",
      "epoch: 16 iter: 110 reconn_loss: 8693.42578125 kl_loss: 1840.08740234375\n",
      "epoch: 16 iter: 111 reconn_loss: 9379.548828125 kl_loss: 1843.41552734375\n",
      "epoch: 16 iter: 112 reconn_loss: 8868.25390625 kl_loss: 1823.4066162109375\n",
      "epoch: 16 iter: 113 reconn_loss: 9122.125 kl_loss: 1822.673095703125\n",
      "epoch: 16 iter: 114 reconn_loss: 9053.3662109375 kl_loss: 1855.42822265625\n",
      "epoch: 16 iter: 115 reconn_loss: 9467.83203125 kl_loss: 1871.189697265625\n",
      "epoch: 16 iter: 116 reconn_loss: 9102.1494140625 kl_loss: 1826.8624267578125\n",
      "epoch: 16 iter: 117 reconn_loss: 9189.5068359375 kl_loss: 1838.9658203125\n",
      "epoch: 16 iter: 118 reconn_loss: 9190.9599609375 kl_loss: 1864.301025390625\n",
      "epoch: 16 iter: 119 reconn_loss: 9585.943359375 kl_loss: 1865.0709228515625\n",
      "epoch: 16 iter: 120 reconn_loss: 8987.412109375 kl_loss: 1819.1873779296875\n",
      "epoch: 16 iter: 121 reconn_loss: 9247.3125 kl_loss: 1807.4775390625\n",
      "epoch: 16 iter: 122 reconn_loss: 9395.0361328125 kl_loss: 1829.355224609375\n",
      "epoch: 16 iter: 123 reconn_loss: 9250.451171875 kl_loss: 1870.119873046875\n",
      "epoch: 16 iter: 124 reconn_loss: 9259.5595703125 kl_loss: 1864.4520263671875\n",
      "epoch: 16 iter: 125 reconn_loss: 9234.19140625 kl_loss: 1800.8428955078125\n",
      "epoch: 16 iter: 126 reconn_loss: 8972.2880859375 kl_loss: 1843.763916015625\n",
      "epoch: 16 iter: 127 reconn_loss: 9141.0966796875 kl_loss: 1839.33056640625\n",
      "epoch: 16 iter: 128 reconn_loss: 9253.5986328125 kl_loss: 1854.6168212890625\n",
      "epoch: 16 iter: 129 reconn_loss: 9293.0 kl_loss: 1844.1207275390625\n",
      "epoch: 16 iter: 130 reconn_loss: 9239.8876953125 kl_loss: 1865.980712890625\n",
      "epoch: 16 iter: 131 reconn_loss: 9275.2392578125 kl_loss: 1858.358642578125\n",
      "epoch: 16 iter: 132 reconn_loss: 9361.517578125 kl_loss: 1792.30078125\n",
      "epoch: 16 iter: 133 reconn_loss: 9157.822265625 kl_loss: 1807.2969970703125\n",
      "epoch: 16 iter: 134 reconn_loss: 8883.943359375 kl_loss: 1801.6368408203125\n",
      "epoch: 16 iter: 135 reconn_loss: 9662.677734375 kl_loss: 1844.0093994140625\n",
      "epoch: 16 iter: 136 reconn_loss: 9186.9638671875 kl_loss: 1842.595947265625\n",
      "epoch: 16 iter: 137 reconn_loss: 8831.7548828125 kl_loss: 1876.290771484375\n",
      "epoch: 16 iter: 138 reconn_loss: 9020.57421875 kl_loss: 1832.22265625\n",
      "epoch: 16 iter: 139 reconn_loss: 9270.7490234375 kl_loss: 1855.417236328125\n",
      "epoch: 16 iter: 140 reconn_loss: 9186.390625 kl_loss: 1849.47509765625\n",
      "epoch: 16 iter: 141 reconn_loss: 9103.1396484375 kl_loss: 1830.3802490234375\n",
      "epoch: 16 iter: 142 reconn_loss: 9294.912109375 kl_loss: 1831.3857421875\n",
      "epoch: 16 iter: 143 reconn_loss: 9094.13671875 kl_loss: 1849.1702880859375\n",
      "epoch: 16 iter: 144 reconn_loss: 9110.2705078125 kl_loss: 1839.736572265625\n",
      "epoch: 16 iter: 145 reconn_loss: 9080.3076171875 kl_loss: 1829.5606689453125\n",
      "epoch: 16 iter: 146 reconn_loss: 8871.390625 kl_loss: 1836.3944091796875\n",
      "epoch: 16 iter: 147 reconn_loss: 9082.6416015625 kl_loss: 1850.6102294921875\n",
      "epoch: 16 iter: 148 reconn_loss: 9236.4833984375 kl_loss: 1818.8883056640625\n",
      "epoch: 16 iter: 149 reconn_loss: 9452.654296875 kl_loss: 1851.6728515625\n",
      "epoch: 16 iter: 150 reconn_loss: 9032.84765625 kl_loss: 1822.7930908203125\n",
      "epoch: 16 iter: 151 reconn_loss: 9077.095703125 kl_loss: 1863.6605224609375\n",
      "epoch: 16 iter: 152 reconn_loss: 9197.3740234375 kl_loss: 1826.02392578125\n",
      "epoch: 16 iter: 153 reconn_loss: 9179.0908203125 kl_loss: 1818.1810302734375\n",
      "epoch: 16 iter: 154 reconn_loss: 9095.466796875 kl_loss: 1853.93994140625\n",
      "epoch: 16 iter: 155 reconn_loss: 9061.861328125 kl_loss: 1840.06103515625\n",
      "epoch: 16 iter: 156 reconn_loss: 9324.3408203125 kl_loss: 1865.1954345703125\n",
      "epoch: 16 iter: 157 reconn_loss: 8860.423828125 kl_loss: 1850.734130859375\n",
      "epoch: 16 iter: 158 reconn_loss: 8896.4990234375 kl_loss: 1843.0782470703125\n",
      "epoch: 16 iter: 159 reconn_loss: 8989.388671875 kl_loss: 1852.83935546875\n",
      "epoch: 16 iter: 160 reconn_loss: 9176.8583984375 kl_loss: 1847.9703369140625\n",
      "epoch: 16 iter: 161 reconn_loss: 9084.47265625 kl_loss: 1874.103759765625\n",
      "epoch: 16 iter: 162 reconn_loss: 9260.40625 kl_loss: 1863.469970703125\n",
      "epoch: 16 iter: 163 reconn_loss: 8873.4267578125 kl_loss: 1837.4617919921875\n",
      "epoch: 16 iter: 164 reconn_loss: 9265.0546875 kl_loss: 1846.1524658203125\n",
      "epoch: 16 iter: 165 reconn_loss: 9025.9775390625 kl_loss: 1799.064208984375\n",
      "epoch: 16 iter: 166 reconn_loss: 8794.267578125 kl_loss: 1808.1849365234375\n",
      "epoch: 16 iter: 167 reconn_loss: 8837.4658203125 kl_loss: 1877.598388671875\n",
      "epoch: 16 iter: 168 reconn_loss: 9088.7109375 kl_loss: 1860.94970703125\n",
      "epoch: 16 iter: 169 reconn_loss: 9017.2255859375 kl_loss: 1848.5810546875\n",
      "epoch: 16 iter: 170 reconn_loss: 8938.5537109375 kl_loss: 1854.7664794921875\n",
      "epoch: 16 iter: 171 reconn_loss: 9320.03125 kl_loss: 1875.205078125\n",
      "epoch: 16 iter: 172 reconn_loss: 9240.1435546875 kl_loss: 1892.0831298828125\n",
      "epoch: 16 iter: 173 reconn_loss: 9247.5126953125 kl_loss: 1849.121826171875\n",
      "epoch: 16 iter: 174 reconn_loss: 9318.66015625 kl_loss: 1835.2230224609375\n",
      "epoch: 16 iter: 175 reconn_loss: 9268.939453125 kl_loss: 1877.95166015625\n",
      "epoch: 16 iter: 176 reconn_loss: 9198.0068359375 kl_loss: 1851.199462890625\n",
      "epoch: 16 iter: 177 reconn_loss: 8769.224609375 kl_loss: 1824.72705078125\n",
      "epoch: 16 iter: 178 reconn_loss: 8919.8203125 kl_loss: 1814.414306640625\n",
      "epoch: 16 iter: 179 reconn_loss: 9539.9814453125 kl_loss: 1841.6671142578125\n",
      "epoch: 16 iter: 180 reconn_loss: 8863.7890625 kl_loss: 1825.9437255859375\n",
      "epoch: 16 iter: 181 reconn_loss: 9231.0693359375 kl_loss: 1852.7518310546875\n",
      "epoch: 16 iter: 182 reconn_loss: 9163.564453125 kl_loss: 1828.7550048828125\n",
      "epoch: 16 iter: 183 reconn_loss: 8985.0654296875 kl_loss: 1845.26416015625\n",
      "epoch: 16 iter: 184 reconn_loss: 8928.7119140625 kl_loss: 1858.551513671875\n",
      "epoch: 16 iter: 185 reconn_loss: 8827.3955078125 kl_loss: 1853.8665771484375\n",
      "epoch: 16 iter: 186 reconn_loss: 8972.1455078125 kl_loss: 1856.111572265625\n",
      "epoch: 16 iter: 187 reconn_loss: 9150.9697265625 kl_loss: 1847.1392822265625\n",
      "epoch: 16 iter: 188 reconn_loss: 8963.7734375 kl_loss: 1878.54931640625\n",
      "epoch: 16 iter: 189 reconn_loss: 8924.5068359375 kl_loss: 1869.805908203125\n",
      "epoch: 16 iter: 190 reconn_loss: 9322.2294921875 kl_loss: 1874.697265625\n",
      "epoch: 16 iter: 191 reconn_loss: 9059.3955078125 kl_loss: 1836.5435791015625\n",
      "epoch: 16 iter: 192 reconn_loss: 9273.783203125 kl_loss: 1870.291748046875\n",
      "epoch: 16 iter: 193 reconn_loss: 9174.4482421875 kl_loss: 1848.1796875\n",
      "epoch: 16 iter: 194 reconn_loss: 9068.7822265625 kl_loss: 1834.8262939453125\n",
      "epoch: 16 iter: 195 reconn_loss: 9182.42578125 kl_loss: 1863.361572265625\n",
      "epoch: 16 iter: 196 reconn_loss: 9051.119140625 kl_loss: 1816.96240234375\n",
      "epoch: 16 iter: 197 reconn_loss: 9423.5703125 kl_loss: 1848.0748291015625\n",
      "epoch: 16 iter: 198 reconn_loss: 9206.076171875 kl_loss: 1847.4521484375\n",
      "epoch: 16 iter: 199 reconn_loss: 9045.2080078125 kl_loss: 1842.9947509765625\n",
      "epoch: 16 iter: 200 reconn_loss: 8918.6328125 kl_loss: 1873.740234375\n",
      "epoch: 16 iter: 201 reconn_loss: 8998.0048828125 kl_loss: 1817.686767578125\n",
      "epoch: 16 iter: 202 reconn_loss: 9549.11328125 kl_loss: 1888.97802734375\n",
      "epoch: 16 iter: 203 reconn_loss: 9512.40625 kl_loss: 1859.905517578125\n",
      "epoch: 16 iter: 204 reconn_loss: 9151.5595703125 kl_loss: 1839.2711181640625\n",
      "epoch: 16 iter: 205 reconn_loss: 8849.296875 kl_loss: 1813.1038818359375\n",
      "epoch: 16 iter: 206 reconn_loss: 9059.7099609375 kl_loss: 1810.489013671875\n",
      "epoch: 16 iter: 207 reconn_loss: 9216.0517578125 kl_loss: 1869.1502685546875\n",
      "epoch: 16 iter: 208 reconn_loss: 9091.61328125 kl_loss: 1859.466796875\n",
      "epoch: 16 iter: 209 reconn_loss: 8875.3935546875 kl_loss: 1840.7430419921875\n",
      "epoch: 16 iter: 210 reconn_loss: 9034.4501953125 kl_loss: 1823.86767578125\n",
      "epoch: 16 iter: 211 reconn_loss: 9289.1630859375 kl_loss: 1824.19287109375\n",
      "epoch: 16 iter: 212 reconn_loss: 9188.5947265625 kl_loss: 1838.931396484375\n",
      "epoch: 16 iter: 213 reconn_loss: 9306.615234375 kl_loss: 1851.7911376953125\n",
      "epoch: 16 iter: 214 reconn_loss: 9202.736328125 kl_loss: 1890.2235107421875\n",
      "epoch: 16 iter: 215 reconn_loss: 9297.34765625 kl_loss: 1831.1265869140625\n",
      "epoch: 16 iter: 216 reconn_loss: 8926.212890625 kl_loss: 1857.2606201171875\n",
      "epoch: 16 iter: 217 reconn_loss: 9161.083984375 kl_loss: 1831.0548095703125\n",
      "epoch: 16 iter: 218 reconn_loss: 9008.6845703125 kl_loss: 1903.212890625\n",
      "epoch: 16 iter: 219 reconn_loss: 9310.115234375 kl_loss: 1871.024169921875\n",
      "epoch: 16 iter: 220 reconn_loss: 8987.38671875 kl_loss: 1876.8927001953125\n",
      "epoch: 16 iter: 221 reconn_loss: 8706.380859375 kl_loss: 1854.870849609375\n",
      "epoch: 16 iter: 222 reconn_loss: 5921.4560546875 kl_loss: 1198.7703857421875\n",
      "0.weight tensor(65.1921) tensor(-59.0496)\n",
      "0.bias tensor(39.4169) tensor(-23.8592)\n",
      "2.weight tensor(20.7472) tensor(-16.1050)\n",
      "2.bias tensor(11.4221) tensor(-7.0923)\n",
      "4.weight tensor(9.2947) tensor(-11.2470)\n",
      "4.bias tensor(4.0236) tensor(-4.4351)\n",
      "epoch: 17 iter: 0 reconn_loss: 9054.3759765625 kl_loss: 1872.3619384765625\n",
      "epoch: 17 iter: 1 reconn_loss: 9610.6357421875 kl_loss: 1861.205810546875\n",
      "epoch: 17 iter: 2 reconn_loss: 8995.9580078125 kl_loss: 1844.5555419921875\n",
      "epoch: 17 iter: 3 reconn_loss: 9125.158203125 kl_loss: 1885.041259765625\n",
      "epoch: 17 iter: 4 reconn_loss: 9193.69140625 kl_loss: 1845.457275390625\n",
      "epoch: 17 iter: 5 reconn_loss: 8881.0703125 kl_loss: 1832.800537109375\n",
      "epoch: 17 iter: 6 reconn_loss: 8984.294921875 kl_loss: 1870.5789794921875\n",
      "epoch: 17 iter: 7 reconn_loss: 8792.431640625 kl_loss: 1912.8463134765625\n",
      "epoch: 17 iter: 8 reconn_loss: 9101.90234375 kl_loss: 1904.3653564453125\n",
      "epoch: 17 iter: 9 reconn_loss: 8779.64453125 kl_loss: 1877.4541015625\n",
      "epoch: 17 iter: 10 reconn_loss: 9065.892578125 kl_loss: 1826.664794921875\n",
      "epoch: 17 iter: 11 reconn_loss: 9123.1318359375 kl_loss: 1833.2431640625\n",
      "epoch: 17 iter: 12 reconn_loss: 9434.498046875 kl_loss: 1842.413330078125\n",
      "epoch: 17 iter: 13 reconn_loss: 9218.46875 kl_loss: 1831.6248779296875\n",
      "epoch: 17 iter: 14 reconn_loss: 9108.615234375 kl_loss: 1856.857421875\n",
      "epoch: 17 iter: 15 reconn_loss: 9125.865234375 kl_loss: 1847.6610107421875\n",
      "epoch: 17 iter: 16 reconn_loss: 9059.0849609375 kl_loss: 1822.334228515625\n",
      "epoch: 17 iter: 17 reconn_loss: 9233.5830078125 kl_loss: 1847.755126953125\n",
      "epoch: 17 iter: 18 reconn_loss: 9029.0048828125 kl_loss: 1826.8056640625\n",
      "epoch: 17 iter: 19 reconn_loss: 9103.3251953125 kl_loss: 1829.7630615234375\n",
      "epoch: 17 iter: 20 reconn_loss: 9139.609375 kl_loss: 1846.70263671875\n",
      "epoch: 17 iter: 21 reconn_loss: 9140.4189453125 kl_loss: 1865.453369140625\n",
      "epoch: 17 iter: 22 reconn_loss: 8948.599609375 kl_loss: 1820.69873046875\n",
      "epoch: 17 iter: 23 reconn_loss: 9284.427734375 kl_loss: 1862.77880859375\n",
      "epoch: 17 iter: 24 reconn_loss: 9137.0546875 kl_loss: 1832.6387939453125\n",
      "epoch: 17 iter: 25 reconn_loss: 9082.7734375 kl_loss: 1861.849365234375\n",
      "epoch: 17 iter: 26 reconn_loss: 8846.341796875 kl_loss: 1847.69189453125\n",
      "epoch: 17 iter: 27 reconn_loss: 9062.7568359375 kl_loss: 1892.7728271484375\n",
      "epoch: 17 iter: 28 reconn_loss: 8882.3798828125 kl_loss: 1833.767333984375\n",
      "epoch: 17 iter: 29 reconn_loss: 8928.15234375 kl_loss: 1859.6181640625\n",
      "epoch: 17 iter: 30 reconn_loss: 9345.3212890625 kl_loss: 1881.2032470703125\n",
      "epoch: 17 iter: 31 reconn_loss: 9368.8515625 kl_loss: 1843.821044921875\n",
      "epoch: 17 iter: 32 reconn_loss: 9211.5166015625 kl_loss: 1821.211669921875\n",
      "epoch: 17 iter: 33 reconn_loss: 8910.111328125 kl_loss: 1864.462890625\n",
      "epoch: 17 iter: 34 reconn_loss: 8925.1533203125 kl_loss: 1832.283447265625\n",
      "epoch: 17 iter: 35 reconn_loss: 9499.625 kl_loss: 1869.18212890625\n",
      "epoch: 17 iter: 36 reconn_loss: 8902.462890625 kl_loss: 1875.8731689453125\n",
      "epoch: 17 iter: 37 reconn_loss: 8942.484375 kl_loss: 1831.607421875\n",
      "epoch: 17 iter: 38 reconn_loss: 9474.6767578125 kl_loss: 1865.7872314453125\n",
      "epoch: 17 iter: 39 reconn_loss: 8911.884765625 kl_loss: 1848.724853515625\n",
      "epoch: 17 iter: 40 reconn_loss: 9042.185546875 kl_loss: 1850.593505859375\n",
      "epoch: 17 iter: 41 reconn_loss: 9167.177734375 kl_loss: 1847.83740234375\n",
      "epoch: 17 iter: 42 reconn_loss: 8803.0244140625 kl_loss: 1849.9405517578125\n",
      "epoch: 17 iter: 43 reconn_loss: 9183.849609375 kl_loss: 1855.3763427734375\n",
      "epoch: 17 iter: 44 reconn_loss: 9209.3564453125 kl_loss: 1817.850341796875\n",
      "epoch: 17 iter: 45 reconn_loss: 8896.6259765625 kl_loss: 1796.3958740234375\n",
      "epoch: 17 iter: 46 reconn_loss: 9234.86328125 kl_loss: 1862.277099609375\n",
      "epoch: 17 iter: 47 reconn_loss: 9086.8857421875 kl_loss: 1846.608154296875\n",
      "epoch: 17 iter: 48 reconn_loss: 8993.9658203125 kl_loss: 1874.149169921875\n",
      "epoch: 17 iter: 49 reconn_loss: 9119.3603515625 kl_loss: 1835.96875\n",
      "epoch: 17 iter: 50 reconn_loss: 9024.57421875 kl_loss: 1868.284423828125\n",
      "epoch: 17 iter: 51 reconn_loss: 9151.646484375 kl_loss: 1804.53076171875\n",
      "epoch: 17 iter: 52 reconn_loss: 8796.9755859375 kl_loss: 1846.3892822265625\n",
      "epoch: 17 iter: 53 reconn_loss: 9342.263671875 kl_loss: 1851.9256591796875\n",
      "epoch: 17 iter: 54 reconn_loss: 9143.7880859375 kl_loss: 1842.1768798828125\n",
      "epoch: 17 iter: 55 reconn_loss: 9040.2119140625 kl_loss: 1822.751220703125\n",
      "epoch: 17 iter: 56 reconn_loss: 8968.595703125 kl_loss: 1826.4837646484375\n",
      "epoch: 17 iter: 57 reconn_loss: 8880.9453125 kl_loss: 1846.1865234375\n",
      "epoch: 17 iter: 58 reconn_loss: 9118.1533203125 kl_loss: 1799.5477294921875\n",
      "epoch: 17 iter: 59 reconn_loss: 9009.958984375 kl_loss: 1890.589599609375\n",
      "epoch: 17 iter: 60 reconn_loss: 9104.5126953125 kl_loss: 1839.2470703125\n",
      "epoch: 17 iter: 61 reconn_loss: 9498.8408203125 kl_loss: 1832.4962158203125\n",
      "epoch: 17 iter: 62 reconn_loss: 8931.5771484375 kl_loss: 1801.139404296875\n",
      "epoch: 17 iter: 63 reconn_loss: 9156.619140625 kl_loss: 1773.2186279296875\n",
      "epoch: 17 iter: 64 reconn_loss: 9220.6904296875 kl_loss: 1836.546875\n",
      "epoch: 17 iter: 65 reconn_loss: 8954.0595703125 kl_loss: 1840.8759765625\n",
      "epoch: 17 iter: 66 reconn_loss: 9027.8583984375 kl_loss: 1858.6044921875\n",
      "epoch: 17 iter: 67 reconn_loss: 9245.451171875 kl_loss: 1906.94580078125\n",
      "epoch: 17 iter: 68 reconn_loss: 9088.30078125 kl_loss: 1883.2579345703125\n",
      "epoch: 17 iter: 69 reconn_loss: 9130.046875 kl_loss: 1860.1181640625\n",
      "epoch: 17 iter: 70 reconn_loss: 9423.7265625 kl_loss: 1892.7371826171875\n",
      "epoch: 17 iter: 71 reconn_loss: 8904.2021484375 kl_loss: 1869.2108154296875\n",
      "epoch: 17 iter: 72 reconn_loss: 9189.720703125 kl_loss: 1848.2454833984375\n",
      "epoch: 17 iter: 73 reconn_loss: 8941.232421875 kl_loss: 1847.7421875\n",
      "epoch: 17 iter: 74 reconn_loss: 9389.6455078125 kl_loss: 1861.4317626953125\n",
      "epoch: 17 iter: 75 reconn_loss: 9297.2587890625 kl_loss: 1864.728271484375\n",
      "epoch: 17 iter: 76 reconn_loss: 9267.9326171875 kl_loss: 1869.9111328125\n",
      "epoch: 17 iter: 77 reconn_loss: 8927.990234375 kl_loss: 1864.047119140625\n",
      "epoch: 17 iter: 78 reconn_loss: 9164.2216796875 kl_loss: 1852.560302734375\n",
      "epoch: 17 iter: 79 reconn_loss: 9257.4189453125 kl_loss: 1815.3353271484375\n",
      "epoch: 17 iter: 80 reconn_loss: 9134.4560546875 kl_loss: 1844.9234619140625\n",
      "epoch: 17 iter: 81 reconn_loss: 9009.62109375 kl_loss: 1844.6317138671875\n",
      "epoch: 17 iter: 82 reconn_loss: 8871.5869140625 kl_loss: 1819.387939453125\n",
      "epoch: 17 iter: 83 reconn_loss: 9145.4013671875 kl_loss: 1816.6319580078125\n",
      "epoch: 17 iter: 84 reconn_loss: 9245.5849609375 kl_loss: 1817.6417236328125\n",
      "epoch: 17 iter: 85 reconn_loss: 8671.1259765625 kl_loss: 1860.350830078125\n",
      "epoch: 17 iter: 86 reconn_loss: 8877.775390625 kl_loss: 1851.1029052734375\n",
      "epoch: 17 iter: 87 reconn_loss: 9207.591796875 kl_loss: 1911.300537109375\n",
      "epoch: 17 iter: 88 reconn_loss: 8895.8984375 kl_loss: 1911.357421875\n",
      "epoch: 17 iter: 89 reconn_loss: 9114.2880859375 kl_loss: 1831.5230712890625\n",
      "epoch: 17 iter: 90 reconn_loss: 9193.712890625 kl_loss: 1888.4613037109375\n",
      "epoch: 17 iter: 91 reconn_loss: 9146.466796875 kl_loss: 1863.1514892578125\n",
      "epoch: 17 iter: 92 reconn_loss: 8839.4892578125 kl_loss: 1862.0137939453125\n",
      "epoch: 17 iter: 93 reconn_loss: 8807.1806640625 kl_loss: 1840.1920166015625\n",
      "epoch: 17 iter: 94 reconn_loss: 9252.953125 kl_loss: 1812.45703125\n",
      "epoch: 17 iter: 95 reconn_loss: 9178.6904296875 kl_loss: 1916.3116455078125\n",
      "epoch: 17 iter: 96 reconn_loss: 8947.419921875 kl_loss: 1927.7962646484375\n",
      "epoch: 17 iter: 97 reconn_loss: 9043.08984375 kl_loss: 1824.6488037109375\n",
      "epoch: 17 iter: 98 reconn_loss: 9419.3583984375 kl_loss: 1868.4693603515625\n",
      "epoch: 17 iter: 99 reconn_loss: 9296.1376953125 kl_loss: 1843.436767578125\n",
      "epoch: 17 iter: 100 reconn_loss: 9236.607421875 kl_loss: 1879.2039794921875\n",
      "epoch: 17 iter: 101 reconn_loss: 8955.4375 kl_loss: 1882.0947265625\n",
      "epoch: 17 iter: 102 reconn_loss: 9164.1171875 kl_loss: 1885.04150390625\n",
      "epoch: 17 iter: 103 reconn_loss: 9460.4501953125 kl_loss: 1915.459716796875\n",
      "epoch: 17 iter: 104 reconn_loss: 9156.0927734375 kl_loss: 1853.804931640625\n",
      "epoch: 17 iter: 105 reconn_loss: 9066.25 kl_loss: 1848.4310302734375\n",
      "epoch: 17 iter: 106 reconn_loss: 9289.380859375 kl_loss: 1873.61083984375\n",
      "epoch: 17 iter: 107 reconn_loss: 8657.4951171875 kl_loss: 1885.0865478515625\n",
      "epoch: 17 iter: 108 reconn_loss: 8849.5322265625 kl_loss: 1853.40283203125\n",
      "epoch: 17 iter: 109 reconn_loss: 8961.537109375 kl_loss: 1819.97705078125\n",
      "epoch: 17 iter: 110 reconn_loss: 9231.466796875 kl_loss: 1825.7470703125\n",
      "epoch: 17 iter: 111 reconn_loss: 9100.123046875 kl_loss: 1842.344482421875\n",
      "epoch: 17 iter: 112 reconn_loss: 9088.66015625 kl_loss: 1851.9892578125\n",
      "epoch: 17 iter: 113 reconn_loss: 9131.0732421875 kl_loss: 1866.6275634765625\n",
      "epoch: 17 iter: 114 reconn_loss: 9010.5517578125 kl_loss: 1864.7598876953125\n",
      "epoch: 17 iter: 115 reconn_loss: 9068.447265625 kl_loss: 1838.3477783203125\n",
      "epoch: 17 iter: 116 reconn_loss: 9097.5068359375 kl_loss: 1855.513427734375\n",
      "epoch: 17 iter: 117 reconn_loss: 9146.330078125 kl_loss: 1859.5557861328125\n",
      "epoch: 17 iter: 118 reconn_loss: 8981.421875 kl_loss: 1857.03125\n",
      "epoch: 17 iter: 119 reconn_loss: 8892.33203125 kl_loss: 1827.093017578125\n",
      "epoch: 17 iter: 120 reconn_loss: 9038.7451171875 kl_loss: 1846.4161376953125\n",
      "epoch: 17 iter: 121 reconn_loss: 9228.0419921875 kl_loss: 1886.6031494140625\n",
      "epoch: 17 iter: 122 reconn_loss: 9408.7314453125 kl_loss: 1864.9154052734375\n",
      "epoch: 17 iter: 123 reconn_loss: 9309.068359375 kl_loss: 1857.583740234375\n",
      "epoch: 17 iter: 124 reconn_loss: 9269.4130859375 kl_loss: 1881.0472412109375\n",
      "epoch: 17 iter: 125 reconn_loss: 8940.94140625 kl_loss: 1841.54931640625\n",
      "epoch: 17 iter: 126 reconn_loss: 9391.8974609375 kl_loss: 1923.562255859375\n",
      "epoch: 17 iter: 127 reconn_loss: 9086.2919921875 kl_loss: 1814.713134765625\n",
      "epoch: 17 iter: 128 reconn_loss: 9027.2626953125 kl_loss: 1841.17626953125\n",
      "epoch: 17 iter: 129 reconn_loss: 9207.08203125 kl_loss: 1884.03173828125\n",
      "epoch: 17 iter: 130 reconn_loss: 9223.5341796875 kl_loss: 1848.3231201171875\n",
      "epoch: 17 iter: 131 reconn_loss: 8944.9404296875 kl_loss: 1827.27685546875\n",
      "epoch: 17 iter: 132 reconn_loss: 9288.818359375 kl_loss: 1865.1920166015625\n",
      "epoch: 17 iter: 133 reconn_loss: 9201.7763671875 kl_loss: 1837.8013916015625\n",
      "epoch: 17 iter: 134 reconn_loss: 9014.6875 kl_loss: 1859.2984619140625\n",
      "epoch: 17 iter: 135 reconn_loss: 8830.5703125 kl_loss: 1871.0340576171875\n",
      "epoch: 17 iter: 136 reconn_loss: 9334.5546875 kl_loss: 1872.65234375\n",
      "epoch: 17 iter: 137 reconn_loss: 9000.142578125 kl_loss: 1842.3980712890625\n",
      "epoch: 17 iter: 138 reconn_loss: 8974.6884765625 kl_loss: 1834.8109130859375\n",
      "epoch: 17 iter: 139 reconn_loss: 9217.48046875 kl_loss: 1861.28125\n",
      "epoch: 17 iter: 140 reconn_loss: 8920.5029296875 kl_loss: 1853.390380859375\n",
      "epoch: 17 iter: 141 reconn_loss: 9045.0546875 kl_loss: 1884.035888671875\n",
      "epoch: 17 iter: 142 reconn_loss: 9029.53125 kl_loss: 1860.6396484375\n",
      "epoch: 17 iter: 143 reconn_loss: 9177.5166015625 kl_loss: 1847.5869140625\n",
      "epoch: 17 iter: 144 reconn_loss: 9252.884765625 kl_loss: 1868.351806640625\n",
      "epoch: 17 iter: 145 reconn_loss: 9140.0361328125 kl_loss: 1797.4869384765625\n",
      "epoch: 17 iter: 146 reconn_loss: 9051.79296875 kl_loss: 1858.4385986328125\n",
      "epoch: 17 iter: 147 reconn_loss: 9508.9541015625 kl_loss: 1877.2088623046875\n",
      "epoch: 17 iter: 148 reconn_loss: 9006.08984375 kl_loss: 1842.2362060546875\n",
      "epoch: 17 iter: 149 reconn_loss: 9182.10546875 kl_loss: 1893.59521484375\n",
      "epoch: 17 iter: 150 reconn_loss: 8942.7021484375 kl_loss: 1920.5501708984375\n",
      "epoch: 17 iter: 151 reconn_loss: 8930.455078125 kl_loss: 1955.4542236328125\n",
      "epoch: 17 iter: 152 reconn_loss: 9301.42578125 kl_loss: 1853.921142578125\n",
      "epoch: 17 iter: 153 reconn_loss: 9033.091796875 kl_loss: 1844.4215087890625\n",
      "epoch: 17 iter: 154 reconn_loss: 8502.35546875 kl_loss: 1818.347412109375\n",
      "epoch: 17 iter: 155 reconn_loss: 9431.109375 kl_loss: 1875.17236328125\n",
      "epoch: 17 iter: 156 reconn_loss: 9202.783203125 kl_loss: 1874.2108154296875\n",
      "epoch: 17 iter: 157 reconn_loss: 8990.576171875 kl_loss: 1864.3931884765625\n",
      "epoch: 17 iter: 158 reconn_loss: 8733.517578125 kl_loss: 1817.948974609375\n",
      "epoch: 17 iter: 159 reconn_loss: 9237.439453125 kl_loss: 1879.51171875\n",
      "epoch: 17 iter: 160 reconn_loss: 8731.287109375 kl_loss: 1861.3236083984375\n",
      "epoch: 17 iter: 161 reconn_loss: 8983.8046875 kl_loss: 1814.566162109375\n",
      "epoch: 17 iter: 162 reconn_loss: 9368.724609375 kl_loss: 1827.9169921875\n",
      "epoch: 17 iter: 163 reconn_loss: 8956.138671875 kl_loss: 1868.6689453125\n",
      "epoch: 17 iter: 164 reconn_loss: 9276.453125 kl_loss: 1869.2216796875\n",
      "epoch: 17 iter: 165 reconn_loss: 9312.515625 kl_loss: 1820.857666015625\n",
      "epoch: 17 iter: 166 reconn_loss: 9399.2392578125 kl_loss: 1869.9266357421875\n",
      "epoch: 17 iter: 167 reconn_loss: 9006.22265625 kl_loss: 1878.5819091796875\n",
      "epoch: 17 iter: 168 reconn_loss: 9133.5322265625 kl_loss: 1919.7352294921875\n",
      "epoch: 17 iter: 169 reconn_loss: 9037.138671875 kl_loss: 1846.151611328125\n",
      "epoch: 17 iter: 170 reconn_loss: 9168.2978515625 kl_loss: 1827.3106689453125\n",
      "epoch: 17 iter: 171 reconn_loss: 9367.23046875 kl_loss: 1864.372802734375\n",
      "epoch: 17 iter: 172 reconn_loss: 9099.173828125 kl_loss: 1852.94482421875\n",
      "epoch: 17 iter: 173 reconn_loss: 8959.0419921875 kl_loss: 1878.229736328125\n",
      "epoch: 17 iter: 174 reconn_loss: 8647.53125 kl_loss: 1856.62890625\n",
      "epoch: 17 iter: 175 reconn_loss: 8825.6708984375 kl_loss: 1911.1871337890625\n",
      "epoch: 17 iter: 176 reconn_loss: 9389.662109375 kl_loss: 1881.689453125\n",
      "epoch: 17 iter: 177 reconn_loss: 8840.5390625 kl_loss: 1853.782958984375\n",
      "epoch: 17 iter: 178 reconn_loss: 8808.91015625 kl_loss: 1867.015869140625\n",
      "epoch: 17 iter: 179 reconn_loss: 8907.6337890625 kl_loss: 1871.0609130859375\n",
      "epoch: 17 iter: 180 reconn_loss: 9305.6591796875 kl_loss: 1861.466064453125\n",
      "epoch: 17 iter: 181 reconn_loss: 9363.974609375 kl_loss: 1934.0240478515625\n",
      "epoch: 17 iter: 182 reconn_loss: 9081.5859375 kl_loss: 1878.3861083984375\n",
      "epoch: 17 iter: 183 reconn_loss: 9033.4140625 kl_loss: 1867.7777099609375\n",
      "epoch: 17 iter: 184 reconn_loss: 8743.33984375 kl_loss: 1870.1976318359375\n",
      "epoch: 17 iter: 185 reconn_loss: 9233.71875 kl_loss: 1847.783447265625\n",
      "epoch: 17 iter: 186 reconn_loss: 9078.30859375 kl_loss: 1888.191162109375\n",
      "epoch: 17 iter: 187 reconn_loss: 9020.4619140625 kl_loss: 1845.3912353515625\n",
      "epoch: 17 iter: 188 reconn_loss: 9262.490234375 kl_loss: 1831.42822265625\n",
      "epoch: 17 iter: 189 reconn_loss: 9134.0263671875 kl_loss: 1844.04833984375\n",
      "epoch: 17 iter: 190 reconn_loss: 8957.8125 kl_loss: 1868.76904296875\n",
      "epoch: 17 iter: 191 reconn_loss: 9084.2451171875 kl_loss: 1806.554443359375\n",
      "epoch: 17 iter: 192 reconn_loss: 8634.5908203125 kl_loss: 1810.230224609375\n",
      "epoch: 17 iter: 193 reconn_loss: 9223.5751953125 kl_loss: 1860.8536376953125\n",
      "epoch: 17 iter: 194 reconn_loss: 8654.375 kl_loss: 1844.508056640625\n",
      "epoch: 17 iter: 195 reconn_loss: 9197.2548828125 kl_loss: 1850.305908203125\n",
      "epoch: 17 iter: 196 reconn_loss: 9448.9580078125 kl_loss: 1882.740234375\n",
      "epoch: 17 iter: 197 reconn_loss: 8999.673828125 kl_loss: 1861.0816650390625\n",
      "epoch: 17 iter: 198 reconn_loss: 8992.142578125 kl_loss: 1881.5145263671875\n",
      "epoch: 17 iter: 199 reconn_loss: 8785.134765625 kl_loss: 1844.1181640625\n",
      "epoch: 17 iter: 200 reconn_loss: 9024.400390625 kl_loss: 1845.472900390625\n",
      "epoch: 17 iter: 201 reconn_loss: 9025.66796875 kl_loss: 1886.177490234375\n",
      "epoch: 17 iter: 202 reconn_loss: 9133.333984375 kl_loss: 1835.3729248046875\n",
      "epoch: 17 iter: 203 reconn_loss: 8890.1708984375 kl_loss: 1848.3380126953125\n",
      "epoch: 17 iter: 204 reconn_loss: 9029.9755859375 kl_loss: 1874.9703369140625\n",
      "epoch: 17 iter: 205 reconn_loss: 8942.470703125 kl_loss: 1891.196044921875\n",
      "epoch: 17 iter: 206 reconn_loss: 9490.0712890625 kl_loss: 1854.9129638671875\n",
      "epoch: 17 iter: 207 reconn_loss: 8918.73828125 kl_loss: 1875.939697265625\n",
      "epoch: 17 iter: 208 reconn_loss: 9452.3125 kl_loss: 1841.739501953125\n",
      "epoch: 17 iter: 209 reconn_loss: 9226.205078125 kl_loss: 1841.189697265625\n",
      "epoch: 17 iter: 210 reconn_loss: 9428.5107421875 kl_loss: 1925.5552978515625\n",
      "epoch: 17 iter: 211 reconn_loss: 9182.7705078125 kl_loss: 1854.7174072265625\n",
      "epoch: 17 iter: 212 reconn_loss: 9264.2236328125 kl_loss: 1887.9351806640625\n",
      "epoch: 17 iter: 213 reconn_loss: 9070.05078125 kl_loss: 1881.8626708984375\n",
      "epoch: 17 iter: 214 reconn_loss: 9206.8662109375 kl_loss: 1874.4072265625\n",
      "epoch: 17 iter: 215 reconn_loss: 8729.701171875 kl_loss: 1868.1728515625\n",
      "epoch: 17 iter: 216 reconn_loss: 9167.6162109375 kl_loss: 1860.3619384765625\n",
      "epoch: 17 iter: 217 reconn_loss: 8996.4560546875 kl_loss: 1850.915283203125\n",
      "epoch: 17 iter: 218 reconn_loss: 9269.0107421875 kl_loss: 1858.4423828125\n",
      "epoch: 17 iter: 219 reconn_loss: 9018.0712890625 kl_loss: 1912.858642578125\n",
      "epoch: 17 iter: 220 reconn_loss: 9288.2431640625 kl_loss: 1856.982666015625\n",
      "epoch: 17 iter: 221 reconn_loss: 9089.841796875 kl_loss: 1845.3795166015625\n",
      "epoch: 17 iter: 222 reconn_loss: 5872.16943359375 kl_loss: 1191.4798583984375\n",
      "0.weight tensor(64.0023) tensor(-53.7554)\n",
      "0.bias tensor(25.7120) tensor(-31.7942)\n",
      "2.weight tensor(24.9587) tensor(-16.6303)\n",
      "2.bias tensor(11.6223) tensor(-10.3817)\n",
      "4.weight tensor(10.9802) tensor(-11.7267)\n",
      "4.bias tensor(3.7078) tensor(-5.9055)\n",
      "epoch: 18 iter: 0 reconn_loss: 9092.828125 kl_loss: 1907.26708984375\n",
      "epoch: 18 iter: 1 reconn_loss: 9298.283203125 kl_loss: 1904.414794921875\n",
      "epoch: 18 iter: 2 reconn_loss: 8776.138671875 kl_loss: 1856.2218017578125\n",
      "epoch: 18 iter: 3 reconn_loss: 8762.1484375 kl_loss: 1869.41162109375\n",
      "epoch: 18 iter: 4 reconn_loss: 8910.162109375 kl_loss: 1866.359375\n",
      "epoch: 18 iter: 5 reconn_loss: 9041.083984375 kl_loss: 1905.44970703125\n",
      "epoch: 18 iter: 6 reconn_loss: 9493.4296875 kl_loss: 1949.0081787109375\n",
      "epoch: 18 iter: 7 reconn_loss: 9089.51171875 kl_loss: 1859.056884765625\n",
      "epoch: 18 iter: 8 reconn_loss: 9002.4072265625 kl_loss: 1911.1639404296875\n",
      "epoch: 18 iter: 9 reconn_loss: 9071.3720703125 kl_loss: 1919.398681640625\n",
      "epoch: 18 iter: 10 reconn_loss: 9170.45703125 kl_loss: 1890.91162109375\n",
      "epoch: 18 iter: 11 reconn_loss: 9107.76171875 kl_loss: 1906.7591552734375\n",
      "epoch: 18 iter: 12 reconn_loss: 9119.23046875 kl_loss: 1904.155517578125\n",
      "epoch: 18 iter: 13 reconn_loss: 9257.353515625 kl_loss: 1932.83837890625\n",
      "epoch: 18 iter: 14 reconn_loss: 9111.4130859375 kl_loss: 1911.036865234375\n",
      "epoch: 18 iter: 15 reconn_loss: 9007.4296875 kl_loss: 1852.13232421875\n",
      "epoch: 18 iter: 16 reconn_loss: 8867.99609375 kl_loss: 1833.87646484375\n",
      "epoch: 18 iter: 17 reconn_loss: 9108.599609375 kl_loss: 1832.361328125\n",
      "epoch: 18 iter: 18 reconn_loss: 9111.3388671875 kl_loss: 1834.0030517578125\n",
      "epoch: 18 iter: 19 reconn_loss: 9282.125 kl_loss: 1869.2406005859375\n",
      "epoch: 18 iter: 20 reconn_loss: 8922.630859375 kl_loss: 1874.129150390625\n",
      "epoch: 18 iter: 21 reconn_loss: 9217.4013671875 kl_loss: 1878.531982421875\n",
      "epoch: 18 iter: 22 reconn_loss: 9035.58203125 kl_loss: 1855.65869140625\n",
      "epoch: 18 iter: 23 reconn_loss: 9271.01171875 kl_loss: 1867.2388916015625\n",
      "epoch: 18 iter: 24 reconn_loss: 9077.478515625 kl_loss: 1865.7620849609375\n",
      "epoch: 18 iter: 25 reconn_loss: 9157.0986328125 kl_loss: 1881.125732421875\n",
      "epoch: 18 iter: 26 reconn_loss: 9227.1435546875 kl_loss: 1883.267333984375\n",
      "epoch: 18 iter: 27 reconn_loss: 8951.23046875 kl_loss: 1875.9835205078125\n",
      "epoch: 18 iter: 28 reconn_loss: 8928.5625 kl_loss: 1879.52587890625\n",
      "epoch: 18 iter: 29 reconn_loss: 9019.8740234375 kl_loss: 1845.548828125\n",
      "epoch: 18 iter: 30 reconn_loss: 9090.3056640625 kl_loss: 1820.373779296875\n",
      "epoch: 18 iter: 31 reconn_loss: 9131.3994140625 kl_loss: 1914.2357177734375\n",
      "epoch: 18 iter: 32 reconn_loss: 8849.587890625 kl_loss: 1856.7431640625\n",
      "epoch: 18 iter: 33 reconn_loss: 8980.9521484375 kl_loss: 1881.9678955078125\n",
      "epoch: 18 iter: 34 reconn_loss: 9137.0908203125 kl_loss: 1886.27880859375\n",
      "epoch: 18 iter: 35 reconn_loss: 8993.427734375 kl_loss: 1841.112060546875\n",
      "epoch: 18 iter: 36 reconn_loss: 8951.7880859375 kl_loss: 1868.9276123046875\n",
      "epoch: 18 iter: 37 reconn_loss: 9014.64453125 kl_loss: 1882.5654296875\n",
      "epoch: 18 iter: 38 reconn_loss: 9345.88671875 kl_loss: 1900.08447265625\n",
      "epoch: 18 iter: 39 reconn_loss: 8818.013671875 kl_loss: 1848.918701171875\n",
      "epoch: 18 iter: 40 reconn_loss: 9532.2890625 kl_loss: 1909.483642578125\n",
      "epoch: 18 iter: 41 reconn_loss: 8902.1904296875 kl_loss: 1882.9854736328125\n",
      "epoch: 18 iter: 42 reconn_loss: 9074.8095703125 kl_loss: 1855.7359619140625\n",
      "epoch: 18 iter: 43 reconn_loss: 8811.85546875 kl_loss: 1864.537353515625\n",
      "epoch: 18 iter: 44 reconn_loss: 9022.4853515625 kl_loss: 1891.4339599609375\n",
      "epoch: 18 iter: 45 reconn_loss: 9061.2216796875 kl_loss: 1888.059814453125\n",
      "epoch: 18 iter: 46 reconn_loss: 9127.353515625 kl_loss: 1870.42431640625\n",
      "epoch: 18 iter: 47 reconn_loss: 9088.9951171875 kl_loss: 1916.3795166015625\n",
      "epoch: 18 iter: 48 reconn_loss: 9221.3642578125 kl_loss: 1887.525146484375\n",
      "epoch: 18 iter: 49 reconn_loss: 8854.3955078125 kl_loss: 1925.1845703125\n",
      "epoch: 18 iter: 50 reconn_loss: 9059.3203125 kl_loss: 1889.4755859375\n",
      "epoch: 18 iter: 51 reconn_loss: 8870.0068359375 kl_loss: 1865.605224609375\n",
      "epoch: 18 iter: 52 reconn_loss: 8677.8916015625 kl_loss: 1848.4688720703125\n",
      "epoch: 18 iter: 53 reconn_loss: 9005.865234375 kl_loss: 1899.8431396484375\n",
      "epoch: 18 iter: 54 reconn_loss: 9035.662109375 kl_loss: 1905.7000732421875\n",
      "epoch: 18 iter: 55 reconn_loss: 9204.6484375 kl_loss: 1885.585205078125\n",
      "epoch: 18 iter: 56 reconn_loss: 9479.01171875 kl_loss: 1866.2357177734375\n",
      "epoch: 18 iter: 57 reconn_loss: 9112.8779296875 kl_loss: 1845.978271484375\n",
      "epoch: 18 iter: 58 reconn_loss: 9260.1689453125 kl_loss: 1870.8360595703125\n",
      "epoch: 18 iter: 59 reconn_loss: 9116.9951171875 kl_loss: 1932.2431640625\n",
      "epoch: 18 iter: 60 reconn_loss: 9090.1865234375 kl_loss: 1916.973388671875\n",
      "epoch: 18 iter: 61 reconn_loss: 8985.43359375 kl_loss: 1866.5626220703125\n",
      "epoch: 18 iter: 62 reconn_loss: 9214.177734375 kl_loss: 1921.33984375\n",
      "epoch: 18 iter: 63 reconn_loss: 9382.037109375 kl_loss: 1900.19580078125\n",
      "epoch: 18 iter: 64 reconn_loss: 9339.470703125 kl_loss: 1858.676513671875\n",
      "epoch: 18 iter: 65 reconn_loss: 9160.4755859375 kl_loss: 1896.949462890625\n",
      "epoch: 18 iter: 66 reconn_loss: 9159.7216796875 kl_loss: 1891.0965576171875\n",
      "epoch: 18 iter: 67 reconn_loss: 9234.9375 kl_loss: 1881.12744140625\n",
      "epoch: 18 iter: 68 reconn_loss: 9037.0869140625 kl_loss: 1853.8890380859375\n",
      "epoch: 18 iter: 69 reconn_loss: 8772.3828125 kl_loss: 1843.682861328125\n",
      "epoch: 18 iter: 70 reconn_loss: 9101.5908203125 kl_loss: 1848.9505615234375\n",
      "epoch: 18 iter: 71 reconn_loss: 8839.3291015625 kl_loss: 1840.662353515625\n",
      "epoch: 18 iter: 72 reconn_loss: 8980.412109375 kl_loss: 1872.798583984375\n",
      "epoch: 18 iter: 73 reconn_loss: 8917.8984375 kl_loss: 1845.30029296875\n",
      "epoch: 18 iter: 74 reconn_loss: 9169.291015625 kl_loss: 1879.0703125\n",
      "epoch: 18 iter: 75 reconn_loss: 8986.794921875 kl_loss: 1846.4095458984375\n",
      "epoch: 18 iter: 76 reconn_loss: 8912.80859375 kl_loss: 1871.702392578125\n",
      "epoch: 18 iter: 77 reconn_loss: 9157.7412109375 kl_loss: 1841.520263671875\n",
      "epoch: 18 iter: 78 reconn_loss: 9044.1796875 kl_loss: 1841.4443359375\n",
      "epoch: 18 iter: 79 reconn_loss: 9190.9580078125 kl_loss: 1866.365234375\n",
      "epoch: 18 iter: 80 reconn_loss: 9068.46875 kl_loss: 1856.12841796875\n",
      "epoch: 18 iter: 81 reconn_loss: 8819.720703125 kl_loss: 1861.838134765625\n",
      "epoch: 18 iter: 82 reconn_loss: 8985.1142578125 kl_loss: 1863.8912353515625\n",
      "epoch: 18 iter: 83 reconn_loss: 9310.0361328125 kl_loss: 1859.421630859375\n",
      "epoch: 18 iter: 84 reconn_loss: 9315.8779296875 kl_loss: 1819.9263916015625\n",
      "epoch: 18 iter: 85 reconn_loss: 8642.96875 kl_loss: 1869.7852783203125\n",
      "epoch: 18 iter: 86 reconn_loss: 9131.0703125 kl_loss: 1889.615234375\n",
      "epoch: 18 iter: 87 reconn_loss: 9301.333984375 kl_loss: 1882.0986328125\n",
      "epoch: 18 iter: 88 reconn_loss: 8855.7626953125 kl_loss: 1896.85400390625\n",
      "epoch: 18 iter: 89 reconn_loss: 9007.962890625 kl_loss: 1879.250244140625\n",
      "epoch: 18 iter: 90 reconn_loss: 9134.365234375 kl_loss: 1872.0975341796875\n",
      "epoch: 18 iter: 91 reconn_loss: 9034.0791015625 kl_loss: 1901.0328369140625\n",
      "epoch: 18 iter: 92 reconn_loss: 8944.6591796875 kl_loss: 1871.821044921875\n",
      "epoch: 18 iter: 93 reconn_loss: 9162.59765625 kl_loss: 1847.705078125\n",
      "epoch: 18 iter: 94 reconn_loss: 8962.9375 kl_loss: 1890.6229248046875\n",
      "epoch: 18 iter: 95 reconn_loss: 9149.857421875 kl_loss: 1882.2069091796875\n",
      "epoch: 18 iter: 96 reconn_loss: 8704.34765625 kl_loss: 1877.5185546875\n",
      "epoch: 18 iter: 97 reconn_loss: 8758.1875 kl_loss: 1840.665771484375\n",
      "epoch: 18 iter: 98 reconn_loss: 8929.8505859375 kl_loss: 1875.188720703125\n",
      "epoch: 18 iter: 99 reconn_loss: 9053.8681640625 kl_loss: 1880.2806396484375\n",
      "epoch: 18 iter: 100 reconn_loss: 8863.373046875 kl_loss: 1819.7987060546875\n",
      "epoch: 18 iter: 101 reconn_loss: 8883.658203125 kl_loss: 1836.2275390625\n",
      "epoch: 18 iter: 102 reconn_loss: 9190.5302734375 kl_loss: 1835.951904296875\n",
      "epoch: 18 iter: 103 reconn_loss: 8858.951171875 kl_loss: 1836.352294921875\n",
      "epoch: 18 iter: 104 reconn_loss: 9006.4892578125 kl_loss: 1862.712646484375\n",
      "epoch: 18 iter: 105 reconn_loss: 8899.0361328125 kl_loss: 1876.79541015625\n",
      "epoch: 18 iter: 106 reconn_loss: 9440.2529296875 kl_loss: 1865.5057373046875\n",
      "epoch: 18 iter: 107 reconn_loss: 9275.4619140625 kl_loss: 1844.9993896484375\n",
      "epoch: 18 iter: 108 reconn_loss: 9081.845703125 kl_loss: 1888.5106201171875\n",
      "epoch: 18 iter: 109 reconn_loss: 9128.390625 kl_loss: 1836.97607421875\n",
      "epoch: 18 iter: 110 reconn_loss: 8914.197265625 kl_loss: 1899.81396484375\n",
      "epoch: 18 iter: 111 reconn_loss: 9186.0009765625 kl_loss: 1841.7564697265625\n",
      "epoch: 18 iter: 112 reconn_loss: 8897.91015625 kl_loss: 1845.0977783203125\n",
      "epoch: 18 iter: 113 reconn_loss: 9264.6455078125 kl_loss: 1877.875\n",
      "epoch: 18 iter: 114 reconn_loss: 8795.15234375 kl_loss: 1835.825439453125\n",
      "epoch: 18 iter: 115 reconn_loss: 8921.333984375 kl_loss: 1878.1239013671875\n",
      "epoch: 18 iter: 116 reconn_loss: 8940.501953125 kl_loss: 1858.385498046875\n",
      "epoch: 18 iter: 117 reconn_loss: 9010.8134765625 kl_loss: 1879.65625\n",
      "epoch: 18 iter: 118 reconn_loss: 8841.4033203125 kl_loss: 1861.7818603515625\n",
      "epoch: 18 iter: 119 reconn_loss: 8887.41796875 kl_loss: 1861.4822998046875\n",
      "epoch: 18 iter: 120 reconn_loss: 8914.98828125 kl_loss: 1868.732666015625\n",
      "epoch: 18 iter: 121 reconn_loss: 8948.3994140625 kl_loss: 1887.6134033203125\n",
      "epoch: 18 iter: 122 reconn_loss: 9084.091796875 kl_loss: 1877.1142578125\n",
      "epoch: 18 iter: 123 reconn_loss: 8997.212890625 kl_loss: 1851.16650390625\n",
      "epoch: 18 iter: 124 reconn_loss: 9397.7568359375 kl_loss: 1904.739501953125\n",
      "epoch: 18 iter: 125 reconn_loss: 9156.943359375 kl_loss: 1888.4901123046875\n",
      "epoch: 18 iter: 126 reconn_loss: 9017.953125 kl_loss: 1872.69287109375\n",
      "epoch: 18 iter: 127 reconn_loss: 9355.4462890625 kl_loss: 1894.423583984375\n",
      "epoch: 18 iter: 128 reconn_loss: 9316.4052734375 kl_loss: 1884.8145751953125\n",
      "epoch: 18 iter: 129 reconn_loss: 8827.84375 kl_loss: 1878.2327880859375\n",
      "epoch: 18 iter: 130 reconn_loss: 8948.5107421875 kl_loss: 1926.5277099609375\n",
      "epoch: 18 iter: 131 reconn_loss: 9058.28125 kl_loss: 1834.180908203125\n",
      "epoch: 18 iter: 132 reconn_loss: 9087.7607421875 kl_loss: 1844.0218505859375\n",
      "epoch: 18 iter: 133 reconn_loss: 9239.1455078125 kl_loss: 1880.53173828125\n",
      "epoch: 18 iter: 134 reconn_loss: 9186.2626953125 kl_loss: 1908.0484619140625\n",
      "epoch: 18 iter: 135 reconn_loss: 8851.3125 kl_loss: 1842.1219482421875\n",
      "epoch: 18 iter: 136 reconn_loss: 9104.2734375 kl_loss: 1870.9698486328125\n",
      "epoch: 18 iter: 137 reconn_loss: 8604.2216796875 kl_loss: 1881.709228515625\n",
      "epoch: 18 iter: 138 reconn_loss: 9049.681640625 kl_loss: 1879.61767578125\n",
      "epoch: 18 iter: 139 reconn_loss: 9269.275390625 kl_loss: 1889.1357421875\n",
      "epoch: 18 iter: 140 reconn_loss: 9375.513671875 kl_loss: 1890.479736328125\n",
      "epoch: 18 iter: 141 reconn_loss: 9140.412109375 kl_loss: 1889.0947265625\n",
      "epoch: 18 iter: 142 reconn_loss: 9009.03515625 kl_loss: 1909.64404296875\n",
      "epoch: 18 iter: 143 reconn_loss: 8897.4052734375 kl_loss: 1861.7467041015625\n",
      "epoch: 18 iter: 144 reconn_loss: 9258.509765625 kl_loss: 1891.49951171875\n",
      "epoch: 18 iter: 145 reconn_loss: 9125.357421875 kl_loss: 1836.767333984375\n",
      "epoch: 18 iter: 146 reconn_loss: 9213.048828125 kl_loss: 1860.913330078125\n",
      "epoch: 18 iter: 147 reconn_loss: 8898.001953125 kl_loss: 1856.615966796875\n",
      "epoch: 18 iter: 148 reconn_loss: 9268.9443359375 kl_loss: 1919.50537109375\n",
      "epoch: 18 iter: 149 reconn_loss: 9190.8154296875 kl_loss: 1919.108154296875\n",
      "epoch: 18 iter: 150 reconn_loss: 8970.5400390625 kl_loss: 1876.618408203125\n",
      "epoch: 18 iter: 151 reconn_loss: 9156.1943359375 kl_loss: 1829.364013671875\n",
      "epoch: 18 iter: 152 reconn_loss: 9269.546875 kl_loss: 1906.952392578125\n",
      "epoch: 18 iter: 153 reconn_loss: 9416.0048828125 kl_loss: 1889.366455078125\n",
      "epoch: 18 iter: 154 reconn_loss: 9247.435546875 kl_loss: 1841.4569091796875\n",
      "epoch: 18 iter: 155 reconn_loss: 9040.73828125 kl_loss: 1893.43603515625\n",
      "epoch: 18 iter: 156 reconn_loss: 9210.9794921875 kl_loss: 1863.7781982421875\n",
      "epoch: 18 iter: 157 reconn_loss: 9199.326171875 kl_loss: 1887.8826904296875\n",
      "epoch: 18 iter: 158 reconn_loss: 8766.875 kl_loss: 1836.65966796875\n",
      "epoch: 18 iter: 159 reconn_loss: 9158.685546875 kl_loss: 1859.6707763671875\n",
      "epoch: 18 iter: 160 reconn_loss: 9199.2392578125 kl_loss: 1876.652587890625\n",
      "epoch: 18 iter: 161 reconn_loss: 9067.626953125 kl_loss: 1880.9130859375\n",
      "epoch: 18 iter: 162 reconn_loss: 8749.4326171875 kl_loss: 1841.0811767578125\n",
      "epoch: 18 iter: 163 reconn_loss: 9440.830078125 kl_loss: 1894.33740234375\n",
      "epoch: 18 iter: 164 reconn_loss: 9093.13671875 kl_loss: 1881.854248046875\n",
      "epoch: 18 iter: 165 reconn_loss: 9189.7890625 kl_loss: 1874.876220703125\n",
      "epoch: 18 iter: 166 reconn_loss: 9113.837890625 kl_loss: 1915.49169921875\n",
      "epoch: 18 iter: 167 reconn_loss: 8857.2099609375 kl_loss: 1894.344970703125\n",
      "epoch: 18 iter: 168 reconn_loss: 9072.3193359375 kl_loss: 1903.126220703125\n",
      "epoch: 18 iter: 169 reconn_loss: 9087.830078125 kl_loss: 1882.43017578125\n",
      "epoch: 18 iter: 170 reconn_loss: 9137.642578125 kl_loss: 1888.478271484375\n",
      "epoch: 18 iter: 171 reconn_loss: 9114.9287109375 kl_loss: 1886.4635009765625\n",
      "epoch: 18 iter: 172 reconn_loss: 9018.4814453125 kl_loss: 1879.339111328125\n",
      "epoch: 18 iter: 173 reconn_loss: 9096.98828125 kl_loss: 1843.0423583984375\n",
      "epoch: 18 iter: 174 reconn_loss: 9485.05859375 kl_loss: 1940.2677001953125\n",
      "epoch: 18 iter: 175 reconn_loss: 8742.9912109375 kl_loss: 1857.9921875\n",
      "epoch: 18 iter: 176 reconn_loss: 8869.08984375 kl_loss: 1911.3887939453125\n",
      "epoch: 18 iter: 177 reconn_loss: 8949.171875 kl_loss: 1905.56982421875\n",
      "epoch: 18 iter: 178 reconn_loss: 9043.2802734375 kl_loss: 1925.5601806640625\n",
      "epoch: 18 iter: 179 reconn_loss: 9170.154296875 kl_loss: 1900.1693115234375\n",
      "epoch: 18 iter: 180 reconn_loss: 9020.95703125 kl_loss: 1882.287841796875\n",
      "epoch: 18 iter: 181 reconn_loss: 9034.7607421875 kl_loss: 1876.279052734375\n",
      "epoch: 18 iter: 182 reconn_loss: 9109.1357421875 kl_loss: 1917.608154296875\n",
      "epoch: 18 iter: 183 reconn_loss: 8932.693359375 kl_loss: 1869.61962890625\n",
      "epoch: 18 iter: 184 reconn_loss: 9000.25390625 kl_loss: 1881.57861328125\n",
      "epoch: 18 iter: 185 reconn_loss: 9039.6005859375 kl_loss: 1907.5296630859375\n",
      "epoch: 18 iter: 186 reconn_loss: 8927.0654296875 kl_loss: 1877.8865966796875\n",
      "epoch: 18 iter: 187 reconn_loss: 9154.9970703125 kl_loss: 1872.4290771484375\n",
      "epoch: 18 iter: 188 reconn_loss: 8899.8515625 kl_loss: 1838.404541015625\n",
      "epoch: 18 iter: 189 reconn_loss: 8798.4814453125 kl_loss: 1910.6197509765625\n",
      "epoch: 18 iter: 190 reconn_loss: 9300.3193359375 kl_loss: 1876.0882568359375\n",
      "epoch: 18 iter: 191 reconn_loss: 8777.130859375 kl_loss: 1879.200439453125\n",
      "epoch: 18 iter: 192 reconn_loss: 9264.03125 kl_loss: 1884.28857421875\n",
      "epoch: 18 iter: 193 reconn_loss: 8894.6103515625 kl_loss: 1855.214599609375\n",
      "epoch: 18 iter: 194 reconn_loss: 8977.4384765625 kl_loss: 1901.3114013671875\n",
      "epoch: 18 iter: 195 reconn_loss: 9239.1328125 kl_loss: 1894.0284423828125\n",
      "epoch: 18 iter: 196 reconn_loss: 8700.98046875 kl_loss: 1899.10546875\n",
      "epoch: 18 iter: 197 reconn_loss: 9068.4931640625 kl_loss: 1883.0926513671875\n",
      "epoch: 18 iter: 198 reconn_loss: 9518.7119140625 kl_loss: 1875.8907470703125\n",
      "epoch: 18 iter: 199 reconn_loss: 9274.1923828125 kl_loss: 1869.8231201171875\n",
      "epoch: 18 iter: 200 reconn_loss: 8800.1455078125 kl_loss: 1864.9505615234375\n",
      "epoch: 18 iter: 201 reconn_loss: 8863.4873046875 kl_loss: 1902.985595703125\n",
      "epoch: 18 iter: 202 reconn_loss: 8930.25 kl_loss: 1901.28076171875\n",
      "epoch: 18 iter: 203 reconn_loss: 8939.732421875 kl_loss: 1873.620849609375\n",
      "epoch: 18 iter: 204 reconn_loss: 9203.162109375 kl_loss: 1842.9757080078125\n",
      "epoch: 18 iter: 205 reconn_loss: 8791.48828125 kl_loss: 1856.160400390625\n",
      "epoch: 18 iter: 206 reconn_loss: 8793.1494140625 kl_loss: 1885.0452880859375\n",
      "epoch: 18 iter: 207 reconn_loss: 8921.3564453125 kl_loss: 1904.7342529296875\n",
      "epoch: 18 iter: 208 reconn_loss: 9062.6484375 kl_loss: 1950.19189453125\n",
      "epoch: 18 iter: 209 reconn_loss: 9060.1650390625 kl_loss: 1918.923095703125\n",
      "epoch: 18 iter: 210 reconn_loss: 8858.0234375 kl_loss: 1887.1956787109375\n",
      "epoch: 18 iter: 211 reconn_loss: 8803.4462890625 kl_loss: 1905.9522705078125\n",
      "epoch: 18 iter: 212 reconn_loss: 9351.412109375 kl_loss: 1900.38427734375\n",
      "epoch: 18 iter: 213 reconn_loss: 9310.6650390625 kl_loss: 1913.86767578125\n",
      "epoch: 18 iter: 214 reconn_loss: 8819.88671875 kl_loss: 1860.3829345703125\n",
      "epoch: 18 iter: 215 reconn_loss: 8851.833984375 kl_loss: 1897.414794921875\n",
      "epoch: 18 iter: 216 reconn_loss: 9304.19921875 kl_loss: 1922.2117919921875\n",
      "epoch: 18 iter: 217 reconn_loss: 8712.9140625 kl_loss: 1842.6097412109375\n",
      "epoch: 18 iter: 218 reconn_loss: 9293.341796875 kl_loss: 1890.4322509765625\n",
      "epoch: 18 iter: 219 reconn_loss: 9103.4306640625 kl_loss: 1905.5738525390625\n",
      "epoch: 18 iter: 220 reconn_loss: 8806.173828125 kl_loss: 1863.9820556640625\n",
      "epoch: 18 iter: 221 reconn_loss: 9146.02734375 kl_loss: 1876.1517333984375\n",
      "epoch: 18 iter: 222 reconn_loss: 5914.453125 kl_loss: 1255.303955078125\n",
      "0.weight tensor(76.0548) tensor(-56.6058)\n",
      "0.bias tensor(26.2332) tensor(-47.8030)\n",
      "2.weight tensor(17.2950) tensor(-17.4404)\n",
      "2.bias tensor(9.7947) tensor(-8.9561)\n",
      "4.weight tensor(11.2182) tensor(-11.1692)\n",
      "4.bias tensor(3.5575) tensor(-5.7473)\n",
      "epoch: 19 iter: 0 reconn_loss: 8933.2421875 kl_loss: 1851.0048828125\n",
      "epoch: 19 iter: 1 reconn_loss: 8971.0263671875 kl_loss: 1909.9093017578125\n",
      "epoch: 19 iter: 2 reconn_loss: 8686.896484375 kl_loss: 1906.4971923828125\n",
      "epoch: 19 iter: 3 reconn_loss: 9129.78125 kl_loss: 1833.31103515625\n",
      "epoch: 19 iter: 4 reconn_loss: 9113.6923828125 kl_loss: 1866.3231201171875\n",
      "epoch: 19 iter: 5 reconn_loss: 9077.12109375 kl_loss: 1901.18212890625\n",
      "epoch: 19 iter: 6 reconn_loss: 9019.1484375 kl_loss: 1880.52001953125\n",
      "epoch: 19 iter: 7 reconn_loss: 9060.1982421875 kl_loss: 1861.3172607421875\n",
      "epoch: 19 iter: 8 reconn_loss: 8794.2548828125 kl_loss: 1870.9442138671875\n",
      "epoch: 19 iter: 9 reconn_loss: 8628.3681640625 kl_loss: 1866.012939453125\n",
      "epoch: 19 iter: 10 reconn_loss: 9167.4921875 kl_loss: 1895.95947265625\n",
      "epoch: 19 iter: 11 reconn_loss: 9055.87109375 kl_loss: 1870.0302734375\n",
      "epoch: 19 iter: 12 reconn_loss: 8770.49609375 kl_loss: 1852.0992431640625\n",
      "epoch: 19 iter: 13 reconn_loss: 8764.41796875 kl_loss: 1845.5220947265625\n",
      "epoch: 19 iter: 14 reconn_loss: 8881.876953125 kl_loss: 1916.0791015625\n",
      "epoch: 19 iter: 15 reconn_loss: 9199.931640625 kl_loss: 1874.1014404296875\n",
      "epoch: 19 iter: 16 reconn_loss: 8754.35546875 kl_loss: 1866.7830810546875\n",
      "epoch: 19 iter: 17 reconn_loss: 9144.458984375 kl_loss: 1892.722412109375\n",
      "epoch: 19 iter: 18 reconn_loss: 8985.6845703125 kl_loss: 1849.9718017578125\n",
      "epoch: 19 iter: 19 reconn_loss: 8798.6748046875 kl_loss: 1848.054443359375\n",
      "epoch: 19 iter: 20 reconn_loss: 9038.8212890625 kl_loss: 1852.164306640625\n",
      "epoch: 19 iter: 21 reconn_loss: 8808.822265625 kl_loss: 1856.8455810546875\n",
      "epoch: 19 iter: 22 reconn_loss: 9158.8974609375 kl_loss: 1830.200439453125\n",
      "epoch: 19 iter: 23 reconn_loss: 8935.8134765625 kl_loss: 1834.40087890625\n",
      "epoch: 19 iter: 24 reconn_loss: 9161.609375 kl_loss: 1851.0986328125\n",
      "epoch: 19 iter: 25 reconn_loss: 9064.298828125 kl_loss: 1854.3060302734375\n",
      "epoch: 19 iter: 26 reconn_loss: 9362.8984375 kl_loss: 1891.2236328125\n",
      "epoch: 19 iter: 27 reconn_loss: 9093.3017578125 kl_loss: 1854.2100830078125\n",
      "epoch: 19 iter: 28 reconn_loss: 9049.181640625 kl_loss: 1879.56005859375\n",
      "epoch: 19 iter: 29 reconn_loss: 9116.67578125 kl_loss: 1856.674072265625\n",
      "epoch: 19 iter: 30 reconn_loss: 9399.5888671875 kl_loss: 1889.117919921875\n",
      "epoch: 19 iter: 31 reconn_loss: 8820.228515625 kl_loss: 1908.2030029296875\n",
      "epoch: 19 iter: 32 reconn_loss: 8874.8125 kl_loss: 1874.73681640625\n",
      "epoch: 19 iter: 33 reconn_loss: 9199.892578125 kl_loss: 1866.421875\n",
      "epoch: 19 iter: 34 reconn_loss: 9116.7275390625 kl_loss: 1893.9559326171875\n",
      "epoch: 19 iter: 35 reconn_loss: 9025.8251953125 kl_loss: 1904.2286376953125\n",
      "epoch: 19 iter: 36 reconn_loss: 8917.0966796875 kl_loss: 1883.6708984375\n",
      "epoch: 19 iter: 37 reconn_loss: 9139.697265625 kl_loss: 1877.0439453125\n",
      "epoch: 19 iter: 38 reconn_loss: 9046.7099609375 kl_loss: 1906.367431640625\n",
      "epoch: 19 iter: 39 reconn_loss: 9026.9013671875 kl_loss: 1899.0150146484375\n",
      "epoch: 19 iter: 40 reconn_loss: 8629.2607421875 kl_loss: 1900.341796875\n",
      "epoch: 19 iter: 41 reconn_loss: 8964.310546875 kl_loss: 1892.05224609375\n",
      "epoch: 19 iter: 42 reconn_loss: 8980.3896484375 kl_loss: 1880.1005859375\n",
      "epoch: 19 iter: 43 reconn_loss: 8843.525390625 kl_loss: 1886.112548828125\n",
      "epoch: 19 iter: 44 reconn_loss: 9353.6103515625 kl_loss: 1883.239501953125\n",
      "epoch: 19 iter: 45 reconn_loss: 9191.2626953125 kl_loss: 1899.9102783203125\n",
      "epoch: 19 iter: 46 reconn_loss: 9039.88671875 kl_loss: 1876.778076171875\n",
      "epoch: 19 iter: 47 reconn_loss: 9111.2509765625 kl_loss: 1861.2174072265625\n",
      "epoch: 19 iter: 48 reconn_loss: 9103.439453125 kl_loss: 1897.6170654296875\n",
      "epoch: 19 iter: 49 reconn_loss: 9165.7490234375 kl_loss: 1912.831787109375\n",
      "epoch: 19 iter: 50 reconn_loss: 8957.810546875 kl_loss: 1889.3253173828125\n",
      "epoch: 19 iter: 51 reconn_loss: 8772.474609375 kl_loss: 1864.1685791015625\n",
      "epoch: 19 iter: 52 reconn_loss: 9145.302734375 kl_loss: 1885.7730712890625\n",
      "epoch: 19 iter: 53 reconn_loss: 9454.60546875 kl_loss: 1909.9222412109375\n",
      "epoch: 19 iter: 54 reconn_loss: 9051.8525390625 kl_loss: 1814.503662109375\n",
      "epoch: 19 iter: 55 reconn_loss: 8900.5166015625 kl_loss: 1885.4716796875\n",
      "epoch: 19 iter: 56 reconn_loss: 9075.19140625 kl_loss: 1929.173095703125\n",
      "epoch: 19 iter: 57 reconn_loss: 8959.8486328125 kl_loss: 1898.8785400390625\n",
      "epoch: 19 iter: 58 reconn_loss: 9203.7412109375 kl_loss: 1894.7449951171875\n",
      "epoch: 19 iter: 59 reconn_loss: 9208.6875 kl_loss: 1894.04296875\n",
      "epoch: 19 iter: 60 reconn_loss: 9274.08984375 kl_loss: 1907.2789306640625\n",
      "epoch: 19 iter: 61 reconn_loss: 8845.712890625 kl_loss: 1837.198486328125\n",
      "epoch: 19 iter: 62 reconn_loss: 8983.568359375 kl_loss: 1855.97265625\n",
      "epoch: 19 iter: 63 reconn_loss: 9240.736328125 kl_loss: 1847.46240234375\n",
      "epoch: 19 iter: 64 reconn_loss: 8947.69140625 kl_loss: 1886.7310791015625\n",
      "epoch: 19 iter: 65 reconn_loss: 8960.7939453125 kl_loss: 1887.757080078125\n",
      "epoch: 19 iter: 66 reconn_loss: 9082.46875 kl_loss: 1888.6304931640625\n",
      "epoch: 19 iter: 67 reconn_loss: 8986.0771484375 kl_loss: 1874.650146484375\n",
      "epoch: 19 iter: 68 reconn_loss: 9010.8935546875 kl_loss: 1873.614013671875\n",
      "epoch: 19 iter: 69 reconn_loss: 9021.60546875 kl_loss: 1872.7620849609375\n",
      "epoch: 19 iter: 70 reconn_loss: 8719.513671875 kl_loss: 1891.3546142578125\n",
      "epoch: 19 iter: 71 reconn_loss: 9142.203125 kl_loss: 1890.0521240234375\n",
      "epoch: 19 iter: 72 reconn_loss: 8869.9326171875 kl_loss: 1885.149169921875\n",
      "epoch: 19 iter: 73 reconn_loss: 9050.1669921875 kl_loss: 1882.695068359375\n",
      "epoch: 19 iter: 74 reconn_loss: 8835.6328125 kl_loss: 1861.3463134765625\n",
      "epoch: 19 iter: 75 reconn_loss: 8609.392578125 kl_loss: 1877.917724609375\n",
      "epoch: 19 iter: 76 reconn_loss: 9021.64453125 kl_loss: 1845.6016845703125\n",
      "epoch: 19 iter: 77 reconn_loss: 8773.431640625 kl_loss: 1847.4766845703125\n",
      "epoch: 19 iter: 78 reconn_loss: 9088.5234375 kl_loss: 1866.580078125\n",
      "epoch: 19 iter: 79 reconn_loss: 9060.7978515625 kl_loss: 1913.7593994140625\n",
      "epoch: 19 iter: 80 reconn_loss: 9096.787109375 kl_loss: 1881.0328369140625\n",
      "epoch: 19 iter: 81 reconn_loss: 8783.578125 kl_loss: 1884.365966796875\n",
      "epoch: 19 iter: 82 reconn_loss: 8762.603515625 kl_loss: 1848.525634765625\n",
      "epoch: 19 iter: 83 reconn_loss: 8904.71875 kl_loss: 1912.5977783203125\n",
      "epoch: 19 iter: 84 reconn_loss: 8857.41796875 kl_loss: 1899.9461669921875\n",
      "epoch: 19 iter: 85 reconn_loss: 8981.44921875 kl_loss: 1895.73095703125\n",
      "epoch: 19 iter: 86 reconn_loss: 8790.3310546875 kl_loss: 1880.526611328125\n",
      "epoch: 19 iter: 87 reconn_loss: 9001.8828125 kl_loss: 1868.3369140625\n",
      "epoch: 19 iter: 88 reconn_loss: 8693.64453125 kl_loss: 1911.1761474609375\n",
      "epoch: 19 iter: 89 reconn_loss: 9118.3837890625 kl_loss: 1921.5316162109375\n",
      "epoch: 19 iter: 90 reconn_loss: 8968.6904296875 kl_loss: 1859.6343994140625\n",
      "epoch: 19 iter: 91 reconn_loss: 8941.9658203125 kl_loss: 1936.341064453125\n",
      "epoch: 19 iter: 92 reconn_loss: 8982.5244140625 kl_loss: 1876.4027099609375\n",
      "epoch: 19 iter: 93 reconn_loss: 9181.82421875 kl_loss: 1930.9501953125\n",
      "epoch: 19 iter: 94 reconn_loss: 8894.2021484375 kl_loss: 1911.4520263671875\n",
      "epoch: 19 iter: 95 reconn_loss: 9062.1220703125 kl_loss: 1964.643798828125\n",
      "epoch: 19 iter: 96 reconn_loss: 9036.6806640625 kl_loss: 1842.288330078125\n",
      "epoch: 19 iter: 97 reconn_loss: 9192.078125 kl_loss: 1890.59765625\n",
      "epoch: 19 iter: 98 reconn_loss: 9192.9140625 kl_loss: 1871.8133544921875\n",
      "epoch: 19 iter: 99 reconn_loss: 8765.74609375 kl_loss: 1886.899658203125\n",
      "epoch: 19 iter: 100 reconn_loss: 9230.677734375 kl_loss: 1904.5338134765625\n",
      "epoch: 19 iter: 101 reconn_loss: 9033.455078125 kl_loss: 1911.2305908203125\n",
      "epoch: 19 iter: 102 reconn_loss: 8775.6630859375 kl_loss: 1876.3140869140625\n",
      "epoch: 19 iter: 103 reconn_loss: 9216.62109375 kl_loss: 1892.4364013671875\n",
      "epoch: 19 iter: 104 reconn_loss: 8996.4287109375 kl_loss: 1894.3587646484375\n",
      "epoch: 19 iter: 105 reconn_loss: 9059.095703125 kl_loss: 1937.7735595703125\n",
      "epoch: 19 iter: 106 reconn_loss: 9347.7080078125 kl_loss: 1868.605712890625\n",
      "epoch: 19 iter: 107 reconn_loss: 8899.275390625 kl_loss: 1855.476806640625\n",
      "epoch: 19 iter: 108 reconn_loss: 9436.3369140625 kl_loss: 1854.6121826171875\n",
      "epoch: 19 iter: 109 reconn_loss: 8786.9462890625 kl_loss: 1839.5428466796875\n",
      "epoch: 19 iter: 110 reconn_loss: 9474.9609375 kl_loss: 1891.2322998046875\n",
      "epoch: 19 iter: 111 reconn_loss: 9117.470703125 kl_loss: 1874.086669921875\n",
      "epoch: 19 iter: 112 reconn_loss: 9112.302734375 kl_loss: 1902.6727294921875\n",
      "epoch: 19 iter: 113 reconn_loss: 8918.275390625 kl_loss: 1873.8905029296875\n",
      "epoch: 19 iter: 114 reconn_loss: 9067.591796875 kl_loss: 1894.958984375\n",
      "epoch: 19 iter: 115 reconn_loss: 9074.84765625 kl_loss: 1912.5987548828125\n",
      "epoch: 19 iter: 116 reconn_loss: 8926.513671875 kl_loss: 1879.39697265625\n",
      "epoch: 19 iter: 117 reconn_loss: 9135.119140625 kl_loss: 1920.78466796875\n",
      "epoch: 19 iter: 118 reconn_loss: 9106.951171875 kl_loss: 1882.6884765625\n",
      "epoch: 19 iter: 119 reconn_loss: 9319.509765625 kl_loss: 1882.3466796875\n",
      "epoch: 19 iter: 120 reconn_loss: 9104.30859375 kl_loss: 1863.9326171875\n",
      "epoch: 19 iter: 121 reconn_loss: 9325.630859375 kl_loss: 1897.5380859375\n",
      "epoch: 19 iter: 122 reconn_loss: 9335.68359375 kl_loss: 1920.8197021484375\n",
      "epoch: 19 iter: 123 reconn_loss: 9362.431640625 kl_loss: 1953.3885498046875\n",
      "epoch: 19 iter: 124 reconn_loss: 9187.263671875 kl_loss: 1875.3359375\n",
      "epoch: 19 iter: 125 reconn_loss: 9037.6923828125 kl_loss: 1901.0126953125\n",
      "epoch: 19 iter: 126 reconn_loss: 8988.544921875 kl_loss: 1898.020263671875\n",
      "epoch: 19 iter: 127 reconn_loss: 9166.275390625 kl_loss: 1886.4405517578125\n",
      "epoch: 19 iter: 128 reconn_loss: 8978.052734375 kl_loss: 1833.969482421875\n",
      "epoch: 19 iter: 129 reconn_loss: 9159.775390625 kl_loss: 1909.9002685546875\n",
      "epoch: 19 iter: 130 reconn_loss: 9239.0986328125 kl_loss: 1854.8135986328125\n",
      "epoch: 19 iter: 131 reconn_loss: 9005.30078125 kl_loss: 1866.65087890625\n",
      "epoch: 19 iter: 132 reconn_loss: 8831.833984375 kl_loss: 1875.82373046875\n",
      "epoch: 19 iter: 133 reconn_loss: 8900.2939453125 kl_loss: 1860.135498046875\n",
      "epoch: 19 iter: 134 reconn_loss: 8751.3515625 kl_loss: 1854.527587890625\n",
      "epoch: 19 iter: 135 reconn_loss: 8664.2890625 kl_loss: 1860.52880859375\n",
      "epoch: 19 iter: 136 reconn_loss: 9250.75 kl_loss: 1875.0830078125\n",
      "epoch: 19 iter: 137 reconn_loss: 8777.2626953125 kl_loss: 1901.481689453125\n",
      "epoch: 19 iter: 138 reconn_loss: 9237.88671875 kl_loss: 1899.99169921875\n",
      "epoch: 19 iter: 139 reconn_loss: 8887.6884765625 kl_loss: 1918.681884765625\n",
      "epoch: 19 iter: 140 reconn_loss: 8746.2490234375 kl_loss: 1876.3743896484375\n",
      "epoch: 19 iter: 141 reconn_loss: 8930.7802734375 kl_loss: 1871.58935546875\n",
      "epoch: 19 iter: 142 reconn_loss: 9061.4453125 kl_loss: 1965.085693359375\n",
      "epoch: 19 iter: 143 reconn_loss: 9173.6240234375 kl_loss: 1941.692138671875\n",
      "epoch: 19 iter: 144 reconn_loss: 8747.46484375 kl_loss: 1938.8055419921875\n",
      "epoch: 19 iter: 145 reconn_loss: 8868.458984375 kl_loss: 1949.4482421875\n",
      "epoch: 19 iter: 146 reconn_loss: 9091.4384765625 kl_loss: 1950.9075927734375\n",
      "epoch: 19 iter: 147 reconn_loss: 9092.9970703125 kl_loss: 1924.7674560546875\n",
      "epoch: 19 iter: 148 reconn_loss: 8800.0244140625 kl_loss: 1951.856689453125\n",
      "epoch: 19 iter: 149 reconn_loss: 8999.3984375 kl_loss: 1894.4256591796875\n",
      "epoch: 19 iter: 150 reconn_loss: 9107.1484375 kl_loss: 1886.9327392578125\n",
      "epoch: 19 iter: 151 reconn_loss: 9051.1240234375 kl_loss: 1908.6619873046875\n",
      "epoch: 19 iter: 152 reconn_loss: 9690.8095703125 kl_loss: 1949.559814453125\n",
      "epoch: 19 iter: 153 reconn_loss: 9154.86328125 kl_loss: 1918.6575927734375\n",
      "epoch: 19 iter: 154 reconn_loss: 8739.916015625 kl_loss: 1879.718017578125\n",
      "epoch: 19 iter: 155 reconn_loss: 8877.7265625 kl_loss: 1911.3846435546875\n",
      "epoch: 19 iter: 156 reconn_loss: 8711.779296875 kl_loss: 1863.85595703125\n",
      "epoch: 19 iter: 157 reconn_loss: 9107.125 kl_loss: 1888.505615234375\n",
      "epoch: 19 iter: 158 reconn_loss: 9098.109375 kl_loss: 1864.5184326171875\n",
      "epoch: 19 iter: 159 reconn_loss: 9057.9609375 kl_loss: 1911.4017333984375\n",
      "epoch: 19 iter: 160 reconn_loss: 9218.283203125 kl_loss: 1935.13427734375\n",
      "epoch: 19 iter: 161 reconn_loss: 8875.2255859375 kl_loss: 1880.031982421875\n",
      "epoch: 19 iter: 162 reconn_loss: 9203.45703125 kl_loss: 1928.14501953125\n",
      "epoch: 19 iter: 163 reconn_loss: 8981.9091796875 kl_loss: 1895.838623046875\n",
      "epoch: 19 iter: 164 reconn_loss: 8968.400390625 kl_loss: 1874.0672607421875\n",
      "epoch: 19 iter: 165 reconn_loss: 9035.4638671875 kl_loss: 1861.862060546875\n",
      "epoch: 19 iter: 166 reconn_loss: 9270.9365234375 kl_loss: 1911.841064453125\n",
      "epoch: 19 iter: 167 reconn_loss: 9046.8974609375 kl_loss: 1864.617919921875\n",
      "epoch: 19 iter: 168 reconn_loss: 9288.9775390625 kl_loss: 1880.4827880859375\n",
      "epoch: 19 iter: 169 reconn_loss: 9373.193359375 kl_loss: 1863.478515625\n",
      "epoch: 19 iter: 170 reconn_loss: 8480.6220703125 kl_loss: 1869.7860107421875\n",
      "epoch: 19 iter: 171 reconn_loss: 8939.8095703125 kl_loss: 1935.500244140625\n",
      "epoch: 19 iter: 172 reconn_loss: 8929.6171875 kl_loss: 1879.7225341796875\n",
      "epoch: 19 iter: 173 reconn_loss: 9054.451171875 kl_loss: 1904.083984375\n",
      "epoch: 19 iter: 174 reconn_loss: 9005.158203125 kl_loss: 1896.1473388671875\n",
      "epoch: 19 iter: 175 reconn_loss: 8888.9501953125 kl_loss: 1913.156494140625\n",
      "epoch: 19 iter: 176 reconn_loss: 9182.9833984375 kl_loss: 1906.4495849609375\n",
      "epoch: 19 iter: 177 reconn_loss: 9238.2578125 kl_loss: 1907.4581298828125\n",
      "epoch: 19 iter: 178 reconn_loss: 8840.666015625 kl_loss: 1896.90478515625\n",
      "epoch: 19 iter: 179 reconn_loss: 8748.705078125 kl_loss: 1834.148193359375\n",
      "epoch: 19 iter: 180 reconn_loss: 9097.6103515625 kl_loss: 1908.1407470703125\n",
      "epoch: 19 iter: 181 reconn_loss: 8637.3369140625 kl_loss: 1902.4417724609375\n",
      "epoch: 19 iter: 182 reconn_loss: 9142.0302734375 kl_loss: 1917.001953125\n",
      "epoch: 19 iter: 183 reconn_loss: 9223.7275390625 kl_loss: 1899.4111328125\n",
      "epoch: 19 iter: 184 reconn_loss: 8868.154296875 kl_loss: 1868.5340576171875\n",
      "epoch: 19 iter: 185 reconn_loss: 8900.21875 kl_loss: 1885.3148193359375\n",
      "epoch: 19 iter: 186 reconn_loss: 8973.5625 kl_loss: 1866.453369140625\n",
      "epoch: 19 iter: 187 reconn_loss: 9072.9365234375 kl_loss: 1869.2596435546875\n",
      "epoch: 19 iter: 188 reconn_loss: 9048.63671875 kl_loss: 1861.92431640625\n",
      "epoch: 19 iter: 189 reconn_loss: 9054.7333984375 kl_loss: 1873.1070556640625\n",
      "epoch: 19 iter: 190 reconn_loss: 8580.638671875 kl_loss: 1846.36279296875\n",
      "epoch: 19 iter: 191 reconn_loss: 8613.083984375 kl_loss: 1864.1751708984375\n",
      "epoch: 19 iter: 192 reconn_loss: 9439.6923828125 kl_loss: 1880.173828125\n",
      "epoch: 19 iter: 193 reconn_loss: 8883.126953125 kl_loss: 1838.4581298828125\n",
      "epoch: 19 iter: 194 reconn_loss: 8985.826171875 kl_loss: 1893.82421875\n",
      "epoch: 19 iter: 195 reconn_loss: 8885.080078125 kl_loss: 1808.98876953125\n",
      "epoch: 19 iter: 196 reconn_loss: 9038.474609375 kl_loss: 1931.088623046875\n",
      "epoch: 19 iter: 197 reconn_loss: 9117.765625 kl_loss: 1883.8040771484375\n",
      "epoch: 19 iter: 198 reconn_loss: 8866.80078125 kl_loss: 1897.513427734375\n",
      "epoch: 19 iter: 199 reconn_loss: 9281.552734375 kl_loss: 1886.021484375\n",
      "epoch: 19 iter: 200 reconn_loss: 8990.2783203125 kl_loss: 1908.435302734375\n",
      "epoch: 19 iter: 201 reconn_loss: 9094.6650390625 kl_loss: 1909.5167236328125\n",
      "epoch: 19 iter: 202 reconn_loss: 8912.3115234375 kl_loss: 1901.737548828125\n",
      "epoch: 19 iter: 203 reconn_loss: 8889.4130859375 kl_loss: 1921.0379638671875\n",
      "epoch: 19 iter: 204 reconn_loss: 9203.115234375 kl_loss: 1906.05078125\n",
      "epoch: 19 iter: 205 reconn_loss: 8523.140625 kl_loss: 1895.748291015625\n",
      "epoch: 19 iter: 206 reconn_loss: 8766.830078125 kl_loss: 1882.349609375\n",
      "epoch: 19 iter: 207 reconn_loss: 8891.1435546875 kl_loss: 1867.6253662109375\n",
      "epoch: 19 iter: 208 reconn_loss: 9305.6337890625 kl_loss: 1873.623779296875\n",
      "epoch: 19 iter: 209 reconn_loss: 9188.5546875 kl_loss: 1884.406982421875\n",
      "epoch: 19 iter: 210 reconn_loss: 8822.0400390625 kl_loss: 1865.546142578125\n",
      "epoch: 19 iter: 211 reconn_loss: 8981.265625 kl_loss: 1927.0706787109375\n",
      "epoch: 19 iter: 212 reconn_loss: 9163.7939453125 kl_loss: 1869.7694091796875\n",
      "epoch: 19 iter: 213 reconn_loss: 8943.376953125 kl_loss: 1869.1746826171875\n",
      "epoch: 19 iter: 214 reconn_loss: 9242.611328125 kl_loss: 1884.878662109375\n",
      "epoch: 19 iter: 215 reconn_loss: 9244.2548828125 kl_loss: 1861.5347900390625\n",
      "epoch: 19 iter: 216 reconn_loss: 8700.236328125 kl_loss: 1872.934814453125\n",
      "epoch: 19 iter: 217 reconn_loss: 9270.0361328125 kl_loss: 1896.732421875\n",
      "epoch: 19 iter: 218 reconn_loss: 9164.8916015625 kl_loss: 1856.6219482421875\n",
      "epoch: 19 iter: 219 reconn_loss: 8994.806640625 kl_loss: 1889.2857666015625\n",
      "epoch: 19 iter: 220 reconn_loss: 8920.6015625 kl_loss: 1882.9034423828125\n",
      "epoch: 19 iter: 221 reconn_loss: 8675.7734375 kl_loss: 1847.22119140625\n",
      "epoch: 19 iter: 222 reconn_loss: 5822.298828125 kl_loss: 1243.087646484375\n",
      "0.weight tensor(61.9090) tensor(-61.1503)\n",
      "0.bias tensor(38.5836) tensor(-22.5691)\n",
      "2.weight tensor(16.6576) tensor(-15.2728)\n",
      "2.bias tensor(11.8002) tensor(-7.1089)\n",
      "4.weight tensor(9.5719) tensor(-12.7958)\n",
      "4.bias tensor(5.7195) tensor(-3.3130)\n"
     ]
    }
   ],
   "source": [
    "# solver.lr = solver.lr / 100\n",
    "solver.train(train_loader, 20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 374,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x2768463e670>"
      ]
     },
     "execution_count": 374,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "img = train_set[51][0]\n",
    "plt.imshow(img.reshape(28, 28))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 375,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1080x288 with 3 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "params = [p[1].flatten().detach().numpy() for p in model.decoder.named_parameters() if p[0].find(\"weight\") != -1]\n",
    "num_params = len(params)\n",
    "cols = min(3, num_params)  # adjust layout\n",
    "rows = (num_params + cols - 1) // cols  # ceil division\n",
    "\n",
    "plt.figure(figsize=(cols * 5, rows * 4))\n",
    "\n",
    "for i, param in enumerate(params, 1):\n",
    "    plt.subplot(rows, cols, i)\n",
    "    plt.hist(param, bins=50)\n",
    "    plt.xlabel(\"Value\")\n",
    "    plt.ylabel(\"Frequency\")\n",
    "    plt.title(f\"Param {i}\")\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 418,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x276878a8b80>"
      ]
     },
     "execution_count": 418,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "x, mean, log_var = model(img.to(device).reshape(1, 784))\n",
    "\n",
    "plt.imshow(x.reshape(28, 28).cpu().detach().numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 410,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x27687599bb0>"
      ]
     },
     "execution_count": 410,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "randimg = torch.rand(1, 75, device=device)\n",
    "out = model.decoder(randimg)\n",
    "plt.imshow(out.reshape(28, 28).cpu().detach().numpy())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.7 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "vscode": {
   "interpreter": {
    "hash": "70561045587b4d96f139d942df5566a2d2c41c2f16b761693781ce545d2add28"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
